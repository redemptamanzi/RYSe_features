{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code in this notebook extracts vocabulary features from each query and then returns a data frame of those extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries\n",
    "\n",
    "The following block of code loads all libraries needed for this notebook. Numpy has an established to ensure that the random selection of queries drawn to establish certain features, such as top word n-grams; is consistent across this code and future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(20200522)\n",
    "\n",
    "stopwords = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Vocabulary Features\n",
    "\n",
    "Features used in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a list into a dictionary.\n",
    "#\n",
    "# param lst: is the list that is convert into dictionary\n",
    "# returns resDct: the converted list\n",
    "\n",
    "def convert(lst):\n",
    "    \n",
    "    resDct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    \n",
    "    return resDct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets\n",
    "\n",
    "This block of code loads the data sets and extracts all unique queries from both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) )\n",
    "allQueries = allSessions['query'].tolist() + allSessionsSQS['query'].tolist() \n",
    "allQueries = set(allQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Vocab\n",
    "\n",
    "Loads all vocabulary expected to be learned between Kindergarten to Seventh grade based on Common Core Curriculum, before extracting the ratio of words in each query that are, and are not; found in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:08<00:00, 8326.87it/s] \n"
     ]
    }
   ],
   "source": [
    "kd = ['a', 'all', 'am', 'an', 'and', 'are', 'as', 'at', 'away', 'back', 'ball', 'bell', 'big', 'bird', 'blue', 'book', 'boot', 'box', 'boy', 'brown', 'but', 'by', 'can', 'car', 'cat', 'come', 'cow', 'day', 'do', 'dog', 'down', 'end', 'fall', 'fan', 'fish', 'fly', 'food', 'for', 'from', 'fun', 'get', 'go', 'good', 'gray', 'green', 'groundhog', 'hat', 'he', 'here', 'hill', 'I', 'in', 'into', 'is', 'it', 'inside', 'kitten', 'little', 'look', 'mad', 'me', 'mud', 'my', 'name', 'no', 'not', 'of', 'on', 'orange', 'out', 'paint', 'pet', 'pin', 'play', 'put', 'rain', 'red', 'run', 'sad', 'say', 'see', 'she', 'sing', 'sit', 'so', 'stay', 'stop', 'story', 'sun', 'take', 'that', 'the', 'them', 'then', 'there', 'they', 'this', 'to', 'too', 'up', 'we', 'wet', 'what', 'where', 'who', 'will', 'with', 'work', 'yellow', 'yes', 'you', 'zoo', 'orange', 'white', 'black', 'monday', 'tuesday', 'wednesday','thursday','friday', 'saturday','sunday']\n",
    "oned = ['a', 'all', 'am', 'and', 'at', 'ball', 'be', 'bed', 'big', 'book', 'box', 'boy', 'but', 'came', 'can', 'car', 'cat', 'come', 'cow', 'dad', 'day', 'did', 'do', 'dog', 'fat', 'for', 'fun', 'get', 'go', 'good', 'got', 'had', 'hat', 'he', 'hen', 'here', 'him', 'his', 'home', 'hot', 'I', 'if', 'in', 'into', 'is', 'it', 'its', 'let', 'like', 'look', 'man', 'may', 'me', 'mom', 'my', 'no', 'not', 'of', 'oh', 'old', 'on', 'one', 'out', 'pan', 'pet', 'pig', 'play', 'ran', 'rat', 'red', 'ride', 'run', 'sat', 'see', 'she', 'sit', 'six', 'so', 'stop', 'sun', 'ten', 'the', 'this', 'to', 'top', 'toy', 'two', 'up', 'us', 'was', 'we', 'will', 'yes', 'you' ]\n",
    "twod = ['about', 'add', 'after', 'ago', 'an ', 'any', 'apple', 'are ', 'as', 'ask', 'ate', 'away', 'baby ', 'back', 'bad', 'bag', 'base', 'bat', 'bee', 'been', 'before', 'being', 'best', 'bike', 'bill', 'bird', 'black', 'blue', 'boat', 'both', 'bring', 'brother ', 'brown', 'bus', 'buy ', 'by', 'cake', 'call', 'candy', 'change', 'child', 'city', 'clean', 'club', 'coat', 'cold', 'coming ', 'corn', 'could', 'cry', 'cup', 'cut', 'daddy ', 'dear', 'deep', 'deer', 'doing', 'doll', 'door', 'down ', 'dress', 'drive', 'drop', 'dry', 'duck', 'each', 'eat', 'eating', 'egg', 'end', 'fall', 'far', 'farm', 'fast', 'father ', 'feed', 'feel', 'feet', 'fell ', 'find', 'fine ', 'fire', 'first ', 'fish', 'five', 'fix', 'flag', 'floor', 'fly', 'food', 'foot', 'four', 'fox', 'from ', 'full', 'funny', 'game', 'gas', 'gave', 'girl', 'give', 'glad', 'goat', 'goes ', 'going ', 'gold', 'gone', 'grade ', 'grass', 'green', 'grow', 'hand', 'happy', 'hard', 'has ', 'have ', 'hear ', 'help', 'here ', 'hill', 'hit', 'hold', 'hole', 'hop', 'hope ', 'horse', 'house ', 'how ', 'ice', 'inch', 'inside ', 'job', 'jump', 'just ', 'keep', 'king', 'know ', 'lake', 'land', 'last', 'late', 'lay', 'left', 'leg', 'light', 'line', 'little ', 'live', 'lives', 'long', 'looking', 'lost', 'lot', 'love', 'mad', 'made ', 'make ', 'many ', 'meat', 'men', 'met', 'mile', 'milk', 'mine', 'miss', 'moon', 'more', 'most', 'mother ', 'move', 'much ', 'must', 'myself ', 'nail', 'name ', 'need', 'new ', 'next', 'nice ', 'night', 'nine', 'north', 'now ', 'nut', 'off ', 'only', 'open', 'or ', 'other', 'our', 'outside ', 'over', 'page', 'park', 'part', 'pay', 'pick', 'plant', 'playing', 'pony', 'post', 'pull', 'put', 'rabbit', 'rain', 'read', 'rest', 'riding', 'road', 'rock', 'room', 'said ', 'same', 'sang', 'saw ', 'say', 'school ', 'sea', 'seat', 'seem', 'seen', 'send', 'set', 'seven', 'sheep', 'ship', 'shoe', 'show ', 'sick', 'side', 'sing', 'sky', 'sleep', 'small', 'snow', 'some ', 'soon ', 'spell', 'start', 'stay', 'still', 'store ', 'story', 'take', 'talk', 'tall', 'teach', 'tell', 'than ', 'thank', 'that', 'them ', 'then ', 'there ', 'they ', 'thing', 'think ', 'three', 'time ', 'today ', 'told', 'too ', 'took', 'train ', 'tree', 'truck', 'try', 'use', 'very ', 'walk', 'want ', 'warm', 'wash', 'way', 'week', 'well ', 'went ', 'were ', 'wet', 'what', 'when ', 'while ', 'white', 'who', 'why', 'wind', 'wish', 'with ', 'woke', 'wood', 'work', 'yellow', 'yet', 'your', 'zoo']\n",
    "threed = ['able', 'above', 'afraid', 'afternoon', 'again', 'age', 'air', 'airplane', 'almost', 'alone', 'along', 'already', 'also', 'always', 'animal', 'another', 'anything', 'around', 'art', 'aunt', 'balloon', 'bark', 'barn', 'basket', 'beach', 'bear', 'because', 'become', 'began', 'begin', 'behind', 'believe', 'below', 'belt', 'better', 'birthday', 'body', 'bones', 'born', 'bought', 'bread', 'bright', 'broke', 'brought', 'busy', 'cabin', 'cage', 'camp', 'can\\'t', 'care', 'carry', 'catch', 'cattle', 'cave', 'children', 'class', 'close', 'cloth', 'coal', 'color', 'corner', 'cotton', 'cover', 'dark', 'desert', 'didn\\'t', 'dinner', 'dishes', 'does', 'done', 'don\\'t', 'dragon', 'draw', 'dream', 'drink', 'early', 'earth', 'east', 'eight', 'even', 'ever', 'every', 'everyone', 'everything', 'eyes', 'face', 'family', 'feeling', 'felt', 'few', 'fight', 'fishing', 'flower', 'flying', 'follow', 'forest', 'forgot', 'form', 'found', 'fourth', 'free', 'Friday', 'friend', 'front', 'getting', 'given', 'grandmother', 'great', 'grew', 'ground', 'guess', 'hair', 'half', 'having', 'head', 'heard', 'he\\'s', 'heat', 'hello', 'high', 'himself', 'hour', 'hundred', 'hurry', 'hurt', 'I\\'d', 'I\\'ll', 'I\\'m', 'inches', 'isn\\'t', 'it\\'s', 'I\\'ve', 'kept', 'kids', 'kind', 'kitten', 'knew', 'knife', 'lady', 'large', 'largest', 'later', 'learn', 'leave', 'let\\'s', 'letter', 'life', 'list', 'living', 'lovely', 'loving', 'lunch', 'mail', 'making', 'maybe', 'mean', 'merry', 'might', 'mind', 'money', 'month', 'morning', 'mouse', 'mouth', 'Mr.', 'Mrs.', 'Ms.', 'music', 'near', 'nearly', 'never', 'news', 'noise', 'nothing', 'number', 'o\\'clock', 'often', 'oil', 'once', 'orange', 'order', 'own', 'pair', 'paint', 'paper', 'party', 'pass', 'past', 'penny', 'people', 'person', 'picture', 'place', 'plan', 'plane', 'please', 'pocket', 'point', 'poor', 'race', 'reach', 'reading', 'ready', 'real', 'rich', 'right', 'river', 'rocket', 'rode', 'round', 'rule', 'running', 'salt', 'says', 'sending', 'sent', 'seventh', 'sew', 'shall', 'short', 'shot', 'should', 'sight', 'sister', 'sitting', 'sixth', 'sled', 'smoke', 'soap', 'someone', 'something', 'sometime', 'song', 'sorry', 'sound', 'south', 'space', 'spelling', 'spent', 'sport', 'spring', 'stairs', 'stand', 'state', 'step', 'stick', 'stood', 'stopped', 'stove', 'street', 'strong', 'study', 'such', 'sugar', 'summer', 'Sunday', 'supper', 'table', 'taken', 'taking', 'talking', 'teacher', 'team', 'teeth', 'tenth', 'that\\'s', 'their', 'these', 'thinking', 'third', 'those', 'thought', 'throw', 'tonight', 'trade', 'trick', 'trip', 'trying', 'turn', 'twelve', 'twenty', 'uncle', 'under', 'upon', 'wagon', 'wait', 'walking', 'wasn\\'t', 'watch', 'water', 'weather', 'we\\'re', 'west', 'wheat', 'where', 'which', 'wife', 'wild', 'win', 'window', 'winter', 'without', 'woman', 'won', 'won\\'t', 'wool', 'word', 'working', 'world', 'would', 'write', 'wrong', 'yard', 'year', 'yesterday', 'you\\'re'  ]\n",
    "fourd = ['across', 'against', 'answer', 'awhile', 'between', 'board', 'bottom', 'breakfast', 'broken', 'build', 'building', 'built', 'captain', 'carried', 'caught', 'charge', 'chicken', 'circus', 'cities', 'clothes', 'company', 'couldn\\'t', 'country', 'discover', 'doctor', 'doesn\\'t', 'dollar', 'during', 'eighth', 'else', 'enjoy', 'enough', 'everybody', 'example', 'except', 'excuse', 'field', 'fifth', 'finish', 'following', 'good-by', 'group', 'happened', 'harden', 'haven\\'t', 'heavy', 'held', 'hospital', 'idea', 'instead', 'known', 'laugh', 'middle', 'minute', 'mountain', 'ninth', 'ocean', 'office', 'parent', 'peanut', 'pencil', 'picnic', 'police', 'pretty', 'prize', 'quite', 'radio', 'raise', 'really', 'reason', 'remember', 'return', 'Saturday', 'scare', 'second', 'since', 'slowly', 'stories', 'student', 'sudden', 'suit', 'sure', 'swimming', 'though', 'threw', 'tired', 'together', 'tomorrow', 'toward', 'tried', 'trouble', 'truly', 'turtle', 'until', 'village', 'visit', 'wear', 'we\\'ll', 'whole', 'whose', 'women', 'wouldn\\'t', 'writing', 'written', 'wrote', 'yell', 'young']\n",
    "fived = ['although', 'America', 'among', 'arrive', 'attention', 'beautiful', 'countries', 'course', 'cousin', 'decide', 'different', 'evening', 'favorite', 'finally', 'future', 'happiest', 'happiness', 'important', 'interest', 'piece', 'planet', 'present', 'president', 'principal', 'probably', 'problem', 'receive', 'sentence', 'several', 'special', 'suddenly', 'suppose', 'surely', 'surprise', 'they\\'re', 'through', 'usually', 'action', 'actor', 'actually', 'addition', 'agreed', 'allowed', 'aloud', 'amendment', 'amount', 'amusement', 'annual', 'appointed', 'arrange', 'attention', 'awhile', 'beginning', 'bruise', 'business', 'calves', 'capital', 'capitol', 'captain', 'carefully', 'caught', 'cause', 'celebrate', 'century', 'chemical', 'chocolate', 'circle', 'climate', 'climbed', 'collar', 'column', 'company', 'condition', 'consider', 'consonant', 'constant', 'continent', 'continued', 'country', 'course', 'crystal', 'current', 'curtain', 'daughter', 'daytime', 'decided', 'decimal', 'delicious', 'desert', 'dessert', 'details', 'determine', 'dictionary', 'difference', 'different', 'difficult', 'direction', 'disappoint', 'division', 'eighth', 'election', 'elements', 'energy', 'enjoyment', 'equal', 'equation', 'errands', 'exact', 'except', 'expect', 'explain', 'explode', 'express', 'factory', 'fault', 'favorite', 'finally', 'finished', 'forward', 'fought', 'fraction', 'furniture', 'future', 'general', 'government', 'graceful', 'graph', 'grasp', 'grease', 'grown-ups', 'guest', 'guide', 'happened', 'happily', 'harvest', 'healthy', 'height', 'hoarse', 'human', 'idea', 'imagine', 'include', 'increase', 'indicate', 'information', 'instrument', 'intention', 'interesting', 'inventor', 'island', 'jewel', 'journey', 'jungle', 'knives', 'known', 'language', 'laughter', 'length', 'limb', 'located', 'lumber', 'major', 'mammal', 'manufacture', 'material', 'mayor', 'measure', 'melody', 'members', 'memories', 'message', 'method', 'million', 'minor', 'modern', 'mountain', 'music', 'natural', 'necessary', 'neither', 'newspaper', 'northern', 'notebook', 'notice', 'noun', 'numeral', 'object', 'observe', 'opposite', 'orphan', 'ought', 'outside', 'oxygen', 'paid', 'paint', 'paragraph', 'pattern', 'pause', 'payment', 'perhaps', 'period', 'permit', 'phone', 'phrase', 'pleasant', 'pleasure', 'plural', 'poison', 'position', 'possible', 'practice', 'prepared', 'president', 'probably', 'problem', 'process', 'produce', 'program', 'promise', 'property', 'protection', 'provide', 'puzzle', 'quickly', 'quietly', 'radio', 'raise', 'rarely', 'rather', 'reached', 'receive', 'record', 'region', 'relax', 'remain', 'remove', 'repay', 'repeat', 'report', 'represent', 'respond', 'result', 'rhythm', 'rising', 'ruin', 'salad', 'sandal', 'scale', 'scent', 'schedule', 'science', 'section', 'separate', 'service', 'settled', 'several', 'shadow', 'shelter', 'shoulder', 'shouted', 'shower', 'signal', 'similar', 'sincerely', 'single', 'size', 'slippery', 'soar', 'soil', 'solution', 'solve', 'southern', 'split', 'spoiled', 'sports', 'square', 'squeeze', 'stain', 'state', 'statement', 'station', 'steer', 'stomach', 'stopping', 'straight', 'straighten', 'stream', 'stretched', 'suggest', 'suitcase', 'sunset', 'supply', 'sure', 'surface', 'surprise', 'surround', 'sweater', 'syllable', 'syrup', 'tablet', 'tasty', 'teaspoon', 'terrible', 'though', 'thoughtful', 'thrown', 'tornado', 'toward', 'traffic', 'trail', 'treasure', 'treatment', 'triangle', 'trouble', 'tunnel', 'type', 'understood', 'unknown', 'usually', 'value', 'various', 'warn', 'weigh', 'weight', 'weird', 'western', 'whisper', 'whoever', 'whole', 'whose', 'wives', 'women', 'wonderful', 'wound', 'wreck', 'x-ray', 'yesterday']\n",
    "sixd = ['Abandon', 'abundant', 'access', 'accommodate', 'accumulate', 'adapt', 'adhere', 'agony', 'allegiance', 'ambition', 'ample', 'anguish', 'anticipate', 'anxious', 'apparel', 'appeal', 'apprehensive', 'arid', 'arrogant', 'awe', 'Barren', 'beacon', 'beneficial', 'blunder', 'boisterous', 'boycott', 'burden', 'Campaign', 'capacity', 'capital', 'chronological', 'civic', 'clarity', 'collaborate', 'collide', 'commend', 'commentary', 'compact', 'composure', 'concise', 'consent', 'consequence', 'conserve', 'conspicuous', 'constant', 'contaminate', 'context', 'continuous', 'controversy', 'convenient', 'cope', 'cordial', 'cultivate', 'cumulative', '', 'Declare', 'deluge', 'dense', 'deplete', 'deposit', 'designate', 'desperate', 'deteriorate', 'dialogue', 'diligent', 'diminish', 'discretion', 'dissent', 'dissolve', 'distinct', 'diversity', 'domestic', 'dominate', 'drastic', 'duration', 'dwell', 'Eclipse', 'economy', 'eerie', 'effect', 'efficient', 'elaborate', 'eligible', 'elude', 'encounter', 'equivalent', 'erupt', 'esteem', 'evolve', 'exaggerate', 'excel', 'exclude', 'expanse', 'exploit', 'extinct', 'extract', 'Factor', 'former', 'formulates', 'fuse', 'futile', 'Generate', 'genre', 'Habitat', 'hazardous', 'hoax', 'hostile', 'Idiom', 'ignite', 'immense', 'improvises', 'inept', 'inevitable', 'influence', 'ingenious', 'innovation', 'intimidate', 'Jovial', 'Knack', 'Leeway', 'legislation', 'leisure', 'liberate', 'likeness', 'linger', 'literal', 'loathe', 'lure', 'Majority', 'makeshift', 'manipulate', 'marvel', 'massive', 'maximum', 'meager', 'mere', 'migration', 'mimic', 'minute', 'monotonous', 'Negotiate', 'Objective', 'obstacle', 'omniscient', 'onset', 'optimist', 'originate', 'Painstaking', 'paraphrase', 'parody', 'persecute', 'plummet', 'possess', 'poverty', 'precise', 'predicament', 'predict', 'prejudice', 'preliminary', 'primitive', 'priority', 'prominent', 'propel', 'prosecute', 'prosper', 'provoke', 'pursue', 'Quest', 'Recount', 'refuge', 'reinforce', 'reluctant', 'remorse', 'remote', 'resolute', 'restrain', 'retaliate', 'retrieve', 'rigorous', 'rural', 'Salvage', 'sanctuary', 'siege', 'significant', 'solar', 'soothe', 'stationary', 'stifle', 'strive', 'subordinate', 'subsequent', 'superior', 'supplement', 'swarm', 'Tangible', 'terminate', 'terrain', 'trait', 'transform', 'transport', 'treacherous', 'Unanimous', 'unique', 'unruly', 'urban', 'Vacate', 'verdict', 'verge', 'vibrant', 'vital', 'vow', 'accept', 'accidentally', 'acquire', 'ambulance', 'ancient', 'appearance', 'appointment', 'arithmetic', 'audience', 'autumn', 'beautifully', 'beliefs', 'blown', 'bough', 'bows', 'calendar', 'canyon', 'capable', 'capacity', 'caution', 'ceiling', 'champion', 'choir', 'cleanse', 'combination', 'comfortable', 'community', 'complain', 'concentration', 'concern', 'connection', 'constitution', 'contagious', 'conversation', 'cooperation', 'correct', 'coupon', 'creative', 'creature', 'crisis', 'culture', 'curious', 'dangerous', 'decision', 'demonstrate', 'denominator', 'department', 'departure', 'depth', 'descendant', 'disagreement', 'disastrous', 'discussion', 'distance', 'distributed', 'earliest', 'echoes', 'edition', 'educate', 'electricity', 'element', 'elevator', 'emergency', 'employer', 'emptiness', 'encouragement', 'encyclopedia', 'entire', 'entrance', 'envelope', 'equator', 'especially', 'establish', 'example', 'excellent', 'excitement', 'exercise', 'experience', 'exterior', 'familiar', 'faucet', 'fierce', 'fireproof', 'following', 'forgetting', 'forgiveness', 'fossil', 'freight', 'frighten', 'fuel', 'further', 'gallon', 'gaze', 'gesture', 'governor', 'graduation', 'grateful', 'grief', 'halves', 'hamburger', 'hangar', 'hanger', 'happiness', 'headache', 'heroes', 'history', 'honorable', 'horizon', 'hunger', 'hyphen', 'ignore', 'imagination', 'immediate', 'importance', 'improvement', 'independence', 'ingredient', 'injury', 'inquire', 'instead', 'instruction', 'intermission', 'interview', 'invisible', 'invitation', 'involve', 'jealous', 'junior', 'knowledge', 'lawyer', 'league', 'legal', 'liberty', 'liquid', 'listening', 'loaves', 'location', 'luggage', 'manager', 'manner', 'manor', 'marriage', 'meant', 'mechanic', 'medicine', 'mention', 'minus', 'minute', 'mistaken', 'misunderstand', 'mixture', 'mourn', 'multiple', 'muscle', 'museum', 'musician', 'mute', 'myth', 'nationality', 'negative', 'noisy', 'noticeable', 'novel', 'numerator', 'obtain', 'occur', 'official', 'operate', 'original', 'outline', 'partial', 'passenger', 'patient', 'penalty', 'penguin', 'percent', 'performance', 'personal', 'persuade', 'physical', 'piano', 'plumber', 'poem', 'poet', 'policy', 'pollute', 'pollution', 'positive', 'potatoes', 'predict', 'prefer', 'pressure', 'prevent', 'principal', 'private', 'project', 'pumpkins', 'purchase', 'purse', 'quote', 'radius', 'rapid', 'ratio', 'realize', 'recently', 'recycle', 'reduce', 'referred', 'regardless', 'regular', 'rehearse', 'relief', 'relieve', 'remarkable', 'remind', 'remote', 'replacement', 'replied', 'reply', 'requirement', 'rescue', 'resident', 'resources', 'respectful', 'review', 'roam', 'routine', 'rumor', 'rural', 'safety', 'sailor', 'salute', 'satisfy', 'scarcely', 'scientific', 'scissors', 'selection', 'senior', 'sentence', 'separately', 'serious', 'session', 'shampoo', 'shelves', 'shorten', 'silent', 'simply', 'sketch', 'skillful', 'solar', 'sought', 'spaghetti', 'sponge', 'squawk', 'storage', 'strain', 'strategy', 'strength', 'strive', 'struggle', 'studios', 'success', 'suggestion', 'support', 'surrounded', 'sword', 'system', 'telephone', 'television', 'temperature', 'theme', 'themselves', 'therefore', 'thicken', 'thousand', 'threat', 'tomatoes', 'trophies', 'tutor', 'unbelievable', 'underneath', 'unite', 'vacuum', 'vain', 'variety', 'vary', 'vault', 'vegetable', 'vein', 'violence', 'visible', 'vision', 'waste', 'who\\'s', 'whose', 'wrestle', 'wrinkle', 'yield']\n",
    "sevend = ['abbreviation', 'absence', 'absolutely', 'absorb', 'abundant', 'accessible', 'accompanied', 'accomplishment', 'accurate', 'achievement', 'acres', 'adequate', 'adjustable', 'admit', 'admittance', 'advice', 'advise', 'afghan', 'alternate', 'alternative', 'amusement', 'analysis', 'analyze', 'ancestor', 'anniversary', 'appreciate', 'artificial', 'assistance', 'association', 'athlete', 'atmosphere', 'attendance', 'authority', 'bacteria', 'bagel', 'baggage', 'benefited', 'benefiting', 'bicycle', 'biscuit', 'bizarre', 'boulevard', 'boundary', 'bouquet', 'brilliant', 'brochure', 'bulletin', 'bureau', 'campaign', 'cancellation', 'candidate', 'capable', 'capital', 'capitol', 'category', 'celery', 'cemetery', 'changeable', 'chaperone', 'character', 'cinnamon', 'civilize', 'commercial', 'committed', 'committee', 'commotion', 'companion', 'competent', 'competition', 'complement', 'complex', 'compliment', 'compressor', 'concentrate', 'concentration', 'conductor', 'confetti', 'congratulations', 'consequently', 'controlling', 'cringe', 'culminate', 'culprit', 'deceive', 'delayed', 'democracy', 'deodorant', 'descendent', 'description', 'diameter', 'diamond', 'discourage', 'disgraceful', 'dismissal', 'distinguished', 'dreadful', 'economics', 'economy', 'elementary', 'embarrass', 'emotion', 'emphasize', 'encircle', 'enclosing', 'encounter', 'endurance', 'engineer', 'environment', 'episode', 'erosion', 'eruption', 'evident', 'exchange', 'executive', 'exhibit', 'expensive', 'extinct', 'extinguish', 'extraordinary', 'extremely', 'fabricate', 'failure', 'fascinating', 'fatigue', 'flagrant', 'foreign', 'forfeit', 'frequently', 'fundamental', 'genuine', 'ghetto', 'gossiping', 'gradual', 'graffiti', 'grammar', 'grievance', 'guarantee', 'harass', 'havoc', 'heroic', 'hesitate', 'horrify', 'hospital', 'humid', 'humility', 'hygiene', 'identical', 'idle', 'idol', 'illegal', 'illustration', 'imaginary', 'immediately', 'immobilize', 'impossibility', 'inconvenient', 'incredible', 'individual', 'infamous', 'influence', 'informant', 'inhabit', 'inherit', 'innocence', 'innocent', 'instructor', 'intelligent', 'interruption', 'introduction', 'involvement', 'irate', 'irresistible', 'jealousy', 'judgment', 'juvenile', 'kettle', 'knitting', 'laboratory', 'language', 'legibly', 'liquidation', 'management', 'maneuver', 'media', 'mileage', 'miniature', 'misbehaved', 'morale', 'mortgage', 'movement', 'murmur', 'musician', 'mysterious', 'negotiate', 'nervous', 'nuisance', 'nurture', 'oases', 'oasis', 'obedient', 'obstacle', 'obviously', 'occasion', 'ordinarily', 'ordinary', 'organization', 'pamphlet', 'panic', 'panicked', 'panicky', 'parallel', 'paralysis', 'paralyze', 'penicillin', 'pedestrian', 'phantom', 'pheasant', 'phrase', 'politely', 'popular', 'precipitation', 'principal', 'principle', 'privilege', 'procedure', 'pronunciation', 'psychology', 'puny', 'qualified', 'qualifying', 'quotation', 'raspberry', 'reasonable', 'receipt', 'receiving', 'recipe', 'recognition', 'recommend', 'recruit', 'reddest', 'reprimand', 'resigned', 'restaurant', 'rotten', 'sandwich', 'scarcity', 'scenery', 'secretary', 'securing', 'significance', 'simile', 'sincerely', 'sincerity', 'situation', 'skeptical', 'slumber', 'smudge', 'solemn', 'souvenir', 'spacious', 'specific', 'stationary', 'stationery', 'statistics', 'subscription', 'substitute', 'superintendent', 'supervisor', 'supposedly', 'threatening', 'tolerate', 'tongue', 'tournament', 'tragedy', 'traitor', 'transferred', 'transferring', 'transmitted', 'traveled', 'traveling', 'unfortunately', 'uniform', 'university', 'unnecessary', 'valuable', 'various', 'vehicle', 'version', 'vertical', 'victim', 'vigorously', 'violation', 'visualize', 'volcano', 'voyage', 'wealthy', 'weapon', 'wheeze', 'wilderness', 'Abate', 'abnormal', 'abode', 'abrupt', 'accelerate', 'acclaim', 'acknowledge', 'acquire', 'aspire', 'acrid', 'addict', 'adjacent', 'admonish', 'affliction', 'agitate', 'ajar', 'akin', 'allege', 'annihilate', 'anonymous', 'antagonize', 'apathy', 'arbitrate', 'astute', 'authentic', 'avert', 'Bellow', 'beseech', 'bestow', 'bewilder', 'bigot', 'blatant', 'bleak', 'braggart', 'brawl', 'browse', 'bystander', 'Candid', 'canine', 'canny', 'capricious', 'capsize', 'casual', 'casualty', 'catastrophe', 'cater', 'chorus', 'citrus', 'clamber', 'climax', 'compromise', 'concur', 'confront', 'congested', 'conjure', 'consult', 'corrupt', 'counterfeit', 'covet', 'customary', 'Debut', 'deceased', 'dependent', 'despondent', 'detach', 'devour', 'dishearten', 'dismal', 'dismantle', 'distraught', 'docile', 'downright', 'drone', 'dumbfound', 'Emblem', 'endure', 'ensue', 'enthrall', 'epidemic', 'erode', 'exuberant', 'Fathom', 'feud', 'figment', 'firebrand', 'flabbergast', 'flagrant', 'flaw', 'fruitless', 'Gaudy', 'geography', 'gratify', 'gravity', 'grim', 'grimy', 'grueling', 'gruesome', 'Haggle', 'headlong', 'hilarious', 'homage', 'homicide', 'hospitable', 'hurtle', 'hybrid', 'Illiterate', 'impede', 'implore', 'incident', 'incredulous', 'infamous', 'infuriate', 'insinuate', 'intensified', 'inundate', 'irate', 'Lavish', 'legacy', 'legitimate', 'lethal', 'loath', 'lurk', 'Magnetic', 'mirth', 'quench', 'magnitude', 'maternal', 'maul', 'melancholy', 'mellow', 'momentum', 'mortify', 'mull', 'murky', 'Narrative', 'negligent', 'nimble', 'nomadic', 'noteworthy', 'notify', 'notorious', 'nurture', 'Obnoxious', 'oration', 'orthodox', 'overwhelm', 'Pamper', 'patronize', 'peevish', 'pelt', 'pending', 'perceived', 'perjury', 'permanent', 'persist', 'perturb', 'pique', 'pluck', 'poised', 'ponder', 'potential', 'predatory', 'presume', 'preview', 'prior', 'prowess', 'Radiant', 'random', 'rant', 'recede', 'reprimand', 'resume', 'retort', 'robust', 'rupture', 'Saga', 'sequel', 'sham', 'shirk', 'simultaneously', 'snare', 'species', 'status', 'stodgy', 'substantial', 'subtle', 'sullen', 'supervise', 'Tamper', 'throb', 'toxic', 'tragedy', 'trickle', 'trivial', 'Uncertainty', 'unscathed', 'upright', 'urgent', 'utmost', 'Vengeance', 'vicious', 'vindictive', 'vista', 'vocation', 'void', 'Wary', 'whim', 'wince', 'wrath', 'Yearn']\n",
    "\n",
    "coreVocab = []\n",
    "vocab = []\n",
    "nonVocab = []\n",
    "\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "for word in kd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in oned:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in twod:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in threed:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in fourd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in fived:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in sixd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in sevend:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "    \n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "\n",
    "        splitQuery = [st.lemmatize(i.lower()) for i in query.split(' ')]\n",
    "\n",
    "        queryVocab = 0\n",
    "        nonqueryVocab = 0\n",
    "        totalVocab = 0\n",
    "\n",
    "        for word in splitQuery:\n",
    "            if word in coreVocab:\n",
    "                queryVocab  +=1\n",
    "                totalVocab  +=1\n",
    "            else:\n",
    "                nonqueryVocab +=1\n",
    "                totalVocab  +=1\n",
    "\n",
    "        vocab.append(queryVocab/totalVocab) \n",
    "        nonVocab.append(nonqueryVocab/totalVocab) \n",
    "        pbar.update()\n",
    "\n",
    "Vocab = pd.DataFrame(data=vocab, columns = ['coreVocab'])\n",
    "Vocab['query'] = allQueries\n",
    "Vocab['nonCoreVocab'] = nonVocab\n",
    "Vocab = Vocab.set_index('query')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age of Acquisition features\n",
    "\n",
    "In this block of code we first load up the Age of Acquistion data set (which is a csv with multiple columns representing a variety of information) and process it into a dictionary where the key is the word, and the value is AoA rating. We then find the AoA rating for each word in the query, extracting the min, max, average (known as query complexity), and ratio of words expected to be learned by the age of 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:07<00:00, 9090.54it/s] \n"
     ]
    }
   ],
   "source": [
    "AoAvocab = []\n",
    "\n",
    "with open('DataSets/AoA/AoA_51715_words.csv') as csvFile:\n",
    "    csvReader = csv.reader(csvFile)\n",
    "    lineCount = 0\n",
    "    for row in csvReader:\n",
    "        if lineCount == 0:\n",
    "            lineCount += 1\n",
    "        else:\n",
    "            AoAvocab.append(row[7])\n",
    "            AoAvocab.append(row[10])\n",
    "            \n",
    "AoAVConv = convert(AoAvocab)\n",
    "\n",
    "minAoA = []\n",
    "maxAoA = []\n",
    "averageVocab = []\n",
    "ratioAoA = []\n",
    "\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        count = 0\n",
    "        vocab = []\n",
    "\n",
    "        for word in query.split(' '):\n",
    "            word = word.lower().strip()\n",
    "            word = re.sub(r'[^\\w\\s]','',word)\n",
    "            word = st.lemmatize(word)\n",
    "            if word in AoAVConv:\n",
    "                vocab.append(float(AoAVConv[word]))\n",
    "            else:\n",
    "                vocab.append(0)\n",
    "\n",
    "\n",
    "        vocab = np.array(vocab)\n",
    "        \n",
    "        if vocab.size == 0:\n",
    "            minAoA.append(-1) \n",
    "            maxAoA.append(-1) \n",
    "            averageVocab.append(-1)\n",
    "            ratioAoA.append(0)\n",
    "        elif vocab.size > 0:\n",
    "            minAoA.append(np.min(vocab))\n",
    "            maxAoA.append(np.max(vocab))\n",
    "            averageVocab.append(np.mean(vocab))\n",
    "            for entry in vocab:\n",
    "                if entry < 13 and entry > 0:\n",
    "                    count +=1\n",
    "            ratioAoA.append(count/len(vocab))\n",
    "        \n",
    "        pbar.update()\n",
    "\n",
    "Vocab['minAoA'] = minAoA\n",
    "Vocab['maxAoA'] = maxAoA\n",
    "Vocab['ratioAoA'] = ratioAoA\n",
    "Vocab['queryComplexity'] = averageVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sven Features\n",
    "\n",
    "Int his block of code we load the list of commonly searched for terms used by children as established in the Is Sven Seven data set, and then determines the ratio of those terms occuring in each query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [02:17<00:00, 511.29it/s]\n"
     ]
    }
   ],
   "source": [
    "SVENwords = []\n",
    "st = WordNetLemmatizer()\n",
    "with open('DataSets/Sven/ChildrenDict.tsv') as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter = '\\t')\n",
    "    lineCount = 0\n",
    "    for row in csvReader:\n",
    "        if lineCount == 0:\n",
    "            lineCount +=1\n",
    "        else:\n",
    "            SVENwords.append((row[1]))\n",
    "            \n",
    "SVENcount = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "            wordCount +=1\n",
    "            if word in SVENwords:\n",
    "                countWord +=1\n",
    "\n",
    "        SVENcount.append(countWord/wordCount)\n",
    "        pbar.update()\n",
    "        \n",
    "Vocab['SVEN'] = SVENcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Stereotype Uni-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who belong to our stereotype, extract the 250 most common word uni-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:01<00:00, 56076.54it/s]\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "\n",
    "for query in queries:\n",
    "    text += query.lower() + \" \"\n",
    "    \n",
    "queryWords = text.split()\n",
    "\n",
    "resultWords  = [word for word in queryWords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultWords)\n",
    "text = text.split(' ')\n",
    "fdist1 = nltk.FreqDist(text)\n",
    "top250 = []\n",
    "\n",
    "for x in fdist1.most_common(250):\n",
    "    top250.append(x[0])\n",
    "    \n",
    "top250count = []\n",
    "top250avg = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "            wordCount +=1\n",
    "            if word in top250:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top250count.append(countWord)\n",
    "        top250avg.append(countWord/wordCount)\n",
    "        pbar.update()\n",
    "\n",
    "\n",
    "Vocab['top250SterCount'] = top250count\n",
    "Vocab['top250SterRatAnt'] = top250avg\n",
    "Vocab['top250SterRatCon'] = 1-Vocab['top250SterRatAnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Non-Stereotype Uni-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who do not belong to our stereotype, extract the 250 most common word uni-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:01<00:00, 58794.97it/s]\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "\n",
    "for query in queries:\n",
    "    text += query.lower() + \" \"\n",
    "    \n",
    "queryWords = text.split()\n",
    "\n",
    "resultWords  = [word for word in queryWords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultWords)\n",
    "text = text.split(' ')\n",
    "fdist1 = nltk.FreqDist(text)\n",
    "top250 = []\n",
    "\n",
    "for x in fdist1.most_common(250):\n",
    "    top250.append(x[0])\n",
    "    \n",
    "top250Count = []\n",
    "top250Avg = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "            wordCount +=1\n",
    "            if word in top250:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top250Count.append(countWord)\n",
    "        top250Avg.append(countWord/wordCount)\n",
    "        pbar.update()\n",
    "    \n",
    "Vocab['top250NonSterCount'] = top250Count\n",
    "Vocab['top250NonSterRatAnt'] = top250Avg\n",
    "Vocab['top250NonSterRatCon'] = 1-Vocab['top250NonSterRatAnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Stereotype Bi-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who belong to our stereotype, extract the 50 most common word bi-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:00<00:00, 104654.30it/s]\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "queries2 = []\n",
    "for query in queries:\n",
    "    queries2.append(query.lower())\n",
    "queries = queries2\n",
    "\n",
    "bigrams = [b for l in queries for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "\n",
    "fdist1 = nltk.FreqDist(bigrams)\n",
    "\n",
    "top50 = []\n",
    "\n",
    "for x in fdist1.most_common(50):\n",
    "     top50.append(x[0])\n",
    "        \n",
    "top50count = []\n",
    "top50avg = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        query = query.lower()\n",
    "        query = query.split(\" \")\n",
    "        split = nltk.bigrams(query)\n",
    "        for word in split:\n",
    "            wordCount +=1\n",
    "            if word in top50:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top50count.append(countWord)\n",
    "        if wordCount > 0:\n",
    "            top50avg.append(countWord/wordCount)\n",
    "        else:\n",
    "            top50avg.append(0)\n",
    "        pbar.update()\n",
    "        \n",
    "Vocab['top50SterCount'] = top50count\n",
    "Vocab['top50SterRatAnt'] = top50avg\n",
    "Vocab['top50SterAntCon'] = 1-Vocab['top50SterRatAnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Non-Stereotype Bi-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who do not belong to our stereotype, extract the 50 most common word bi-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:00<00:00, 100183.64it/s]\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "queries2 = []\n",
    "for query in queries:\n",
    "    queries2.append(query.lower())\n",
    "queries = queries2\n",
    "\n",
    "bigrams = [b for l in queries for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "\n",
    "fdist1 = nltk.FreqDist(bigrams)\n",
    "\n",
    "top50 = []\n",
    "\n",
    "for x in fdist1.most_common(50):\n",
    "     top50.append(x[0])\n",
    "        \n",
    "top50count = []\n",
    "top50avg = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        query = query.lower()\n",
    "        query = query.split(\" \")\n",
    "        split = nltk.bigrams(query)\n",
    "        for word in split:\n",
    "            wordCount +=1\n",
    "            if word in top50:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top50count.append(countWord)\n",
    "        if wordCount > 0:\n",
    "            top50avg.append(countWord/wordCount)\n",
    "        else:\n",
    "            top50avg.append(0)\n",
    "        pbar.update()\n",
    "        \n",
    "Vocab['top50NonSterCount'] = top50count\n",
    "Vocab['top50NonSterRatAnt'] = top50avg\n",
    "Vocab['top50NonSterAntCon'] = 1-Vocab['top50NonSterRatAnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF All\n",
    "\n",
    "In the following block of code we take an 80% sample of all sessions found in SWC, and then calculate the TF-IDF values for each all queries with each query in every session seen as an individual document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:27<00:00, 2587.17it/s]\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "\n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for m in vectors:\n",
    "        if(m.sum() != 0):\n",
    "            listTFIDF.append(m.sum() / m.count_nonzero())\n",
    "        else:\n",
    "            listTFIDF.append(-1)\n",
    "        pbar.update()\n",
    "        \n",
    "VocabTFIDFAll = pd.DataFrame(data=listTFIDF, columns = ['tfidfAll']).fillna(-1)\n",
    "VocabTFIDFAll['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabTFIDFAll, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Stereotype\n",
    "\n",
    "In the following block of code we take an 80% sample of all stereotype sessions found in SWC, and then calculate the TF-IDF values for each all queries with each query in every session seen as an individual document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:18<00:00, 3713.11it/s]\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allQueries = list(set(allQueries))\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "    \n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for m in vectors:\n",
    "        if(m.sum() != 0):\n",
    "            listTFIDF.append(m.sum() / m.count_nonzero())\n",
    "        else:\n",
    "            listTFIDF.append(-1)\n",
    "        pbar.update()\n",
    "\n",
    "        \n",
    "VocabTFIDF = pd.DataFrame(data=listTFIDF, columns = ['tfidfS']).fillna(-1)\n",
    "VocabTFIDF['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabTFIDF, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Non-Stereotype\n",
    "\n",
    "In the following block of code we take an 80% sample of all non-stereotype sessions found in SWC, and then calculate the TF-IDF values for each all queries with each query in every session seen as an individual document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:24<00:00, 2813.07it/s]\n"
     ]
    }
   ],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allQueries = list(set(allQueries))\n",
    "allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "sID = allSessionsQ['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "    \n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for m in vectors:\n",
    "        if(m.sum() != 0):\n",
    "            listTFIDF.append(m.sum() / m.count_nonzero())\n",
    "        else:\n",
    "            listTFIDF.append(-1)\n",
    "        pbar.update()\n",
    "    \n",
    "VocabTFIDFNA = pd.DataFrame(data=listTFIDF, columns = ['tfidfNS']).fillna(-1)\n",
    "VocabTFIDFNA['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabTFIDFNA, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "In the following block of code we load up a list of stopwords (not those found in NLTK) and then count the number of stopwords found in each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:05<00:00, 11893.33it/s]\n"
     ]
    }
   ],
   "source": [
    "stopWords = []\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "with open('DataSets/stopwords.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        if (row):\n",
    "            stopWords.append(st.lemmatize(row[0]))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "stopCount = []\n",
    "stopAverage= []\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        count = 0\n",
    "        for word in query.split(' '):\n",
    "            word = word.lower().strip()\n",
    "            word = re.sub(r'[^\\w\\s]','',word)\n",
    "            word = st.lemmatize(word)\n",
    "            if word in stopWords:\n",
    "                count +=1\n",
    "            else:\n",
    "                pass\n",
    "        stopCount.append(count)\n",
    "        stopAverage.append(count/len(query.split(' ')))\n",
    "        pbar.update()\n",
    "    \n",
    "VocabStop = pd.DataFrame(data=stopCount, columns = ['stopCount'])\n",
    "VocabStop['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabStop, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Vocab\n",
    "\n",
    "In the following block of code we count the occurence of individual net vocabulary found in each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:00<00:00, 276000.98it/s]\n"
     ]
    }
   ],
   "source": [
    "www = []\n",
    "com = []\n",
    "net = []\n",
    "org = []\n",
    "gov = []\n",
    "edu = []\n",
    "http = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "\n",
    "        if \"www.\" in query:\n",
    "            www.append(1)\n",
    "        else:\n",
    "            www.append(0)\n",
    "\n",
    "        if \".com\" in query:\n",
    "            com.append(1)\n",
    "        else:\n",
    "            com.append(0)\n",
    "\n",
    "        if \".net\" in query:\n",
    "            net.append(1)\n",
    "        else:\n",
    "            net.append(0)\n",
    "\n",
    "        if \".org\" in query:\n",
    "            org.append(1)\n",
    "        else:\n",
    "            org.append(0)\n",
    "\n",
    "        if \".edu\" in query:\n",
    "            edu.append(1)\n",
    "        else:\n",
    "            edu.append(0)\n",
    "\n",
    "        if \".gov\" in query:\n",
    "            gov.append(1)\n",
    "        else:\n",
    "            gov.append(0)\n",
    "\n",
    "        if \"http\" in query:\n",
    "            http.append(1)\n",
    "        else:\n",
    "            http.append(0)\n",
    "        \n",
    "        pbar.update()\n",
    "        \n",
    "VocabNet = pd.DataFrame(data=com, columns = ['com'])\n",
    "VocabNet['net'] = net\n",
    "VocabNet['org'] = org\n",
    "VocabNet['edu'] = edu\n",
    "VocabNet['gov'] = gov\n",
    "VocabNet['http'] = http\n",
    "VocabNet['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabNet, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Operators\n",
    "\n",
    "In the following block of code we count the occurence of individual search operators found in each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:00<00:00, 457781.48it/s]\n"
     ]
    }
   ],
   "source": [
    "AND = []\n",
    "OR = []\n",
    "quotes = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "\n",
    "        if \"AND\" in query:\n",
    "            AND.append(1)\n",
    "        else:\n",
    "            AND.append(0)\n",
    "\n",
    "        if \"OR\" in query:\n",
    "            OR.append(1)\n",
    "        else:\n",
    "            OR.append(0)\n",
    "\n",
    "        if \"\\\"\" in query:\n",
    "            quotes.append(1) \n",
    "        else:\n",
    "            quotes.append(0)\n",
    "        \n",
    "        pbar.update()\n",
    "            \n",
    "VocabOP = pd.DataFrame(data=AND, columns = ['AND'])\n",
    "VocabOP['OR'] = OR\n",
    "VocabOP['quotes'] = quotes\n",
    "VocabOP['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabOP, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interogatives \n",
    "\n",
    "In the following block of code we determine if a query contains an interogative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70112/70112 [00:02<00:00, 28844.21it/s]\n"
     ]
    }
   ],
   "source": [
    "inter = []\n",
    "VocabInter = pd.DataFrame(data=vocab, columns = ['coreVocab'])\n",
    "\n",
    "x = len(allQueries)\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for num in range(x):\n",
    "        query = allQueries[num]\n",
    "\n",
    "        if re.match(r\"who( |'re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"what( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"when( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"where( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"why( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"how( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"is \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"are \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"can \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"could \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"should \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"would \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        else:\n",
    "            inter.append(0)\n",
    "    \n",
    "        pbar.update()\n",
    "        \n",
    "VocabInter = pd.DataFrame(data=inter, columns = ['inter'])\n",
    "VocabInter['query'] = allQueries\n",
    "Vocab = Vocab.merge(VocabInter, on = 'query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Feature Set\n",
    "\n",
    "Due to the length of this notebook, we have been merging data frames as we go. Therefore, at this point we only have to return the overall feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Vocab, open( \"Pickles/VocabFeat.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
