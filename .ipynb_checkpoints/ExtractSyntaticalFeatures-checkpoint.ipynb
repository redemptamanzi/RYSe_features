{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts syntatical features from the queries found in SWC and SQS, returning a data frame containing those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "The following block of code loads all libraries needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shlex\n",
    "import stanza\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from subprocess import Popen, PIPE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Functions\n",
    "\n",
    "The following block of code declares functions used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates n-grams generated from a string.\n",
    "#\n",
    "# param s: is the string passed into this function\n",
    "# param n: is the n in n-grams\n",
    "# returns: the n-grams\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets\n",
    "\n",
    "This block of code loads the data sets and extracts all unique queries from both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "allSessionsSQS = pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" ) )\n",
    "allQueries = allSessions['query'].tolist() + allSessionsSQS['query'].tolist()  \n",
    "setQueries = set(allQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract D-Level Features\n",
    "\n",
    "The following block of code extracts D-Level features from each query. This code is extremely slow as it is making system calls which execute another block of code. I have encountered difficulties with getting this code to run before, as COLLINS-PARSER/code is compiled C code that may need to be recompiled to ensure compatibility with processor. The solution is to run the make clean, and then make again. Further information about this suite of code can be found at:\n",
    "\n",
    "http://www.personal.psu.edu/xxl13/downloads/d-level.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 13:57:32 WARNING: Can not find tokenize: gsd from official model list. Ignoring it.\n",
      "2021-11-02 13:57:32 WARNING: Can not find pos: bnc from official model list. Ignoring it.\n",
      "2021-11-02 13:57:32 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-11-02 13:57:32 INFO: Use device: cpu\n",
      "2021-11-02 13:57:32 INFO: Loading: tokenize\n",
      "2021-11-02 13:57:32 INFO: Loading: pos\n",
      "2021-11-02 13:57:33 INFO: Loading: lemma\n",
      "2021-11-02 13:57:33 INFO: Loading: depparse\n",
      "2021-11-02 13:57:34 INFO: Loading: sentiment\n",
      "2021-11-02 13:57:35 INFO: Loading: ner\n",
      "2021-11-02 13:57:36 INFO: Done loading processors!\n",
      " 19%|█▉        | 13184/70112 [4:33:53<14:01:53,  1.13it/s]    "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "input_file = 'DLA/data/lemmatize_pos_sentences.tagged'\n",
    "loc_file =  '../../data/lemmatize_pos_sentences.tagged'\n",
    "\n",
    "processor_dict = {\n",
    "    'tokenize': 'gsd',\n",
    "    'pos': 'bnc',\n",
    "    'lemma': 'default'\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors=processor_dict)\n",
    "\n",
    "from tqdm import tqdm\n",
    "with tqdm(total = len(setQueries) ) as pbar:\n",
    "    for text in setQueries:\n",
    "        doc = nlp(text)\n",
    "        out = open(input_file, 'w')\n",
    "        \n",
    "        for sentence in doc.sentences:\n",
    "            s = ''\n",
    "            l = 0\n",
    "            for word in sentence.words:\n",
    "                s+='{} {}'.format(word.lemma, word.xpos) + ' ' # needs to be xpos so it uses Penn Treebank\n",
    "                l+=1\n",
    "            out.write('{} {}\\n'.format(l, s.strip()))\n",
    "        out.close()\n",
    "        \n",
    "        cmd = 'cd DLA/d-level-analyzer/COLLINS-PARSER;'\n",
    "        cmd += ' code/parser {} models/model2/grammar 10000 1 1 1 1 > ../../data/parsed.m2;'.format(loc_file)\n",
    "        cmd += 'cd ..;'\n",
    "        cmd += 'python d-level.py ../data/parsed.m2 > ../data/dlevel.dla;'\n",
    "        proc = subprocess.Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True).wait()\n",
    "        if count == 0:\n",
    "            dl = pd.read_csv('DLA/data/dlevel.dla')\n",
    "            dl['query'] = text\n",
    "            dLevel = dl\n",
    "            count += 1\n",
    "        else:\n",
    "            dl = pd.read_csv('DLA/data/dlevel.dla')\n",
    "            dl['query'] = text\n",
    "            dLevel = dLevel.append(dl)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Part of Speech Features\n",
    "\n",
    "The following block of code first generates part of speech uni-gram, bi-gram, and tri-gram for each query, then takes the top 10 most common bi-grams and top 5 most common tri-grams (was determined be initial research); returning the ratio of all n-grams for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posData = []\n",
    "for document in setQueries:\n",
    "    text = nltk.word_tokenize(document)\n",
    "    tags = np.array(nltk.pos_tag(text)).flatten()\n",
    "    posData.append(tags[1::2])\n",
    "\n",
    "posMod = []\n",
    "\n",
    "for pos in posData: \n",
    "    string = []\n",
    "    for entry in pos:\n",
    "        string += str(entry) + \" \"\n",
    "    posMod.append(\"\".join(string))\n",
    "\n",
    "    \n",
    "posUni = []\n",
    "posBi = []\n",
    "posTri = []\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generate_ngrams(document,1)\n",
    "    posUni.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generate_ngrams(document,2)\n",
    "    posBi.append(doc)\n",
    "\n",
    "for document in posMod:\n",
    "    doc = generate_ngrams(document,3)\n",
    "    posTri.append(doc)  \n",
    "    \n",
    "posDF = pd.DataFrame(setQueries)\n",
    "\n",
    "posDF['all'] = posMod\n",
    "posDF['uniPos'] = posUni\n",
    "posDF['biPos'] = posBi\n",
    "posDF['triPos']= posTri\n",
    "posDF = posDF.rename(columns={0: \"query\"})\n",
    "\n",
    "allSessionsuni = pd.concat([posDF,pd.get_dummies(posDF['uniPos'].apply(pd.Series).stack()).sum(level=0)],axis=1).drop(['uniPos', 'all', 'biPos', 'triPos'],axis=1)\n",
    "allSessionsbi = pd.concat([posDF,pd.get_dummies(posDF['biPos'].apply(pd.Series).stack()).sum(level=0)],axis=1).drop(['biPos', 'uniPos', 'all', 'triPos'],axis=1)\n",
    "allSessionstri = pd.concat([posDF,pd.get_dummies(posDF['triPos'].apply(pd.Series).stack()).sum(level=0)],axis=1).drop(['uniPos', 'all', 'biPos', 'triPos'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessionsbiLanding = allSessionsbi[[\n",
    "'nn nn',\n",
    "'jj nn',\n",
    "'nn nns',\n",
    "'to vb',\n",
    "'jj nns',\n",
    "'jj to',\n",
    "'nn in',\n",
    "'nns in',\n",
    "'in nn',\n",
    "'dt nn',\n",
    "'query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSessionstriLanding = allSessionstri[[\n",
    "'jj nn nn',\n",
    "'nn nn nn',\n",
    "'jj to vb',\n",
    "'nn nn nns',\n",
    "'to vb nn',\n",
    "'query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synFeats = allSessionsuni.merge(allSessionsbiLanding)\n",
    "synFeats = synFeats.merge(allSessionstriLanding)\n",
    "synFeats = synFeats.merge(allSessionstriLanding)\n",
    "synFeats = synFeats.fillna(0)\n",
    "\n",
    "listCols = list(synFeats.columns)\n",
    "listCols.pop(0) ##removes 'query' from the list of columns\n",
    "\n",
    "synFeats['length'] = synFeats['query'].str.split().str.len()\n",
    "\n",
    "\n",
    "for col in listCols:\n",
    "    synFeats[col] = synFeats[col]/synFeats['length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Feature Set\n",
    "\n",
    "Combines all data frames into one, preprocesses out extraneous information and returns the cleaned data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synFeats = synFeats.merge(dLevel, on = 'query')\n",
    "synFeats.drop(columns = [' Sentences', 'length', 'Filename'], inplace = True)\n",
    "pickle.dump(synFeats, open( \"Pickles/SynFeat.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
