{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code in this notebook extracts vocabulary features from each query and then returns a data frame of those extracted features. Only **SQS dataset** used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/assoumerredempta/Documents/aSpring_2023/RYSe_Final/FeatureExtraction'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries\n",
    "\n",
    "The following block of code loads all libraries needed for this notebook. Numpy has an established to ensure that the random selection of queries drawn to establish certain features, such as top word n-grams; is consistent across this code and future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(20200522)\n",
    "\n",
    "stopwords = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Vocabulary Features\n",
    "\n",
    "Features used in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a list into a dictionary.\n",
    "# param lst: is the list that is convert into dictionary\n",
    "# returns resDct: the converted list\n",
    "\n",
    "def convert(lst):\n",
    "    \n",
    "    resDct = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "    \n",
    "    return resDct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets\n",
    "\n",
    "This block of code loads the data sets and extracts all unique queries from both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allSessionsSQS = pickle.load( open( \"../Data/DataSets/SQS/casttrecSQS.p\", \"rb\" ) )\n",
    "\n",
    "allSessionsSQS = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "allQueries = allSessionsSQS['query'].tolist()\n",
    "qID = allSessionsSQS['sID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>class</th>\n",
       "      <th>sID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0</td>\n",
       "      <td>6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scooter brands</td>\n",
       "      <td>0</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>0</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scooter</td>\n",
       "      <td>0</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>0</td>\n",
       "      <td>6221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>House of dreams</td>\n",
       "      <td>1</td>\n",
       "      <td>5975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>1</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>7864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>1</td>\n",
       "      <td>5316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>kid</td>\n",
       "      <td>1</td>\n",
       "      <td>8397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query  class   sID\n",
       "0                   US civil war causes      0  6352\n",
       "1                        scooter brands      0  8305\n",
       "2               scooter brands reliable      0  6814\n",
       "3                               scooter      0  7688\n",
       "4                         scooter cheap      0  6221\n",
       "...                                 ...    ...   ...\n",
       "4741                    House of dreams      1  5975\n",
       "4742  When did Desmond doss get married      1  5233\n",
       "4743                                  H      1  7864\n",
       "4744                find fact about dog      1  5316\n",
       "4745                                kid      1  8397\n",
       "\n",
       "[4746 rows x 3 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSessionsSQS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSessionsSQS['sID'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -- add a column of randomly selected unique values to represent qID that can be used to represent sID\n",
    "# n = len(allSessionsSQS)\n",
    "# allSessionsSQS['sID'] = random.sample(range(1,n+1),n)\n",
    "# allQueries = allSessionsSQS['query'].tolist()\n",
    "# qID = allSessionsSQS['sID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4746"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allSessionsSQS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>class</th>\n",
       "      <th>sID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0</td>\n",
       "      <td>6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scooter brands</td>\n",
       "      <td>0</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>0</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scooter</td>\n",
       "      <td>0</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>0</td>\n",
       "      <td>6221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>House of dreams</td>\n",
       "      <td>1</td>\n",
       "      <td>5975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>1</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>7864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>1</td>\n",
       "      <td>5316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>kid</td>\n",
       "      <td>1</td>\n",
       "      <td>8397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query  class   sID\n",
       "0                   US civil war causes      0  6352\n",
       "1                        scooter brands      0  8305\n",
       "2               scooter brands reliable      0  6814\n",
       "3                               scooter      0  7688\n",
       "4                         scooter cheap      0  6221\n",
       "...                                 ...    ...   ...\n",
       "4741                    House of dreams      1  5975\n",
       "4742  When did Desmond doss get married      1  5233\n",
       "4743                                  H      1  7864\n",
       "4744                find fact about dog      1  5316\n",
       "4745                                kid      1  8397\n",
       "\n",
       "[4746 rows x 3 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSessionsSQS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss = np.random.randint(1, 1000, size=1000)\n",
    "# ss = pd.DataFrame(ss, columns=['random_numbers'])\n",
    "# ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ss['random_numbers'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss['rn'] = random.sample(range(1,1001),1000)\n",
    "# ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ss['rn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss['random_numbers'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random.sample(range(1, 11), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSessionsSQS['query'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4746"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Vocab\n",
    "\n",
    "Loads all vocabulary expected to be learned between Kindergarten to Seventh grade based on Common Core Curriculum, before extracting the ratio of words in each query that are, and are not; found in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 18293.47it/s]\n"
     ]
    }
   ],
   "source": [
    "kd = ['a', 'all', 'am', 'an', 'and', 'are', 'as', 'at', 'away', 'back', 'ball', 'bell', 'big', 'bird', 'blue', 'book', 'boot', 'box', 'boy', 'brown', 'but', 'by', 'can', 'car', 'cat', 'come', 'cow', 'day', 'do', 'dog', 'down', 'end', 'fall', 'fan', 'fish', 'fly', 'food', 'for', 'from', 'fun', 'get', 'go', 'good', 'gray', 'green', 'groundhog', 'hat', 'he', 'here', 'hill', 'I', 'in', 'into', 'is', 'it', 'inside', 'kitten', 'little', 'look', 'mad', 'me', 'mud', 'my', 'name', 'no', 'not', 'of', 'on', 'orange', 'out', 'paint', 'pet', 'pin', 'play', 'put', 'rain', 'red', 'run', 'sad', 'say', 'see', 'she', 'sing', 'sit', 'so', 'stay', 'stop', 'story', 'sun', 'take', 'that', 'the', 'them', 'then', 'there', 'they', 'this', 'to', 'too', 'up', 'we', 'wet', 'what', 'where', 'who', 'will', 'with', 'work', 'yellow', 'yes', 'you', 'zoo', 'orange', 'white', 'black', 'monday', 'tuesday', 'wednesday','thursday','friday', 'saturday','sunday']\n",
    "oned = ['a', 'all', 'am', 'and', 'at', 'ball', 'be', 'bed', 'big', 'book', 'box', 'boy', 'but', 'came', 'can', 'car', 'cat', 'come', 'cow', 'dad', 'day', 'did', 'do', 'dog', 'fat', 'for', 'fun', 'get', 'go', 'good', 'got', 'had', 'hat', 'he', 'hen', 'here', 'him', 'his', 'home', 'hot', 'I', 'if', 'in', 'into', 'is', 'it', 'its', 'let', 'like', 'look', 'man', 'may', 'me', 'mom', 'my', 'no', 'not', 'of', 'oh', 'old', 'on', 'one', 'out', 'pan', 'pet', 'pig', 'play', 'ran', 'rat', 'red', 'ride', 'run', 'sat', 'see', 'she', 'sit', 'six', 'so', 'stop', 'sun', 'ten', 'the', 'this', 'to', 'top', 'toy', 'two', 'up', 'us', 'was', 'we', 'will', 'yes', 'you' ]\n",
    "twod = ['about', 'add', 'after', 'ago', 'an ', 'any', 'apple', 'are ', 'as', 'ask', 'ate', 'away', 'baby ', 'back', 'bad', 'bag', 'base', 'bat', 'bee', 'been', 'before', 'being', 'best', 'bike', 'bill', 'bird', 'black', 'blue', 'boat', 'both', 'bring', 'brother ', 'brown', 'bus', 'buy ', 'by', 'cake', 'call', 'candy', 'change', 'child', 'city', 'clean', 'club', 'coat', 'cold', 'coming ', 'corn', 'could', 'cry', 'cup', 'cut', 'daddy ', 'dear', 'deep', 'deer', 'doing', 'doll', 'door', 'down ', 'dress', 'drive', 'drop', 'dry', 'duck', 'each', 'eat', 'eating', 'egg', 'end', 'fall', 'far', 'farm', 'fast', 'father ', 'feed', 'feel', 'feet', 'fell ', 'find', 'fine ', 'fire', 'first ', 'fish', 'five', 'fix', 'flag', 'floor', 'fly', 'food', 'foot', 'four', 'fox', 'from ', 'full', 'funny', 'game', 'gas', 'gave', 'girl', 'give', 'glad', 'goat', 'goes ', 'going ', 'gold', 'gone', 'grade ', 'grass', 'green', 'grow', 'hand', 'happy', 'hard', 'has ', 'have ', 'hear ', 'help', 'here ', 'hill', 'hit', 'hold', 'hole', 'hop', 'hope ', 'horse', 'house ', 'how ', 'ice', 'inch', 'inside ', 'job', 'jump', 'just ', 'keep', 'king', 'know ', 'lake', 'land', 'last', 'late', 'lay', 'left', 'leg', 'light', 'line', 'little ', 'live', 'lives', 'long', 'looking', 'lost', 'lot', 'love', 'mad', 'made ', 'make ', 'many ', 'meat', 'men', 'met', 'mile', 'milk', 'mine', 'miss', 'moon', 'more', 'most', 'mother ', 'move', 'much ', 'must', 'myself ', 'nail', 'name ', 'need', 'new ', 'next', 'nice ', 'night', 'nine', 'north', 'now ', 'nut', 'off ', 'only', 'open', 'or ', 'other', 'our', 'outside ', 'over', 'page', 'park', 'part', 'pay', 'pick', 'plant', 'playing', 'pony', 'post', 'pull', 'put', 'rabbit', 'rain', 'read', 'rest', 'riding', 'road', 'rock', 'room', 'said ', 'same', 'sang', 'saw ', 'say', 'school ', 'sea', 'seat', 'seem', 'seen', 'send', 'set', 'seven', 'sheep', 'ship', 'shoe', 'show ', 'sick', 'side', 'sing', 'sky', 'sleep', 'small', 'snow', 'some ', 'soon ', 'spell', 'start', 'stay', 'still', 'store ', 'story', 'take', 'talk', 'tall', 'teach', 'tell', 'than ', 'thank', 'that', 'them ', 'then ', 'there ', 'they ', 'thing', 'think ', 'three', 'time ', 'today ', 'told', 'too ', 'took', 'train ', 'tree', 'truck', 'try', 'use', 'very ', 'walk', 'want ', 'warm', 'wash', 'way', 'week', 'well ', 'went ', 'were ', 'wet', 'what', 'when ', 'while ', 'white', 'who', 'why', 'wind', 'wish', 'with ', 'woke', 'wood', 'work', 'yellow', 'yet', 'your', 'zoo']\n",
    "threed = ['able', 'above', 'afraid', 'afternoon', 'again', 'age', 'air', 'airplane', 'almost', 'alone', 'along', 'already', 'also', 'always', 'animal', 'another', 'anything', 'around', 'art', 'aunt', 'balloon', 'bark', 'barn', 'basket', 'beach', 'bear', 'because', 'become', 'began', 'begin', 'behind', 'believe', 'below', 'belt', 'better', 'birthday', 'body', 'bones', 'born', 'bought', 'bread', 'bright', 'broke', 'brought', 'busy', 'cabin', 'cage', 'camp', 'can\\'t', 'care', 'carry', 'catch', 'cattle', 'cave', 'children', 'class', 'close', 'cloth', 'coal', 'color', 'corner', 'cotton', 'cover', 'dark', 'desert', 'didn\\'t', 'dinner', 'dishes', 'does', 'done', 'don\\'t', 'dragon', 'draw', 'dream', 'drink', 'early', 'earth', 'east', 'eight', 'even', 'ever', 'every', 'everyone', 'everything', 'eyes', 'face', 'family', 'feeling', 'felt', 'few', 'fight', 'fishing', 'flower', 'flying', 'follow', 'forest', 'forgot', 'form', 'found', 'fourth', 'free', 'Friday', 'friend', 'front', 'getting', 'given', 'grandmother', 'great', 'grew', 'ground', 'guess', 'hair', 'half', 'having', 'head', 'heard', 'he\\'s', 'heat', 'hello', 'high', 'himself', 'hour', 'hundred', 'hurry', 'hurt', 'I\\'d', 'I\\'ll', 'I\\'m', 'inches', 'isn\\'t', 'it\\'s', 'I\\'ve', 'kept', 'kids', 'kind', 'kitten', 'knew', 'knife', 'lady', 'large', 'largest', 'later', 'learn', 'leave', 'let\\'s', 'letter', 'life', 'list', 'living', 'lovely', 'loving', 'lunch', 'mail', 'making', 'maybe', 'mean', 'merry', 'might', 'mind', 'money', 'month', 'morning', 'mouse', 'mouth', 'Mr.', 'Mrs.', 'Ms.', 'music', 'near', 'nearly', 'never', 'news', 'noise', 'nothing', 'number', 'o\\'clock', 'often', 'oil', 'once', 'orange', 'order', 'own', 'pair', 'paint', 'paper', 'party', 'pass', 'past', 'penny', 'people', 'person', 'picture', 'place', 'plan', 'plane', 'please', 'pocket', 'point', 'poor', 'race', 'reach', 'reading', 'ready', 'real', 'rich', 'right', 'river', 'rocket', 'rode', 'round', 'rule', 'running', 'salt', 'says', 'sending', 'sent', 'seventh', 'sew', 'shall', 'short', 'shot', 'should', 'sight', 'sister', 'sitting', 'sixth', 'sled', 'smoke', 'soap', 'someone', 'something', 'sometime', 'song', 'sorry', 'sound', 'south', 'space', 'spelling', 'spent', 'sport', 'spring', 'stairs', 'stand', 'state', 'step', 'stick', 'stood', 'stopped', 'stove', 'street', 'strong', 'study', 'such', 'sugar', 'summer', 'Sunday', 'supper', 'table', 'taken', 'taking', 'talking', 'teacher', 'team', 'teeth', 'tenth', 'that\\'s', 'their', 'these', 'thinking', 'third', 'those', 'thought', 'throw', 'tonight', 'trade', 'trick', 'trip', 'trying', 'turn', 'twelve', 'twenty', 'uncle', 'under', 'upon', 'wagon', 'wait', 'walking', 'wasn\\'t', 'watch', 'water', 'weather', 'we\\'re', 'west', 'wheat', 'where', 'which', 'wife', 'wild', 'win', 'window', 'winter', 'without', 'woman', 'won', 'won\\'t', 'wool', 'word', 'working', 'world', 'would', 'write', 'wrong', 'yard', 'year', 'yesterday', 'you\\'re'  ]\n",
    "fourd = ['across', 'against', 'answer', 'awhile', 'between', 'board', 'bottom', 'breakfast', 'broken', 'build', 'building', 'built', 'captain', 'carried', 'caught', 'charge', 'chicken', 'circus', 'cities', 'clothes', 'company', 'couldn\\'t', 'country', 'discover', 'doctor', 'doesn\\'t', 'dollar', 'during', 'eighth', 'else', 'enjoy', 'enough', 'everybody', 'example', 'except', 'excuse', 'field', 'fifth', 'finish', 'following', 'good-by', 'group', 'happened', 'harden', 'haven\\'t', 'heavy', 'held', 'hospital', 'idea', 'instead', 'known', 'laugh', 'middle', 'minute', 'mountain', 'ninth', 'ocean', 'office', 'parent', 'peanut', 'pencil', 'picnic', 'police', 'pretty', 'prize', 'quite', 'radio', 'raise', 'really', 'reason', 'remember', 'return', 'Saturday', 'scare', 'second', 'since', 'slowly', 'stories', 'student', 'sudden', 'suit', 'sure', 'swimming', 'though', 'threw', 'tired', 'together', 'tomorrow', 'toward', 'tried', 'trouble', 'truly', 'turtle', 'until', 'village', 'visit', 'wear', 'we\\'ll', 'whole', 'whose', 'women', 'wouldn\\'t', 'writing', 'written', 'wrote', 'yell', 'young']\n",
    "fived = ['although', 'America', 'among', 'arrive', 'attention', 'beautiful', 'countries', 'course', 'cousin', 'decide', 'different', 'evening', 'favorite', 'finally', 'future', 'happiest', 'happiness', 'important', 'interest', 'piece', 'planet', 'present', 'president', 'principal', 'probably', 'problem', 'receive', 'sentence', 'several', 'special', 'suddenly', 'suppose', 'surely', 'surprise', 'they\\'re', 'through', 'usually', 'action', 'actor', 'actually', 'addition', 'agreed', 'allowed', 'aloud', 'amendment', 'amount', 'amusement', 'annual', 'appointed', 'arrange', 'attention', 'awhile', 'beginning', 'bruise', 'business', 'calves', 'capital', 'capitol', 'captain', 'carefully', 'caught', 'cause', 'celebrate', 'century', 'chemical', 'chocolate', 'circle', 'climate', 'climbed', 'collar', 'column', 'company', 'condition', 'consider', 'consonant', 'constant', 'continent', 'continued', 'country', 'course', 'crystal', 'current', 'curtain', 'daughter', 'daytime', 'decided', 'decimal', 'delicious', 'desert', 'dessert', 'details', 'determine', 'dictionary', 'difference', 'different', 'difficult', 'direction', 'disappoint', 'division', 'eighth', 'election', 'elements', 'energy', 'enjoyment', 'equal', 'equation', 'errands', 'exact', 'except', 'expect', 'explain', 'explode', 'express', 'factory', 'fault', 'favorite', 'finally', 'finished', 'forward', 'fought', 'fraction', 'furniture', 'future', 'general', 'government', 'graceful', 'graph', 'grasp', 'grease', 'grown-ups', 'guest', 'guide', 'happened', 'happily', 'harvest', 'healthy', 'height', 'hoarse', 'human', 'idea', 'imagine', 'include', 'increase', 'indicate', 'information', 'instrument', 'intention', 'interesting', 'inventor', 'island', 'jewel', 'journey', 'jungle', 'knives', 'known', 'language', 'laughter', 'length', 'limb', 'located', 'lumber', 'major', 'mammal', 'manufacture', 'material', 'mayor', 'measure', 'melody', 'members', 'memories', 'message', 'method', 'million', 'minor', 'modern', 'mountain', 'music', 'natural', 'necessary', 'neither', 'newspaper', 'northern', 'notebook', 'notice', 'noun', 'numeral', 'object', 'observe', 'opposite', 'orphan', 'ought', 'outside', 'oxygen', 'paid', 'paint', 'paragraph', 'pattern', 'pause', 'payment', 'perhaps', 'period', 'permit', 'phone', 'phrase', 'pleasant', 'pleasure', 'plural', 'poison', 'position', 'possible', 'practice', 'prepared', 'president', 'probably', 'problem', 'process', 'produce', 'program', 'promise', 'property', 'protection', 'provide', 'puzzle', 'quickly', 'quietly', 'radio', 'raise', 'rarely', 'rather', 'reached', 'receive', 'record', 'region', 'relax', 'remain', 'remove', 'repay', 'repeat', 'report', 'represent', 'respond', 'result', 'rhythm', 'rising', 'ruin', 'salad', 'sandal', 'scale', 'scent', 'schedule', 'science', 'section', 'separate', 'service', 'settled', 'several', 'shadow', 'shelter', 'shoulder', 'shouted', 'shower', 'signal', 'similar', 'sincerely', 'single', 'size', 'slippery', 'soar', 'soil', 'solution', 'solve', 'southern', 'split', 'spoiled', 'sports', 'square', 'squeeze', 'stain', 'state', 'statement', 'station', 'steer', 'stomach', 'stopping', 'straight', 'straighten', 'stream', 'stretched', 'suggest', 'suitcase', 'sunset', 'supply', 'sure', 'surface', 'surprise', 'surround', 'sweater', 'syllable', 'syrup', 'tablet', 'tasty', 'teaspoon', 'terrible', 'though', 'thoughtful', 'thrown', 'tornado', 'toward', 'traffic', 'trail', 'treasure', 'treatment', 'triangle', 'trouble', 'tunnel', 'type', 'understood', 'unknown', 'usually', 'value', 'various', 'warn', 'weigh', 'weight', 'weird', 'western', 'whisper', 'whoever', 'whole', 'whose', 'wives', 'women', 'wonderful', 'wound', 'wreck', 'x-ray', 'yesterday']\n",
    "sixd = ['Abandon', 'abundant', 'access', 'accommodate', 'accumulate', 'adapt', 'adhere', 'agony', 'allegiance', 'ambition', 'ample', 'anguish', 'anticipate', 'anxious', 'apparel', 'appeal', 'apprehensive', 'arid', 'arrogant', 'awe', 'Barren', 'beacon', 'beneficial', 'blunder', 'boisterous', 'boycott', 'burden', 'Campaign', 'capacity', 'capital', 'chronological', 'civic', 'clarity', 'collaborate', 'collide', 'commend', 'commentary', 'compact', 'composure', 'concise', 'consent', 'consequence', 'conserve', 'conspicuous', 'constant', 'contaminate', 'context', 'continuous', 'controversy', 'convenient', 'cope', 'cordial', 'cultivate', 'cumulative', '', 'Declare', 'deluge', 'dense', 'deplete', 'deposit', 'designate', 'desperate', 'deteriorate', 'dialogue', 'diligent', 'diminish', 'discretion', 'dissent', 'dissolve', 'distinct', 'diversity', 'domestic', 'dominate', 'drastic', 'duration', 'dwell', 'Eclipse', 'economy', 'eerie', 'effect', 'efficient', 'elaborate', 'eligible', 'elude', 'encounter', 'equivalent', 'erupt', 'esteem', 'evolve', 'exaggerate', 'excel', 'exclude', 'expanse', 'exploit', 'extinct', 'extract', 'Factor', 'former', 'formulates', 'fuse', 'futile', 'Generate', 'genre', 'Habitat', 'hazardous', 'hoax', 'hostile', 'Idiom', 'ignite', 'immense', 'improvises', 'inept', 'inevitable', 'influence', 'ingenious', 'innovation', 'intimidate', 'Jovial', 'Knack', 'Leeway', 'legislation', 'leisure', 'liberate', 'likeness', 'linger', 'literal', 'loathe', 'lure', 'Majority', 'makeshift', 'manipulate', 'marvel', 'massive', 'maximum', 'meager', 'mere', 'migration', 'mimic', 'minute', 'monotonous', 'Negotiate', 'Objective', 'obstacle', 'omniscient', 'onset', 'optimist', 'originate', 'Painstaking', 'paraphrase', 'parody', 'persecute', 'plummet', 'possess', 'poverty', 'precise', 'predicament', 'predict', 'prejudice', 'preliminary', 'primitive', 'priority', 'prominent', 'propel', 'prosecute', 'prosper', 'provoke', 'pursue', 'Quest', 'Recount', 'refuge', 'reinforce', 'reluctant', 'remorse', 'remote', 'resolute', 'restrain', 'retaliate', 'retrieve', 'rigorous', 'rural', 'Salvage', 'sanctuary', 'siege', 'significant', 'solar', 'soothe', 'stationary', 'stifle', 'strive', 'subordinate', 'subsequent', 'superior', 'supplement', 'swarm', 'Tangible', 'terminate', 'terrain', 'trait', 'transform', 'transport', 'treacherous', 'Unanimous', 'unique', 'unruly', 'urban', 'Vacate', 'verdict', 'verge', 'vibrant', 'vital', 'vow', 'accept', 'accidentally', 'acquire', 'ambulance', 'ancient', 'appearance', 'appointment', 'arithmetic', 'audience', 'autumn', 'beautifully', 'beliefs', 'blown', 'bough', 'bows', 'calendar', 'canyon', 'capable', 'capacity', 'caution', 'ceiling', 'champion', 'choir', 'cleanse', 'combination', 'comfortable', 'community', 'complain', 'concentration', 'concern', 'connection', 'constitution', 'contagious', 'conversation', 'cooperation', 'correct', 'coupon', 'creative', 'creature', 'crisis', 'culture', 'curious', 'dangerous', 'decision', 'demonstrate', 'denominator', 'department', 'departure', 'depth', 'descendant', 'disagreement', 'disastrous', 'discussion', 'distance', 'distributed', 'earliest', 'echoes', 'edition', 'educate', 'electricity', 'element', 'elevator', 'emergency', 'employer', 'emptiness', 'encouragement', 'encyclopedia', 'entire', 'entrance', 'envelope', 'equator', 'especially', 'establish', 'example', 'excellent', 'excitement', 'exercise', 'experience', 'exterior', 'familiar', 'faucet', 'fierce', 'fireproof', 'following', 'forgetting', 'forgiveness', 'fossil', 'freight', 'frighten', 'fuel', 'further', 'gallon', 'gaze', 'gesture', 'governor', 'graduation', 'grateful', 'grief', 'halves', 'hamburger', 'hangar', 'hanger', 'happiness', 'headache', 'heroes', 'history', 'honorable', 'horizon', 'hunger', 'hyphen', 'ignore', 'imagination', 'immediate', 'importance', 'improvement', 'independence', 'ingredient', 'injury', 'inquire', 'instead', 'instruction', 'intermission', 'interview', 'invisible', 'invitation', 'involve', 'jealous', 'junior', 'knowledge', 'lawyer', 'league', 'legal', 'liberty', 'liquid', 'listening', 'loaves', 'location', 'luggage', 'manager', 'manner', 'manor', 'marriage', 'meant', 'mechanic', 'medicine', 'mention', 'minus', 'minute', 'mistaken', 'misunderstand', 'mixture', 'mourn', 'multiple', 'muscle', 'museum', 'musician', 'mute', 'myth', 'nationality', 'negative', 'noisy', 'noticeable', 'novel', 'numerator', 'obtain', 'occur', 'official', 'operate', 'original', 'outline', 'partial', 'passenger', 'patient', 'penalty', 'penguin', 'percent', 'performance', 'personal', 'persuade', 'physical', 'piano', 'plumber', 'poem', 'poet', 'policy', 'pollute', 'pollution', 'positive', 'potatoes', 'predict', 'prefer', 'pressure', 'prevent', 'principal', 'private', 'project', 'pumpkins', 'purchase', 'purse', 'quote', 'radius', 'rapid', 'ratio', 'realize', 'recently', 'recycle', 'reduce', 'referred', 'regardless', 'regular', 'rehearse', 'relief', 'relieve', 'remarkable', 'remind', 'remote', 'replacement', 'replied', 'reply', 'requirement', 'rescue', 'resident', 'resources', 'respectful', 'review', 'roam', 'routine', 'rumor', 'rural', 'safety', 'sailor', 'salute', 'satisfy', 'scarcely', 'scientific', 'scissors', 'selection', 'senior', 'sentence', 'separately', 'serious', 'session', 'shampoo', 'shelves', 'shorten', 'silent', 'simply', 'sketch', 'skillful', 'solar', 'sought', 'spaghetti', 'sponge', 'squawk', 'storage', 'strain', 'strategy', 'strength', 'strive', 'struggle', 'studios', 'success', 'suggestion', 'support', 'surrounded', 'sword', 'system', 'telephone', 'television', 'temperature', 'theme', 'themselves', 'therefore', 'thicken', 'thousand', 'threat', 'tomatoes', 'trophies', 'tutor', 'unbelievable', 'underneath', 'unite', 'vacuum', 'vain', 'variety', 'vary', 'vault', 'vegetable', 'vein', 'violence', 'visible', 'vision', 'waste', 'who\\'s', 'whose', 'wrestle', 'wrinkle', 'yield']\n",
    "# sevend = ['abbreviation', 'absence', 'absolutely', 'absorb', 'abundant', 'accessible', 'accompanied', 'accomplishment', 'accurate', 'achievement', 'acres', 'adequate', 'adjustable', 'admit', 'admittance', 'advice', 'advise', 'afghan', 'alternate', 'alternative', 'amusement', 'analysis', 'analyze', 'ancestor', 'anniversary', 'appreciate', 'artificial', 'assistance', 'association', 'athlete', 'atmosphere', 'attendance', 'authority', 'bacteria', 'bagel', 'baggage', 'benefited', 'benefiting', 'bicycle', 'biscuit', 'bizarre', 'boulevard', 'boundary', 'bouquet', 'brilliant', 'brochure', 'bulletin', 'bureau', 'campaign', 'cancellation', 'candidate', 'capable', 'capital', 'capitol', 'category', 'celery', 'cemetery', 'changeable', 'chaperone', 'character', 'cinnamon', 'civilize', 'commercial', 'committed', 'committee', 'commotion', 'companion', 'competent', 'competition', 'complement', 'complex', 'compliment', 'compressor', 'concentrate', 'concentration', 'conductor', 'confetti', 'congratulations', 'consequently', 'controlling', 'cringe', 'culminate', 'culprit', 'deceive', 'delayed', 'democracy', 'deodorant', 'descendent', 'description', 'diameter', 'diamond', 'discourage', 'disgraceful', 'dismissal', 'distinguished', 'dreadful', 'economics', 'economy', 'elementary', 'embarrass', 'emotion', 'emphasize', 'encircle', 'enclosing', 'encounter', 'endurance', 'engineer', 'environment', 'episode', 'erosion', 'eruption', 'evident', 'exchange', 'executive', 'exhibit', 'expensive', 'extinct', 'extinguish', 'extraordinary', 'extremely', 'fabricate', 'failure', 'fascinating', 'fatigue', 'flagrant', 'foreign', 'forfeit', 'frequently', 'fundamental', 'genuine', 'ghetto', 'gossiping', 'gradual', 'graffiti', 'grammar', 'grievance', 'guarantee', 'harass', 'havoc', 'heroic', 'hesitate', 'horrify', 'hospital', 'humid', 'humility', 'hygiene', 'identical', 'idle', 'idol', 'illegal', 'illustration', 'imaginary', 'immediately', 'immobilize', 'impossibility', 'inconvenient', 'incredible', 'individual', 'infamous', 'influence', 'informant', 'inhabit', 'inherit', 'innocence', 'innocent', 'instructor', 'intelligent', 'interruption', 'introduction', 'involvement', 'irate', 'irresistible', 'jealousy', 'judgment', 'juvenile', 'kettle', 'knitting', 'laboratory', 'language', 'legibly', 'liquidation', 'management', 'maneuver', 'media', 'mileage', 'miniature', 'misbehaved', 'morale', 'mortgage', 'movement', 'murmur', 'musician', 'mysterious', 'negotiate', 'nervous', 'nuisance', 'nurture', 'oases', 'oasis', 'obedient', 'obstacle', 'obviously', 'occasion', 'ordinarily', 'ordinary', 'organization', 'pamphlet', 'panic', 'panicked', 'panicky', 'parallel', 'paralysis', 'paralyze', 'penicillin', 'pedestrian', 'phantom', 'pheasant', 'phrase', 'politely', 'popular', 'precipitation', 'principal', 'principle', 'privilege', 'procedure', 'pronunciation', 'psychology', 'puny', 'qualified', 'qualifying', 'quotation', 'raspberry', 'reasonable', 'receipt', 'receiving', 'recipe', 'recognition', 'recommend', 'recruit', 'reddest', 'reprimand', 'resigned', 'restaurant', 'rotten', 'sandwich', 'scarcity', 'scenery', 'secretary', 'securing', 'significance', 'simile', 'sincerely', 'sincerity', 'situation', 'skeptical', 'slumber', 'smudge', 'solemn', 'souvenir', 'spacious', 'specific', 'stationary', 'stationery', 'statistics', 'subscription', 'substitute', 'superintendent', 'supervisor', 'supposedly', 'threatening', 'tolerate', 'tongue', 'tournament', 'tragedy', 'traitor', 'transferred', 'transferring', 'transmitted', 'traveled', 'traveling', 'unfortunately', 'uniform', 'university', 'unnecessary', 'valuable', 'various', 'vehicle', 'version', 'vertical', 'victim', 'vigorously', 'violation', 'visualize', 'volcano', 'voyage', 'wealthy', 'weapon', 'wheeze', 'wilderness', 'Abate', 'abnormal', 'abode', 'abrupt', 'accelerate', 'acclaim', 'acknowledge', 'acquire', 'aspire', 'acrid', 'addict', 'adjacent', 'admonish', 'affliction', 'agitate', 'ajar', 'akin', 'allege', 'annihilate', 'anonymous', 'antagonize', 'apathy', 'arbitrate', 'astute', 'authentic', 'avert', 'Bellow', 'beseech', 'bestow', 'bewilder', 'bigot', 'blatant', 'bleak', 'braggart', 'brawl', 'browse', 'bystander', 'Candid', 'canine', 'canny', 'capricious', 'capsize', 'casual', 'casualty', 'catastrophe', 'cater', 'chorus', 'citrus', 'clamber', 'climax', 'compromise', 'concur', 'confront', 'congested', 'conjure', 'consult', 'corrupt', 'counterfeit', 'covet', 'customary', 'Debut', 'deceased', 'dependent', 'despondent', 'detach', 'devour', 'dishearten', 'dismal', 'dismantle', 'distraught', 'docile', 'downright', 'drone', 'dumbfound', 'Emblem', 'endure', 'ensue', 'enthrall', 'epidemic', 'erode', 'exuberant', 'Fathom', 'feud', 'figment', 'firebrand', 'flabbergast', 'flagrant', 'flaw', 'fruitless', 'Gaudy', 'geography', 'gratify', 'gravity', 'grim', 'grimy', 'grueling', 'gruesome', 'Haggle', 'headlong', 'hilarious', 'homage', 'homicide', 'hospitable', 'hurtle', 'hybrid', 'Illiterate', 'impede', 'implore', 'incident', 'incredulous', 'infamous', 'infuriate', 'insinuate', 'intensified', 'inundate', 'irate', 'Lavish', 'legacy', 'legitimate', 'lethal', 'loath', 'lurk', 'Magnetic', 'mirth', 'quench', 'magnitude', 'maternal', 'maul', 'melancholy', 'mellow', 'momentum', 'mortify', 'mull', 'murky', 'Narrative', 'negligent', 'nimble', 'nomadic', 'noteworthy', 'notify', 'notorious', 'nurture', 'Obnoxious', 'oration', 'orthodox', 'overwhelm', 'Pamper', 'patronize', 'peevish', 'pelt', 'pending', 'perceived', 'perjury', 'permanent', 'persist', 'perturb', 'pique', 'pluck', 'poised', 'ponder', 'potential', 'predatory', 'presume', 'preview', 'prior', 'prowess', 'Radiant', 'random', 'rant', 'recede', 'reprimand', 'resume', 'retort', 'robust', 'rupture', 'Saga', 'sequel', 'sham', 'shirk', 'simultaneously', 'snare', 'species', 'status', 'stodgy', 'substantial', 'subtle', 'sullen', 'supervise', 'Tamper', 'throb', 'toxic', 'tragedy', 'trickle', 'trivial', 'Uncertainty', 'unscathed', 'upright', 'urgent', 'utmost', 'Vengeance', 'vicious', 'vindictive', 'vista', 'vocation', 'void', 'Wary', 'whim', 'wince', 'wrath', 'Yearn']\n",
    "\n",
    "# Note: seven is taken out because the age we are considering \n",
    "\n",
    "coreVocab = []\n",
    "vocab = []\n",
    "nonVocab = []\n",
    "\n",
    "st = WordNetLemmatizer()\n",
    "\"\"\"\n",
    "The following lammatize the splitted words and put them in lower case \n",
    "\n",
    "\"\"\"\n",
    "for word in kd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()])) \n",
    "for word in oned:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in twod:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in threed:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in fourd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in fived:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "for word in sixd:\n",
    "    coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "# for word in sevend:\n",
    "#     coreVocab.append(\" \".join([st.lemmatize(i.lower()) for i in word.split()]))\n",
    "    \n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "\n",
    "        splitQuery = [st.lemmatize(i.lower()) for i in query.split(' ')]\n",
    "\n",
    "        queryVocab = 0\n",
    "        nonqueryVocab = 0\n",
    "        totalVocab = 0\n",
    "\n",
    "        for word in splitQuery:\n",
    "            if word in coreVocab:\n",
    "                queryVocab  +=1\n",
    "                totalVocab  +=1\n",
    "            else:\n",
    "                nonqueryVocab +=1\n",
    "                totalVocab  +=1\n",
    "\n",
    "        vocab.append(queryVocab/totalVocab) \n",
    "        nonVocab.append(nonqueryVocab/totalVocab) \n",
    "        pbar.update() \n",
    "\n",
    "Vocab = pd.DataFrame(data=vocab, columns = ['coreVocab'])\n",
    "Vocab['query'] = allQueries\n",
    "Vocab['nonCoreVocab'] = nonVocab\n",
    "# Vocab['qID'] = qID\n",
    "# Vocab = Vocab.set_index('query')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coreVocab                query  nonCoreVocab\n",
       "0        0.5  US civil war causes           0.5\n",
       "1        0.0       scooter brands           1.0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 3)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not there\n"
     ]
    }
   ],
   "source": [
    "if \"results\" in coreVocab:\n",
    "    print(\"it's there\")\n",
    "else:\n",
    "    print(\"not there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Vocab =  1780\n"
     ]
    }
   ],
   "source": [
    "print(\"Core Vocab = \", len(coreVocab))\n",
    "# print( \"Sum words in all grades = \", len(kd + sevend + oned + twod + threed + fourd + fived + sixd ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>1.00</td>\n",
       "      <td>House of dreams</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>0.50</td>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>0.00</td>\n",
       "      <td>H</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>0.75</td>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>1.00</td>\n",
       "      <td>kid</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      coreVocab                              query  nonCoreVocab\n",
       "0          0.50                US civil war causes          0.50\n",
       "1          0.00                     scooter brands          1.00\n",
       "2          0.00            scooter brands reliable          1.00\n",
       "3          0.00                            scooter          1.00\n",
       "4          0.00                      scooter cheap          1.00\n",
       "...         ...                                ...           ...\n",
       "4741       1.00                    House of dreams          0.00\n",
       "4742       0.50  When did Desmond doss get married          0.50\n",
       "4743       0.00                                  H          1.00\n",
       "4744       0.75                find fact about dog          0.25\n",
       "4745       1.00                                kid          0.00\n",
       "\n",
       "[4746 rows x 3 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4746"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocab['coreVocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1780"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kd + oned + twod + threed + fourd + fived + sixd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment:\n",
    "\n",
    "# 1st query coreVocab is 0.5 because 'consulting' is in core vocab\n",
    "# Sum of coreVocab and nonCoreVocab should = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age of Acquisition features\n",
    "\n",
    "In this block of code we first load up the Age of Acquistion data set (which is a csv with multiple columns representing a variety of information) and process it into a dictionary where the key is the word, and the value is AoA rating. We then find the AoA rating for each word in the query, extracting the min, max, average (known as query complexity), and ratio of words expected to be learned by the age of 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>20415.27</td>\n",
       "      <td>Article</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacus</td>\n",
       "      <td>abacus</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Noun</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>abacus</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abacuses</td>\n",
       "      <td>abacuses</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>abacus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abalone</td>\n",
       "      <td>abalone</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Verb</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>abalone</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Alternative.spelling   Freq_pm Dom_PoS_SUBTLEX  Nletters  Nphon  \\\n",
       "0         a                    a  20415.27         Article         1      1   \n",
       "1  aardvark             aardvark      0.41            Noun         8      7   \n",
       "2    abacus               abacus      0.24            Noun         6      6   \n",
       "3  abacuses             abacuses      0.02            Noun         8      9   \n",
       "4   abalone              abalone      0.51            Verb         7      7   \n",
       "\n",
       "   Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  Perc_known_lem  \\\n",
       "0      1                 a     2.89        1.00         2.89            1.00   \n",
       "1      2          aardvark     9.89        1.00         9.89            1.00   \n",
       "2      3            abacus     8.69        0.65         8.69            0.65   \n",
       "3      4            abacus      NaN         NaN         8.69            0.65   \n",
       "4      4           abalone    12.23        0.72        12.23            0.72   \n",
       "\n",
       "   AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "0          3.16              NaN           NaN         NaN  \n",
       "1           NaN              NaN           NaN         NaN  \n",
       "2           NaN              NaN           NaN         NaN  \n",
       "3           NaN              NaN           NaN         NaN  \n",
       "4           NaN              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the info in the data\n",
    "\n",
    "# the words expected to be known at a certain age\n",
    "\n",
    "dtAoA = pd.read_csv('DataSets/AoA/AoA_51715_words.csv')\n",
    "dtAoA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24706    4.4\n",
       "Name: AoA_Kup_lem, dtype: float64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtAoA.loc[dtAoA['Lemma_highest_PoS']=='it', 'AoA_Kup_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9667    10.95\n",
       "9672    10.95\n",
       "9673    10.95\n",
       "9674    10.95\n",
       "Name: AoA_Kup_lem, dtype: float64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtAoA.loc[dtAoA['Lemma_highest_PoS']=='consult', 'AoA_Kup_lem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 38669.87it/s]\n"
     ]
    }
   ],
   "source": [
    "AoAvocab = [] # a word and it's corresponding age\n",
    "\n",
    "with open('DataSets/AoA/AoA_51715_words.csv') as csvFile:\n",
    "    csvReader = csv.reader(csvFile)\n",
    "    lineCount = 0\n",
    "    for row in csvReader:\n",
    "        if lineCount == 0:\n",
    "            lineCount += 1\n",
    "        else:\n",
    "            AoAvocab.append(row[7]) # row[7]: Column (Lemma_highest_PoS) with lemma of the words\n",
    "            AoAvocab.append(row[10]) # row[10]: Column \n",
    "            \n",
    "AoAVConv = convert(AoAvocab) # make 'AoAvocab' a dict of words and corresponding age\n",
    "\n",
    "minAoA = []\n",
    "maxAoA = []\n",
    "averageVocab = []\n",
    "ratioAoA = []\n",
    "\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        count = 0\n",
    "        vocab = []\n",
    "\n",
    "        for word in query.split(' '):\n",
    "            word = word.lower().strip()\n",
    "            word = re.sub(r'[^\\w\\s]','',word) \n",
    "            word = st.lemmatize(word)\n",
    "            if word in AoAVConv:\n",
    "                vocab.append(float(AoAVConv[word])) # if a word in query is found in listed vocabulary we save it's value\n",
    "            else:\n",
    "                vocab.append(0)\n",
    "\n",
    "\n",
    "        vocab = np.array(vocab)\n",
    "        \n",
    "        if vocab.size == 0:\n",
    "            minAoA.append(-1) \n",
    "            maxAoA.append(-1) \n",
    "            averageVocab.append(-1)\n",
    "            ratioAoA.append(0)\n",
    "        elif vocab.size > 0:\n",
    "            minAoA.append(np.min(vocab))\n",
    "            maxAoA.append(np.max(vocab))\n",
    "            averageVocab.append(np.mean(vocab))\n",
    "            for entry in vocab:\n",
    "                if entry < 13 and entry > 0: # 13 = limited age\n",
    "                    count +=1\n",
    "            ratioAoA.append(count/len(vocab))\n",
    "        \n",
    "        pbar.update()\n",
    "\n",
    "Vocab['minAoA'] = minAoA\n",
    "Vocab['maxAoA'] = maxAoA\n",
    "Vocab['ratioAoA'] = ratioAoA\n",
    "Vocab['queryComplexity'] = averageVocab # mean of age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 7)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start verify ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.28"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " '2.89',\n",
       " 'aardvark',\n",
       " '9.89',\n",
       " 'abacus',\n",
       " '8.69',\n",
       " 'abacus',\n",
       " '8.69',\n",
       " 'abalone',\n",
       " '12.23',\n",
       " 'abalone',\n",
       " '12.23',\n",
       " 'abandon',\n",
       " '8.32',\n",
       " 'abandon',\n",
       " '8.32',\n",
       " 'abandoner',\n",
       " '11.89',\n",
       " 'abandon',\n",
       " '8.32',\n",
       " 'abandonment',\n",
       " '10.27',\n",
       " 'abandon',\n",
       " '8.32',\n",
       " 'abase',\n",
       " '14.57',\n",
       " 'abasement',\n",
       " '15.13',\n",
       " 'abate',\n",
       " '14.44',\n",
       " 'abate',\n",
       " '14.44',\n",
       " 'abatement',\n",
       " '15.12',\n",
       " 'abate',\n",
       " '14.44',\n",
       " 'abate',\n",
       " '14.44',\n",
       " 'abattoir',\n",
       " '15.17',\n",
       " 'abbacy',\n",
       " '14.50',\n",
       " 'abbess',\n",
       " '15.43',\n",
       " 'abbess',\n",
       " '15.43',\n",
       " 'abbey',\n",
       " '13.06',\n",
       " 'abbot',\n",
       " '12.10',\n",
       " 'abbreviate',\n",
       " '9.95',\n",
       " 'abbreviated',\n",
       " '10.50',\n",
       " 'abbreviation',\n",
       " '9.11',\n",
       " 'abbreviation',\n",
       " '9.11',\n",
       " 'abdicate',\n",
       " '12.60',\n",
       " 'abdicate',\n",
       " '12.60',\n",
       " 'abdicate',\n",
       " '12.60',\n",
       " 'abdicate',\n",
       " '12.60',\n",
       " 'abdication',\n",
       " '14.94',\n",
       " 'abdomen',\n",
       " '8.61',\n",
       " 'abdomen',\n",
       " '8.61',\n",
       " 'abdominal',\n",
       " '10.24',\n",
       " 'abduct',\n",
       " '11.26',\n",
       " 'abduct',\n",
       " '11.26',\n",
       " 'abduct',\n",
       " '11.26',\n",
       " 'abduction',\n",
       " '11.94',\n",
       " 'abduction',\n",
       " '11.94',\n",
       " 'abductor',\n",
       " '11.11',\n",
       " 'abductor',\n",
       " '11.11',\n",
       " 'abduct',\n",
       " '11.26',\n",
       " 'abeam',\n",
       " '13.40',\n",
       " 'aberrant',\n",
       " '13.31',\n",
       " 'aberration',\n",
       " '12.69',\n",
       " 'aberration',\n",
       " '12.69',\n",
       " 'abet',\n",
       " '12.15',\n",
       " 'abet',\n",
       " '12.15',\n",
       " 'abet',\n",
       " '12.15',\n",
       " 'abettor',\n",
       " '15.55',\n",
       " 'abeyance',\n",
       " '15.00',\n",
       " 'abhor',\n",
       " '13.76',\n",
       " 'abhor',\n",
       " '13.76',\n",
       " 'abhorrent',\n",
       " '12.14',\n",
       " 'abhor',\n",
       " '13.76',\n",
       " 'abide',\n",
       " '9.50',\n",
       " 'abide',\n",
       " '9.50',\n",
       " 'abide',\n",
       " '9.50',\n",
       " 'abiding',\n",
       " '10.30',\n",
       " 'ability',\n",
       " '8.84',\n",
       " 'ability',\n",
       " '8.84',\n",
       " 'abject',\n",
       " '14.00',\n",
       " 'abjuration',\n",
       " '17.12',\n",
       " 'abjure',\n",
       " '14.60',\n",
       " 'ablation',\n",
       " '13.29',\n",
       " 'ablaze',\n",
       " '10.83',\n",
       " 'able',\n",
       " '7.79',\n",
       " 'able',\n",
       " '7.79',\n",
       " 'able',\n",
       " '7.79',\n",
       " 'ablution',\n",
       " '13.64',\n",
       " 'ablution',\n",
       " '13.64',\n",
       " 'abnormal',\n",
       " '10.05',\n",
       " 'abnormality',\n",
       " '11.58',\n",
       " 'abnormality',\n",
       " '11.58',\n",
       " 'aboard',\n",
       " '8.66',\n",
       " 'abode',\n",
       " '12.06',\n",
       " 'abolish',\n",
       " '10.42',\n",
       " 'abolish',\n",
       " '10.42',\n",
       " 'abolish',\n",
       " '10.42',\n",
       " 'abolish',\n",
       " '10.42',\n",
       " 'abolition',\n",
       " '13.28',\n",
       " 'abolitionism',\n",
       " '11.78',\n",
       " 'abolitionist',\n",
       " '12.60',\n",
       " 'abolitionist',\n",
       " '12.60',\n",
       " 'abominable',\n",
       " '8.94',\n",
       " 'abominate',\n",
       " '15.00',\n",
       " 'abomination',\n",
       " '11.17',\n",
       " 'abomination',\n",
       " '11.17',\n",
       " 'aboriginal',\n",
       " '14.75',\n",
       " 'aboriginal',\n",
       " '14.75',\n",
       " 'aborigine',\n",
       " '12.56',\n",
       " 'aborigine',\n",
       " '12.56',\n",
       " 'aborning',\n",
       " '13.25',\n",
       " 'abort',\n",
       " '12.21',\n",
       " 'abort',\n",
       " '12.21',\n",
       " 'abort',\n",
       " '12.21',\n",
       " 'abortion',\n",
       " '13.21',\n",
       " 'abortionist',\n",
       " '15.00',\n",
       " 'abortionist',\n",
       " '15.00',\n",
       " 'abortion',\n",
       " '13.21',\n",
       " 'abortive',\n",
       " '13.93',\n",
       " 'abort',\n",
       " '12.21',\n",
       " 'abound',\n",
       " '11.65',\n",
       " 'abound',\n",
       " '11.65',\n",
       " 'abound',\n",
       " '11.65',\n",
       " 'abound',\n",
       " '11.65',\n",
       " 'about',\n",
       " '5.07',\n",
       " 'above',\n",
       " '4.55',\n",
       " 'aboveboard',\n",
       " '13.00',\n",
       " 'aboveground',\n",
       " '8.05',\n",
       " 'abovementioned',\n",
       " '14.24',\n",
       " 'abracadabra',\n",
       " '6.89',\n",
       " 'abrade',\n",
       " '14.00',\n",
       " 'abrade',\n",
       " '14.00',\n",
       " 'abrasion',\n",
       " '11.95',\n",
       " 'abrasion',\n",
       " '11.95',\n",
       " 'abrasive',\n",
       " '12.22',\n",
       " 'abrasiveness',\n",
       " '12.06',\n",
       " 'abrasive',\n",
       " '12.22',\n",
       " 'abreaction',\n",
       " '15.80',\n",
       " 'abreast',\n",
       " '13.06',\n",
       " 'abridge',\n",
       " '14.35',\n",
       " 'abridged',\n",
       " '12.56',\n",
       " 'abridgement',\n",
       " '12.32',\n",
       " 'abridge',\n",
       " '14.35',\n",
       " 'abroad',\n",
       " '10.15',\n",
       " 'abrogate',\n",
       " '15.22',\n",
       " 'abrogate',\n",
       " '15.22',\n",
       " 'abrupt',\n",
       " '10.95',\n",
       " 'abruption',\n",
       " '11.82',\n",
       " 'abruptness',\n",
       " '12.39',\n",
       " 'abscess',\n",
       " '13.11',\n",
       " 'abscess',\n",
       " '13.11',\n",
       " 'abscond',\n",
       " '13.36',\n",
       " 'abscond',\n",
       " '13.36',\n",
       " 'absconder',\n",
       " '16.62',\n",
       " 'absconder',\n",
       " '16.62',\n",
       " 'absence',\n",
       " '7.70',\n",
       " 'absence',\n",
       " '7.70',\n",
       " 'absent',\n",
       " '6.50',\n",
       " 'absentee',\n",
       " '11.00',\n",
       " 'absenteeism',\n",
       " '13.43',\n",
       " 'absentee',\n",
       " '11.00',\n",
       " 'absent',\n",
       " '6.50',\n",
       " 'absentminded',\n",
       " '10.84',\n",
       " 'absentmindedness',\n",
       " '9.61',\n",
       " 'absinthe',\n",
       " '16.29',\n",
       " 'absinthe',\n",
       " '16.29',\n",
       " 'absolute',\n",
       " '8.53',\n",
       " 'absolutely',\n",
       " '7.08',\n",
       " 'absoluteness',\n",
       " '12.68',\n",
       " 'absolute',\n",
       " '8.53',\n",
       " 'absolution',\n",
       " '11.71',\n",
       " 'absolve',\n",
       " '12.90',\n",
       " 'absolve',\n",
       " '12.90',\n",
       " 'absolver',\n",
       " '13.60',\n",
       " 'absolve',\n",
       " '12.90',\n",
       " 'absolve',\n",
       " '12.90',\n",
       " 'absorb',\n",
       " '8.83',\n",
       " 'absorb',\n",
       " '8.83',\n",
       " 'absorbent',\n",
       " '10.32',\n",
       " 'absorber',\n",
       " '10.33',\n",
       " 'absorber',\n",
       " '10.33',\n",
       " 'absorb',\n",
       " '8.83',\n",
       " 'absorb',\n",
       " '8.83',\n",
       " 'absorption',\n",
       " '10.94',\n",
       " 'absorption',\n",
       " '10.94',\n",
       " 'abstain',\n",
       " '12.95',\n",
       " 'abstain',\n",
       " '12.95',\n",
       " 'abstain',\n",
       " '12.95',\n",
       " 'abstain',\n",
       " '12.95',\n",
       " 'abstention',\n",
       " '15.19',\n",
       " 'abstention',\n",
       " '15.19',\n",
       " 'abstinence',\n",
       " '12.74',\n",
       " 'abstract',\n",
       " '11.47',\n",
       " 'abstract',\n",
       " '11.47',\n",
       " 'abstraction',\n",
       " '13.65',\n",
       " 'abstractionist',\n",
       " '14.72',\n",
       " 'abstraction',\n",
       " '13.65',\n",
       " 'abstract',\n",
       " '11.47',\n",
       " 'abstruse',\n",
       " '14.60',\n",
       " 'absurd',\n",
       " '10.50',\n",
       " 'absurdist',\n",
       " '13.00',\n",
       " 'absurdity',\n",
       " '13.21',\n",
       " 'absurdity',\n",
       " '13.21',\n",
       " 'abundance',\n",
       " '10.45',\n",
       " 'abundant',\n",
       " '12.84',\n",
       " 'abuse',\n",
       " '8.61',\n",
       " 'abuse',\n",
       " '8.61',\n",
       " 'abuser',\n",
       " '9.94',\n",
       " 'abuser',\n",
       " '9.94',\n",
       " 'abuse',\n",
       " '8.61',\n",
       " 'abuse',\n",
       " '8.61',\n",
       " 'abusive',\n",
       " '9.58',\n",
       " 'abusiveness',\n",
       " '10.47',\n",
       " 'abut',\n",
       " '13.11',\n",
       " 'abutment',\n",
       " '15.33',\n",
       " 'abutment',\n",
       " '15.33',\n",
       " 'abut',\n",
       " '13.11',\n",
       " 'abuzz',\n",
       " '13.00',\n",
       " 'abysm',\n",
       " '15.25',\n",
       " 'abysmal',\n",
       " '13.78',\n",
       " 'abyss',\n",
       " '11.00',\n",
       " 'abyssal',\n",
       " '17.27',\n",
       " 'abyss',\n",
       " '11.00',\n",
       " 'ac',\n",
       " '10.75',\n",
       " 'acacia',\n",
       " '14.25',\n",
       " 'academe',\n",
       " '13.27',\n",
       " 'academe',\n",
       " '13.27',\n",
       " 'academia',\n",
       " '13.74',\n",
       " 'academic',\n",
       " '9.74',\n",
       " 'academician',\n",
       " '15.21',\n",
       " 'academicism',\n",
       " '15.00',\n",
       " 'academic',\n",
       " '9.74',\n",
       " 'academy',\n",
       " '9.05',\n",
       " 'academy',\n",
       " '9.05',\n",
       " 'acanthus',\n",
       " '15.17',\n",
       " 'accede',\n",
       " '14.31',\n",
       " 'accelerando',\n",
       " '14.50',\n",
       " 'accelerant',\n",
       " '12.84',\n",
       " 'accelerate',\n",
       " '10.74',\n",
       " 'accelerated',\n",
       " '10.11',\n",
       " 'accelerate',\n",
       " '10.74',\n",
       " 'accelerate',\n",
       " '10.74',\n",
       " 'acceleration',\n",
       " '9.84',\n",
       " 'accelerator',\n",
       " '10.27',\n",
       " 'accelerator',\n",
       " '10.27',\n",
       " 'accent',\n",
       " '8.60',\n",
       " 'accent',\n",
       " '8.60',\n",
       " 'accent',\n",
       " '8.60',\n",
       " 'accentuate',\n",
       " '13.24',\n",
       " 'accentuate',\n",
       " '13.24',\n",
       " 'accentuate',\n",
       " '13.24',\n",
       " 'accept',\n",
       " '6.74',\n",
       " 'acceptable',\n",
       " '9.84',\n",
       " 'acceptance',\n",
       " '8.56',\n",
       " 'acceptance',\n",
       " '8.56',\n",
       " 'acceptant',\n",
       " '11.89',\n",
       " 'accept',\n",
       " '6.74',\n",
       " 'accept',\n",
       " '6.74',\n",
       " 'accept',\n",
       " '6.74',\n",
       " 'access',\n",
       " '9.10',\n",
       " 'access',\n",
       " '9.10',\n",
       " 'access',\n",
       " '9.10',\n",
       " 'accessibility',\n",
       " '11.05',\n",
       " 'accessible',\n",
       " '9.94',\n",
       " 'access',\n",
       " '9.10',\n",
       " 'accession',\n",
       " '12.25',\n",
       " 'accessory',\n",
       " '11.15',\n",
       " 'accessorize',\n",
       " '11.39',\n",
       " 'accessorize',\n",
       " '11.39',\n",
       " 'accessorize',\n",
       " '11.39',\n",
       " 'accessory',\n",
       " '11.15',\n",
       " 'accident',\n",
       " '5.30',\n",
       " 'accidental',\n",
       " '6.75',\n",
       " 'accidental',\n",
       " '6.75',\n",
       " 'accident',\n",
       " '5.30',\n",
       " 'acclaim',\n",
       " '12.11',\n",
       " 'acclaim',\n",
       " '12.11',\n",
       " 'acclamation',\n",
       " '14.00',\n",
       " 'acclimate',\n",
       " '14.24',\n",
       " 'acclimate',\n",
       " '14.24',\n",
       " 'acclimate',\n",
       " '14.24',\n",
       " 'acclimatization',\n",
       " '16.12',\n",
       " 'acclimatize',\n",
       " '14.00',\n",
       " 'accolade',\n",
       " '12.43',\n",
       " 'accolade',\n",
       " '12.43',\n",
       " 'accommodate',\n",
       " '9.90',\n",
       " 'accommodate',\n",
       " '9.90',\n",
       " 'accommodate',\n",
       " '9.90',\n",
       " 'accommodate',\n",
       " '9.90',\n",
       " 'accommodation',\n",
       " '9.58',\n",
       " 'accommodation',\n",
       " '9.58',\n",
       " 'accommodative',\n",
       " '15.18',\n",
       " 'accompany',\n",
       " '10.75',\n",
       " 'accompany',\n",
       " '10.75',\n",
       " 'accompaniment',\n",
       " '11.18',\n",
       " 'accompanist',\n",
       " '12.78',\n",
       " 'accompanist',\n",
       " '12.78',\n",
       " 'accompany',\n",
       " '10.75',\n",
       " 'accompany',\n",
       " '10.75',\n",
       " 'accomplice',\n",
       " '9.78',\n",
       " 'accomplice',\n",
       " '9.78',\n",
       " 'accomplish',\n",
       " '7.84',\n",
       " 'accomplish',\n",
       " '7.84',\n",
       " 'accomplish',\n",
       " '7.84',\n",
       " 'accomplish',\n",
       " '7.84',\n",
       " 'accomplishment',\n",
       " '9.00',\n",
       " 'accomplishment',\n",
       " '9.00',\n",
       " 'accord',\n",
       " '9.65',\n",
       " 'accordance',\n",
       " '11.75',\n",
       " 'accord',\n",
       " '9.65',\n",
       " 'according',\n",
       " '8.45',\n",
       " 'accordion',\n",
       " '8.61',\n",
       " 'accordion',\n",
       " '8.61',\n",
       " 'accord',\n",
       " '9.65',\n",
       " 'accost',\n",
       " '13.37',\n",
       " 'accost',\n",
       " '13.37',\n",
       " 'accost',\n",
       " '13.37',\n",
       " 'account',\n",
       " '8.26',\n",
       " 'accountability',\n",
       " '12.16',\n",
       " 'accountable',\n",
       " '11.70',\n",
       " 'accountancy',\n",
       " '14.80',\n",
       " 'accountant',\n",
       " '11.67',\n",
       " 'accountant',\n",
       " '11.67',\n",
       " 'account',\n",
       " '8.26',\n",
       " 'accounting',\n",
       " '12.05',\n",
       " 'account',\n",
       " '8.26',\n",
       " 'accoutrement',\n",
       " '13.87',\n",
       " 'accoutrement',\n",
       " '13.87',\n",
       " 'accreditation',\n",
       " '15.53',\n",
       " 'accredited',\n",
       " '13.89',\n",
       " 'accrete',\n",
       " '14.00',\n",
       " 'accrual',\n",
       " '15.00',\n",
       " 'accrue',\n",
       " '14.11',\n",
       " 'accrue',\n",
       " '14.11',\n",
       " 'accrue',\n",
       " '14.11',\n",
       " 'accumulate',\n",
       " '12.30',\n",
       " 'accumulate',\n",
       " '12.30',\n",
       " 'accumulate',\n",
       " '12.30',\n",
       " 'accumulate',\n",
       " '12.30',\n",
       " 'accumulation',\n",
       " '10.50',\n",
       " 'accumulation',\n",
       " '10.50',\n",
       " 'accumulative',\n",
       " '12.50',\n",
       " 'accumulator',\n",
       " '13.94',\n",
       " 'accuracy',\n",
       " '9.71',\n",
       " 'accurate',\n",
       " '9.44',\n",
       " 'accursed',\n",
       " '12.71',\n",
       " 'accusation',\n",
       " '10.17',\n",
       " 'accusation',\n",
       " '10.17',\n",
       " 'accusative',\n",
       " '13.33',\n",
       " 'accusatory',\n",
       " '11.18',\n",
       " 'accuse',\n",
       " '9.10',\n",
       " 'accuse',\n",
       " '9.10',\n",
       " 'accuser',\n",
       " '11.72',\n",
       " 'accuser',\n",
       " '11.72',\n",
       " 'accuse',\n",
       " '9.10',\n",
       " 'accuse',\n",
       " '9.10',\n",
       " 'accustom',\n",
       " '12.00',\n",
       " 'ace',\n",
       " '8.83',\n",
       " 'ace',\n",
       " '8.83',\n",
       " 'acerb',\n",
       " '15.67',\n",
       " 'acerbic',\n",
       " '16.62',\n",
       " 'acerbity',\n",
       " '15.75',\n",
       " 'ace',\n",
       " '8.83',\n",
       " 'acetaminophen',\n",
       " '13.47',\n",
       " 'acetate',\n",
       " '12.85',\n",
       " 'acetate',\n",
       " '12.85',\n",
       " 'acetic',\n",
       " '13.75',\n",
       " 'acetone',\n",
       " '14.05',\n",
       " 'acetyl',\n",
       " '16.31',\n",
       " 'acetylcholine',\n",
       " '16.30',\n",
       " 'acetylene',\n",
       " '16.88',\n",
       " 'acetylsalicylic',\n",
       " '17.71',\n",
       " 'ache',\n",
       " '5.79',\n",
       " 'ache',\n",
       " '5.79',\n",
       " 'ache',\n",
       " '5.79',\n",
       " 'achievable',\n",
       " '9.89',\n",
       " 'achieve',\n",
       " '8.53',\n",
       " 'achieve',\n",
       " '8.53',\n",
       " 'achievement',\n",
       " '8.80',\n",
       " 'achievement',\n",
       " '8.80',\n",
       " 'achiever',\n",
       " '9.11',\n",
       " 'achiever',\n",
       " '9.11',\n",
       " 'achieve',\n",
       " '8.53',\n",
       " 'achieve',\n",
       " '8.53',\n",
       " 'achiness',\n",
       " '10.11',\n",
       " 'achy',\n",
       " '8.06',\n",
       " 'acid',\n",
       " '9.60',\n",
       " 'acidic',\n",
       " '11.21',\n",
       " 'acidity',\n",
       " '12.78',\n",
       " 'acidophilus',\n",
       " '17.40',\n",
       " 'acidosis',\n",
       " '16.85',\n",
       " 'acid',\n",
       " '9.60',\n",
       " 'acidulous',\n",
       " '15.43',\n",
       " 'ace',\n",
       " '8.83',\n",
       " 'acknowledge',\n",
       " '9.11',\n",
       " 'acknowledge',\n",
       " '9.11',\n",
       " 'acknowledgement',\n",
       " '9.95',\n",
       " 'acknowledgement',\n",
       " '9.95',\n",
       " 'acknowledge',\n",
       " '9.11',\n",
       " 'acknowledge',\n",
       " '9.11',\n",
       " 'acknowledgement',\n",
       " '9.95',\n",
       " 'acknowledgement',\n",
       " '9.95',\n",
       " 'acme',\n",
       " '9.54',\n",
       " 'acne',\n",
       " '11.40',\n",
       " 'acne',\n",
       " '11.40',\n",
       " 'acolyte',\n",
       " '13.77',\n",
       " 'acolyte',\n",
       " '13.77',\n",
       " 'aconitum',\n",
       " '11.00',\n",
       " 'acorn',\n",
       " '5.95',\n",
       " 'acorn',\n",
       " '5.95',\n",
       " 'acoustic',\n",
       " '12.39',\n",
       " 'acoustical',\n",
       " '12.90',\n",
       " 'acoustics',\n",
       " '11.32',\n",
       " 'achiever',\n",
       " '9.11',\n",
       " 'acquaintance',\n",
       " '10.48',\n",
       " 'acquaintance',\n",
       " '10.48',\n",
       " 'acquaintanceship',\n",
       " '14.18',\n",
       " 'acquiesce',\n",
       " '14.25',\n",
       " 'acquiesce',\n",
       " '14.25',\n",
       " 'acquiescence',\n",
       " '13.61',\n",
       " 'acquiescent',\n",
       " '14.18',\n",
       " 'acquiesce',\n",
       " '14.25',\n",
       " 'acquire',\n",
       " '9.94',\n",
       " 'acquire',\n",
       " '9.94',\n",
       " 'acquirer',\n",
       " '11.79',\n",
       " 'acquire',\n",
       " '9.94',\n",
       " 'acquire',\n",
       " '9.94',\n",
       " 'acquisition',\n",
       " '13.63',\n",
       " 'acquisition',\n",
       " '13.63',\n",
       " 'acquisitive',\n",
       " '14.19',\n",
       " 'acquisitiveness',\n",
       " '14.38',\n",
       " 'acquit',\n",
       " '13.22',\n",
       " 'acquittal',\n",
       " '12.39',\n",
       " 'acquittal',\n",
       " '12.39',\n",
       " 'acquit',\n",
       " '13.22',\n",
       " 'acquit',\n",
       " '13.22',\n",
       " 'acre',\n",
       " '9.80',\n",
       " 'acreage',\n",
       " '13.13',\n",
       " 'acre',\n",
       " '9.80',\n",
       " 'acrid',\n",
       " '14.07',\n",
       " 'acrimonious',\n",
       " '16.30',\n",
       " 'acrimony',\n",
       " '14.75',\n",
       " 'acrobat',\n",
       " '8.05',\n",
       " 'acrobatic',\n",
       " '9.38',\n",
       " 'acrobatics',\n",
       " '9.00',\n",
       " 'acrobat',\n",
       " '8.05',\n",
       " 'acronym',\n",
       " '11.56',\n",
       " 'acronym',\n",
       " '11.56',\n",
       " 'acrophobia',\n",
       " '14.38',\n",
       " 'acropolis',\n",
       " '15.40',\n",
       " 'across',\n",
       " '6.64',\n",
       " 'acrostic',\n",
       " '14.82',\n",
       " 'acrylic',\n",
       " '11.56',\n",
       " 'act',\n",
       " '6.42',\n",
       " 'act',\n",
       " '6.42',\n",
       " 'act',\n",
       " '6.42',\n",
       " 'actinic',\n",
       " '15.62',\n",
       " 'actinium',\n",
       " 'NA',\n",
       " 'action',\n",
       " '6.67',\n",
       " 'actionable',\n",
       " '12.50',\n",
       " 'action',\n",
       " '6.67',\n",
       " 'activate',\n",
       " '11.11',\n",
       " 'activate',\n",
       " '11.11',\n",
       " 'activate',\n",
       " '11.11',\n",
       " 'activate',\n",
       " '11.11',\n",
       " 'activation',\n",
       " '9.78',\n",
       " 'activator',\n",
       " '11.50',\n",
       " 'active',\n",
       " '6.53',\n",
       " 'active',\n",
       " '6.53',\n",
       " 'activism',\n",
       " '11.89',\n",
       " 'activist',\n",
       " '11.80',\n",
       " 'activist',\n",
       " '11.80',\n",
       " 'activity',\n",
       " '6.47',\n",
       " 'activity',\n",
       " '6.47',\n",
       " 'actor',\n",
       " '7.17',\n",
       " 'actor',\n",
       " '7.17',\n",
       " 'actress',\n",
       " '6.17',\n",
       " 'actress',\n",
       " '6.17',\n",
       " 'act',\n",
       " '6.42',\n",
       " 'actual',\n",
       " '6.94',\n",
       " 'actuality',\n",
       " '12.38',\n",
       " 'actuality',\n",
       " '12.38',\n",
       " 'actualization',\n",
       " '13.84',\n",
       " 'actualize',\n",
       " '13.61',\n",
       " 'actually',\n",
       " '7.33',\n",
       " 'actuarial',\n",
       " '15.85',\n",
       " 'actuary',\n",
       " '17.27',\n",
       " 'actuary',\n",
       " '17.27',\n",
       " 'actuation',\n",
       " '14.69',\n",
       " 'actuator',\n",
       " '13.71',\n",
       " 'actuator',\n",
       " '13.71',\n",
       " 'acuity',\n",
       " '13.88',\n",
       " 'acumen',\n",
       " '14.44',\n",
       " 'acupressure',\n",
       " '14.44',\n",
       " 'acupuncture',\n",
       " '11.79',\n",
       " 'acupuncturist',\n",
       " '14.61',\n",
       " 'acupuncturist',\n",
       " '14.61',\n",
       " 'acute',\n",
       " '11.60',\n",
       " 'acuteness',\n",
       " '12.95',\n",
       " 'acyclovir',\n",
       " '12.57',\n",
       " 'ad',\n",
       " '8.22',\n",
       " 'adage',\n",
       " '16.21',\n",
       " 'adage',\n",
       " '16.21',\n",
       " 'adagio',\n",
       " '12.62',\n",
       " 'adamant',\n",
       " '14.29',\n",
       " 'adamantine',\n",
       " '15.91',\n",
       " 'adapt',\n",
       " '9.00',\n",
       " 'adaptability',\n",
       " '11.65',\n",
       " 'adaptable',\n",
       " '9.37',\n",
       " 'adaptation',\n",
       " '11.39',\n",
       " 'adaptation',\n",
       " '11.39',\n",
       " 'adapt',\n",
       " '9.00',\n",
       " 'adapter',\n",
       " '11.79',\n",
       " 'adapt',\n",
       " '9.00',\n",
       " 'adaption',\n",
       " '12.40',\n",
       " ...]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoAvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': '2.89',\n",
       " 'aardvark': '9.89',\n",
       " 'abacus': '8.69',\n",
       " 'abalone': '12.23',\n",
       " 'abandon': '8.32',\n",
       " 'abandoner': '11.89',\n",
       " 'abandonment': '10.27',\n",
       " 'abase': '14.57',\n",
       " 'abasement': '15.13',\n",
       " 'abate': '14.44',\n",
       " 'abatement': '15.12',\n",
       " 'abattoir': '15.17',\n",
       " 'abbacy': '14.50',\n",
       " 'abbess': '15.43',\n",
       " 'abbey': '13.06',\n",
       " 'abbot': '12.10',\n",
       " 'abbreviate': '9.95',\n",
       " 'abbreviated': '10.50',\n",
       " 'abbreviation': '9.11',\n",
       " 'abdicate': '12.60',\n",
       " 'abdication': '14.94',\n",
       " 'abdomen': '8.61',\n",
       " 'abdominal': '10.24',\n",
       " 'abduct': '11.26',\n",
       " 'abduction': '11.94',\n",
       " 'abductor': '11.11',\n",
       " 'abeam': '13.40',\n",
       " 'aberrant': '13.31',\n",
       " 'aberration': '12.69',\n",
       " 'abet': '12.15',\n",
       " 'abettor': '15.55',\n",
       " 'abeyance': '15.00',\n",
       " 'abhor': '13.76',\n",
       " 'abhorrent': '12.14',\n",
       " 'abide': '9.50',\n",
       " 'abiding': '10.30',\n",
       " 'ability': '8.84',\n",
       " 'abject': '14.00',\n",
       " 'abjuration': '17.12',\n",
       " 'abjure': '14.60',\n",
       " 'ablation': '13.29',\n",
       " 'ablaze': '10.83',\n",
       " 'able': '7.79',\n",
       " 'ablution': '13.64',\n",
       " 'abnormal': '10.05',\n",
       " 'abnormality': '11.58',\n",
       " 'aboard': '8.66',\n",
       " 'abode': '12.06',\n",
       " 'abolish': '10.42',\n",
       " 'abolition': '13.28',\n",
       " 'abolitionism': '11.78',\n",
       " 'abolitionist': '12.60',\n",
       " 'abominable': '8.94',\n",
       " 'abominate': '15.00',\n",
       " 'abomination': '11.17',\n",
       " 'aboriginal': '14.75',\n",
       " 'aborigine': '12.56',\n",
       " 'aborning': '13.25',\n",
       " 'abort': '12.21',\n",
       " 'abortion': '13.21',\n",
       " 'abortionist': '15.00',\n",
       " 'abortive': '13.93',\n",
       " 'abound': '11.65',\n",
       " 'about': '5.07',\n",
       " 'above': '4.55',\n",
       " 'aboveboard': '13.00',\n",
       " 'aboveground': '8.05',\n",
       " 'abovementioned': '14.24',\n",
       " 'abracadabra': '6.89',\n",
       " 'abrade': '14.00',\n",
       " 'abrasion': '11.95',\n",
       " 'abrasive': '12.22',\n",
       " 'abrasiveness': '12.06',\n",
       " 'abreaction': '15.80',\n",
       " 'abreast': '13.06',\n",
       " 'abridge': '14.35',\n",
       " 'abridged': '12.56',\n",
       " 'abridgement': '12.32',\n",
       " 'abroad': '10.15',\n",
       " 'abrogate': '15.22',\n",
       " 'abrupt': '10.95',\n",
       " 'abruption': '11.82',\n",
       " 'abruptness': '12.39',\n",
       " 'abscess': '13.11',\n",
       " 'abscond': '13.36',\n",
       " 'absconder': '16.62',\n",
       " 'absence': '7.70',\n",
       " 'absent': '6.50',\n",
       " 'absentee': '11.00',\n",
       " 'absenteeism': '13.43',\n",
       " 'absentminded': '10.84',\n",
       " 'absentmindedness': '9.61',\n",
       " 'absinthe': '16.29',\n",
       " 'absolute': '8.53',\n",
       " 'absolutely': '7.08',\n",
       " 'absoluteness': '12.68',\n",
       " 'absolution': '11.71',\n",
       " 'absolve': '12.90',\n",
       " 'absolver': '13.60',\n",
       " 'absorb': '8.83',\n",
       " 'absorbent': '10.32',\n",
       " 'absorber': '10.33',\n",
       " 'absorption': '10.94',\n",
       " 'abstain': '12.95',\n",
       " 'abstention': '15.19',\n",
       " 'abstinence': '12.74',\n",
       " 'abstract': '11.47',\n",
       " 'abstraction': '13.65',\n",
       " 'abstractionist': '14.72',\n",
       " 'abstruse': '14.60',\n",
       " 'absurd': '10.50',\n",
       " 'absurdist': '13.00',\n",
       " 'absurdity': '13.21',\n",
       " 'abundance': '10.45',\n",
       " 'abundant': '12.84',\n",
       " 'abuse': '8.61',\n",
       " 'abuser': '9.94',\n",
       " 'abusive': '9.58',\n",
       " 'abusiveness': '10.47',\n",
       " 'abut': '13.11',\n",
       " 'abutment': '15.33',\n",
       " 'abuzz': '13.00',\n",
       " 'abysm': '15.25',\n",
       " 'abysmal': '13.78',\n",
       " 'abyss': '11.00',\n",
       " 'abyssal': '17.27',\n",
       " 'ac': '10.75',\n",
       " 'acacia': '14.25',\n",
       " 'academe': '13.27',\n",
       " 'academia': '13.74',\n",
       " 'academic': '9.74',\n",
       " 'academician': '15.21',\n",
       " 'academicism': '15.00',\n",
       " 'academy': '9.05',\n",
       " 'acanthus': '15.17',\n",
       " 'accede': '14.31',\n",
       " 'accelerando': '14.50',\n",
       " 'accelerant': '12.84',\n",
       " 'accelerate': '10.74',\n",
       " 'accelerated': '10.11',\n",
       " 'acceleration': '9.84',\n",
       " 'accelerator': '10.27',\n",
       " 'accent': '8.60',\n",
       " 'accentuate': '13.24',\n",
       " 'accept': '6.74',\n",
       " 'acceptable': '9.84',\n",
       " 'acceptance': '8.56',\n",
       " 'acceptant': '11.89',\n",
       " 'access': '9.10',\n",
       " 'accessibility': '11.05',\n",
       " 'accessible': '9.94',\n",
       " 'accession': '12.25',\n",
       " 'accessory': '11.15',\n",
       " 'accessorize': '11.39',\n",
       " 'accident': '5.30',\n",
       " 'accidental': '6.75',\n",
       " 'acclaim': '12.11',\n",
       " 'acclamation': '14.00',\n",
       " 'acclimate': '14.24',\n",
       " 'acclimatization': '16.12',\n",
       " 'acclimatize': '14.00',\n",
       " 'accolade': '12.43',\n",
       " 'accommodate': '9.90',\n",
       " 'accommodation': '9.58',\n",
       " 'accommodative': '15.18',\n",
       " 'accompany': '10.75',\n",
       " 'accompaniment': '11.18',\n",
       " 'accompanist': '12.78',\n",
       " 'accomplice': '9.78',\n",
       " 'accomplish': '7.84',\n",
       " 'accomplishment': '9.00',\n",
       " 'accord': '9.65',\n",
       " 'accordance': '11.75',\n",
       " 'according': '8.45',\n",
       " 'accordion': '8.61',\n",
       " 'accost': '13.37',\n",
       " 'account': '8.26',\n",
       " 'accountability': '12.16',\n",
       " 'accountable': '11.70',\n",
       " 'accountancy': '14.80',\n",
       " 'accountant': '11.67',\n",
       " 'accounting': '12.05',\n",
       " 'accoutrement': '13.87',\n",
       " 'accreditation': '15.53',\n",
       " 'accredited': '13.89',\n",
       " 'accrete': '14.00',\n",
       " 'accrual': '15.00',\n",
       " 'accrue': '14.11',\n",
       " 'accumulate': '12.30',\n",
       " 'accumulation': '10.50',\n",
       " 'accumulative': '12.50',\n",
       " 'accumulator': '13.94',\n",
       " 'accuracy': '9.71',\n",
       " 'accurate': '9.44',\n",
       " 'accursed': '12.71',\n",
       " 'accusation': '10.17',\n",
       " 'accusative': '13.33',\n",
       " 'accusatory': '11.18',\n",
       " 'accuse': '9.10',\n",
       " 'accuser': '11.72',\n",
       " 'accustom': '12.00',\n",
       " 'ace': '8.83',\n",
       " 'acerb': '15.67',\n",
       " 'acerbic': '16.62',\n",
       " 'acerbity': '15.75',\n",
       " 'acetaminophen': '13.47',\n",
       " 'acetate': '12.85',\n",
       " 'acetic': '13.75',\n",
       " 'acetone': '14.05',\n",
       " 'acetyl': '16.31',\n",
       " 'acetylcholine': '16.30',\n",
       " 'acetylene': '16.88',\n",
       " 'acetylsalicylic': '17.71',\n",
       " 'ache': '5.79',\n",
       " 'achievable': '9.89',\n",
       " 'achieve': '8.53',\n",
       " 'achievement': '8.80',\n",
       " 'achiever': '9.11',\n",
       " 'achiness': '10.11',\n",
       " 'achy': '8.06',\n",
       " 'acid': '9.60',\n",
       " 'acidic': '11.21',\n",
       " 'acidity': '12.78',\n",
       " 'acidophilus': '17.40',\n",
       " 'acidosis': '16.85',\n",
       " 'acidulous': '15.43',\n",
       " 'acknowledge': '9.11',\n",
       " 'acknowledgement': '9.95',\n",
       " 'acme': '9.54',\n",
       " 'acne': '11.40',\n",
       " 'acolyte': '13.77',\n",
       " 'aconitum': '11.00',\n",
       " 'acorn': '5.95',\n",
       " 'acoustic': '12.39',\n",
       " 'acoustical': '12.90',\n",
       " 'acoustics': '11.32',\n",
       " 'acquaintance': '10.48',\n",
       " 'acquaintanceship': '14.18',\n",
       " 'acquiesce': '14.25',\n",
       " 'acquiescence': '13.61',\n",
       " 'acquiescent': '14.18',\n",
       " 'acquire': '9.94',\n",
       " 'acquirer': '11.79',\n",
       " 'acquisition': '13.63',\n",
       " 'acquisitive': '14.19',\n",
       " 'acquisitiveness': '14.38',\n",
       " 'acquit': '13.22',\n",
       " 'acquittal': '12.39',\n",
       " 'acre': '9.80',\n",
       " 'acreage': '13.13',\n",
       " 'acrid': '14.07',\n",
       " 'acrimonious': '16.30',\n",
       " 'acrimony': '14.75',\n",
       " 'acrobat': '8.05',\n",
       " 'acrobatic': '9.38',\n",
       " 'acrobatics': '9.00',\n",
       " 'acronym': '11.56',\n",
       " 'acrophobia': '14.38',\n",
       " 'acropolis': '15.40',\n",
       " 'across': '6.64',\n",
       " 'acrostic': '14.82',\n",
       " 'acrylic': '11.56',\n",
       " 'act': '6.42',\n",
       " 'actinic': '15.62',\n",
       " 'actinium': 'NA',\n",
       " 'action': '6.67',\n",
       " 'actionable': '12.50',\n",
       " 'activate': '11.11',\n",
       " 'activation': '9.78',\n",
       " 'activator': '11.50',\n",
       " 'active': '6.53',\n",
       " 'activism': '11.89',\n",
       " 'activist': '11.80',\n",
       " 'activity': '6.47',\n",
       " 'actor': '7.17',\n",
       " 'actress': '6.17',\n",
       " 'actual': '6.94',\n",
       " 'actuality': '12.38',\n",
       " 'actualization': '13.84',\n",
       " 'actualize': '13.61',\n",
       " 'actually': '7.33',\n",
       " 'actuarial': '15.85',\n",
       " 'actuary': '17.27',\n",
       " 'actuation': '14.69',\n",
       " 'actuator': '13.71',\n",
       " 'acuity': '13.88',\n",
       " 'acumen': '14.44',\n",
       " 'acupressure': '14.44',\n",
       " 'acupuncture': '11.79',\n",
       " 'acupuncturist': '14.61',\n",
       " 'acute': '11.60',\n",
       " 'acuteness': '12.95',\n",
       " 'acyclovir': '12.57',\n",
       " 'ad': '8.22',\n",
       " 'adage': '16.21',\n",
       " 'adagio': '12.62',\n",
       " 'adamant': '14.29',\n",
       " 'adamantine': '15.91',\n",
       " 'adapt': '9.00',\n",
       " 'adaptability': '11.65',\n",
       " 'adaptable': '9.37',\n",
       " 'adaptation': '11.39',\n",
       " 'adapter': '11.79',\n",
       " 'adaption': '12.40',\n",
       " 'adaptive': '11.33',\n",
       " 'adaptor': '10.21',\n",
       " 'add': '5.10',\n",
       " 'addendum': '15.65',\n",
       " 'adder': '12.13',\n",
       " 'addict': '11.17',\n",
       " 'addiction': '12.11',\n",
       " 'addictive': '10.33',\n",
       " 'addictiveness': '11.61',\n",
       " 'addition': '5.76',\n",
       " 'additional': '9.00',\n",
       " 'additive': '12.28',\n",
       " 'addle': '13.08',\n",
       " 'addled': '12.00',\n",
       " 'address': '5.15',\n",
       " 'addressee': '9.95',\n",
       " 'adduce': '13.00',\n",
       " 'adductor': '14.18',\n",
       " 'adenoidal': '15.33',\n",
       " 'adenoma': '13.78',\n",
       " 'adenosine': '16.29',\n",
       " 'adept': '12.50',\n",
       " 'adeptness': '14.37',\n",
       " 'adequacy': '12.50',\n",
       " 'adequate': '10.35',\n",
       " 'adhere': '10.78',\n",
       " 'adherence': '12.22',\n",
       " 'adherent': '14.17',\n",
       " 'adhesion': '13.50',\n",
       " 'adhesive': '9.17',\n",
       " 'adiabatic': '18.50',\n",
       " 'adipose': '15.70',\n",
       " 'adiposity': '15.86',\n",
       " 'adjacent': '11.05',\n",
       " 'adjective': '8.60',\n",
       " 'adjoining': '11.85',\n",
       " 'adjourn': '12.59',\n",
       " 'adjournment': '14.26',\n",
       " 'adjudge': '15.20',\n",
       " 'adjudicate': '15.25',\n",
       " 'adjudication': '16.33',\n",
       " 'adjudicator': '16.47',\n",
       " 'adjunct': '13.06',\n",
       " 'adjuration': '16.00',\n",
       " 'adjure': '13.10',\n",
       " 'adjust': '8.43',\n",
       " 'adjustability': '12.05',\n",
       " 'adjustable': '8.95',\n",
       " 'adjuster': '11.28',\n",
       " 'adjustment': '9.74',\n",
       " 'adjustor': '13.29',\n",
       " 'adjutant': '13.91',\n",
       " 'adjuvant': '14.25',\n",
       " 'admin': '13.89',\n",
       " 'administer': '9.42',\n",
       " 'administrate': '12.50',\n",
       " 'administration': '11.11',\n",
       " 'administrative': '11.50',\n",
       " 'administrator': '12.22',\n",
       " 'admirable': '11.42',\n",
       " 'admiral': '8.72',\n",
       " 'admiralty': '11.41',\n",
       " 'admiration': '8.95',\n",
       " 'admire': '7.42',\n",
       " 'admirer': '8.78',\n",
       " 'admissibility': '13.79',\n",
       " 'admissible': '13.10',\n",
       " 'admission': '8.84',\n",
       " 'admit': '7.56',\n",
       " 'admittance': '10.39',\n",
       " 'admixture': '14.10',\n",
       " 'admonish': '13.40',\n",
       " 'admonition': '13.86',\n",
       " 'ado': '11.88',\n",
       " 'adobe': '12.22',\n",
       " 'adolescence': '10.42',\n",
       " 'adolescent': '11.05',\n",
       " 'adopt': '7.83',\n",
       " 'adoptable': '9.22',\n",
       " 'adopter': '10.16',\n",
       " 'adoption': '7.79',\n",
       " 'adoptive': '9.16',\n",
       " 'adorability': '11.76',\n",
       " 'adorable': '6.94',\n",
       " 'adorableness': '8.32',\n",
       " 'adoration': '11.80',\n",
       " 'adore': '7.95',\n",
       " 'adoring': '8.53',\n",
       " 'adorn': '11.65',\n",
       " 'adornment': '12.32',\n",
       " 'adrenal': '14.24',\n",
       " 'adrenalin': '11.65',\n",
       " 'adrenaline': '11.63',\n",
       " 'adrift': '11.21',\n",
       " 'adroit': '12.75',\n",
       " 'adroitness': '15.21',\n",
       " 'adulation': '12.72',\n",
       " 'adult': '4.68',\n",
       " 'adulterant': '12.86',\n",
       " 'adulterate': '13.76',\n",
       " 'adulterer': '11.63',\n",
       " 'adulteress': '12.22',\n",
       " 'adulterous': '11.65',\n",
       " 'adultery': '11.84',\n",
       " 'adulthood': '9.95',\n",
       " 'adumbration': '16.50',\n",
       " 'advance': '8.35',\n",
       " 'advanced': '9.05',\n",
       " 'advancement': '10.89',\n",
       " 'advantage': '9.32',\n",
       " 'advantageous': '12.62',\n",
       " 'advent': '10.38',\n",
       " 'adventure': '5.67',\n",
       " 'adventurer': '7.79',\n",
       " 'adventuresome': '10.38',\n",
       " 'adventuress': '10.44',\n",
       " 'adventurist': '8.68',\n",
       " 'adventurous': '10.00',\n",
       " 'adventurousness': '11.81',\n",
       " 'adverb': '8.14',\n",
       " 'adverbial': '13.14',\n",
       " 'adversarial': '14.00',\n",
       " 'adversary': '12.67',\n",
       " 'adverse': '10.72',\n",
       " 'adversity': '11.79',\n",
       " 'advert': '14.22',\n",
       " 'advertise': '9.63',\n",
       " 'advertisement': '7.67',\n",
       " 'advertiser': '11.00',\n",
       " 'advertising': '9.95',\n",
       " 'advice': '8.61',\n",
       " 'advisable': '10.75',\n",
       " 'advise': '8.89',\n",
       " 'advisee': '12.89',\n",
       " 'advisement': '12.50',\n",
       " 'adviser': '10.78',\n",
       " 'advisor': '11.24',\n",
       " 'advisory': '11.55',\n",
       " 'advocacy': '14.06',\n",
       " 'advocate': '11.11',\n",
       " 'aegis': '17.00',\n",
       " 'aerate': '13.44',\n",
       " 'aeration': '12.35',\n",
       " 'aerial': '11.50',\n",
       " 'aerialist': '14.08',\n",
       " 'aerie': '13.00',\n",
       " 'aero': '11.40',\n",
       " 'aerobatics': '9.53',\n",
       " 'aerobic': '10.24',\n",
       " 'aerobics': '11.00',\n",
       " 'aerodynamic': '13.40',\n",
       " 'aerodynamics': '11.50',\n",
       " 'aeronaut': '12.92',\n",
       " 'aeronautical': '14.35',\n",
       " 'aeronautics': '12.35',\n",
       " 'aerosol': '9.47',\n",
       " 'aerospace': '12.06',\n",
       " 'aesthete': '14.90',\n",
       " 'aesthetic': '15.74',\n",
       " 'aesthetics': '15.06',\n",
       " 'afar': '8.21',\n",
       " 'afeard': '10.86',\n",
       " 'affability': '14.07',\n",
       " 'affable': '13.40',\n",
       " 'affair': '10.94',\n",
       " 'affaire': '11.45',\n",
       " 'affect': '9.60',\n",
       " 'affectation': '13.82',\n",
       " 'affection': '8.53',\n",
       " 'affectionate': '8.50',\n",
       " 'affective': '11.74',\n",
       " 'affectless': '13.60',\n",
       " 'affiant': '14.00',\n",
       " 'affidavit': '15.69',\n",
       " 'affiliate': '12.00',\n",
       " 'affiliation': '12.28',\n",
       " 'affinity': '12.22',\n",
       " 'affirm': '12.24',\n",
       " 'affirmation': '12.56',\n",
       " 'affirmative': '10.67',\n",
       " 'affix': '11.94',\n",
       " 'afflict': '11.50',\n",
       " 'affliction': '11.32',\n",
       " 'afflictive': '14.40',\n",
       " 'affluence': '13.88',\n",
       " 'affluent': '13.16',\n",
       " 'afford': '7.47',\n",
       " 'affordability': '10.89',\n",
       " 'affordable': '8.15',\n",
       " 'affray': '13.00',\n",
       " 'affright': '12.77',\n",
       " 'affront': '12.44',\n",
       " 'afghan': '9.11',\n",
       " 'Afghan': '9.11',\n",
       " 'aficionado': '13.86',\n",
       " 'afire': '11.87',\n",
       " 'aflame': '11.22',\n",
       " 'afloat': '9.33',\n",
       " 'afoot': '11.48',\n",
       " 'aforementioned': '14.58',\n",
       " 'aforesaid': '14.44',\n",
       " 'aforethought': '13.72',\n",
       " 'afoul': '11.81',\n",
       " 'afraid': '4.42',\n",
       " 'afro': '9.58',\n",
       " 'Afro': '9.58',\n",
       " 'aft': '13.88',\n",
       " 'after': '6.00',\n",
       " 'afterbirth': '14.06',\n",
       " 'afterburner': '10.69',\n",
       " 'aftercare': '11.62',\n",
       " 'afterdeck': '15.86',\n",
       " 'aftereffect': '12.06',\n",
       " 'afterglow': '12.53',\n",
       " 'afterimage': '13.67',\n",
       " 'afterlife': '8.11',\n",
       " 'aftermarket': '14.20',\n",
       " 'aftermath': '12.05',\n",
       " 'afternoon': '4.65',\n",
       " 'afterschool': '6.89',\n",
       " 'aftershave': '10.42',\n",
       " 'aftershock': '10.33',\n",
       " 'aftertaste': '10.00',\n",
       " 'afterthought': '12.78',\n",
       " 'afterwards': '7.71',\n",
       " 'afterworld': '10.41',\n",
       " 'again': '5.78',\n",
       " 'against': '7.97',\n",
       " 'agate': '13.00',\n",
       " 'agave': '14.57',\n",
       " 'age': '4.38',\n",
       " 'ageism': '17.07',\n",
       " 'ageist': '15.79',\n",
       " 'ageless': '10.30',\n",
       " 'agency': '10.58',\n",
       " 'agenda': '9.95',\n",
       " 'agent': '9.55',\n",
       " 'ageratum': '13.29',\n",
       " 'agglomerate': '15.67',\n",
       " 'agglutinate': '14.89',\n",
       " 'aggrandize': '14.83',\n",
       " 'aggrandizement': '14.75',\n",
       " 'aggravate': '8.32',\n",
       " 'aggravated': '8.61',\n",
       " 'aggravation': '9.11',\n",
       " 'aggregate': '13.67',\n",
       " 'aggregation': '14.56',\n",
       " 'aggress': '12.13',\n",
       " 'aggression': '9.30',\n",
       " 'aggressive': '8.32',\n",
       " 'aggressiveness': '11.06',\n",
       " 'aggressor': '11.47',\n",
       " 'aggrieved': '12.38',\n",
       " 'aghast': '12.00',\n",
       " 'agile': '11.94',\n",
       " 'agility': '10.06',\n",
       " 'aging': '7.35',\n",
       " 'agitate': '10.17',\n",
       " 'agitated': '10.12',\n",
       " 'agitation': '11.50',\n",
       " 'agitator': '11.84',\n",
       " 'agitprop': '16.60',\n",
       " 'agleam': '14.71',\n",
       " 'aglitter': '13.67',\n",
       " 'aglow': '10.06',\n",
       " 'agnate': '16.50',\n",
       " 'agnostic': '13.80',\n",
       " 'ago': '7.12',\n",
       " 'agog': '14.73',\n",
       " 'agony': '9.22',\n",
       " 'agonist': '15.06',\n",
       " 'agonize': '11.58',\n",
       " 'agonizing': '10.89',\n",
       " 'agora': '13.73',\n",
       " 'agoraphobia': '14.44',\n",
       " 'agoraphobic': '14.64',\n",
       " 'agrarian': '14.83',\n",
       " 'agree': '7.68',\n",
       " 'agreeable': '8.41',\n",
       " 'agreement': '8.16',\n",
       " 'agribusiness': '18.07',\n",
       " 'agricultural': '10.11',\n",
       " 'agriculturalist': '12.78',\n",
       " 'agriculture': '10.32',\n",
       " 'ague': '15.20',\n",
       " 'ahead': '5.45',\n",
       " 'ahoy': '7.61',\n",
       " 'aid': '7.33',\n",
       " 'aide': '7.83',\n",
       " 'aider': '12.92',\n",
       " 'aids': '8.89',\n",
       " 'aigrette': '15.00',\n",
       " 'aikido': '14.08',\n",
       " 'ail': '9.71',\n",
       " 'aileron': '14.71',\n",
       " 'ailing': '10.63',\n",
       " 'ailment': '10.37',\n",
       " 'aim': '6.72',\n",
       " 'aimer': '9.85',\n",
       " 'aimless': '10.32',\n",
       " 'air': '3.94',\n",
       " 'airbag': '12.26',\n",
       " 'airbase': '10.35',\n",
       " 'airboat': '10.75',\n",
       " 'airborne': '9.85',\n",
       " 'airbrush': '12.06',\n",
       " 'aircraft': '7.61',\n",
       " 'aircraftsman': '11.67',\n",
       " 'aircrew': '10.15',\n",
       " 'airdrome': '15.11',\n",
       " 'airdrop': '9.18',\n",
       " 'airfare': '10.26',\n",
       " 'airfield': '8.60',\n",
       " 'airflow': '12.42',\n",
       " 'airfoil': '14.64',\n",
       " 'airforce': '6.83',\n",
       " 'airhead': '10.17',\n",
       " 'airiness': '11.35',\n",
       " 'airless': '10.00',\n",
       " 'airlift': '12.06',\n",
       " 'airline': '8.74',\n",
       " 'airliner': '9.89',\n",
       " 'airlock': '11.61',\n",
       " 'airmail': '12.00',\n",
       " 'airman': '10.58',\n",
       " 'airmattress': '9.37',\n",
       " 'airmobile': '12.69',\n",
       " 'airplane': '3.94',\n",
       " 'airplay': '12.93',\n",
       " 'airport': '6.84',\n",
       " 'airship': '10.80',\n",
       " 'airsick': '11.47',\n",
       " 'airsickness': '10.17',\n",
       " 'airspace': '11.50',\n",
       " 'airspeed': '10.90',\n",
       " 'airstream': '12.22',\n",
       " 'airstrike': '12.78',\n",
       " 'airstrip': '8.16',\n",
       " 'airtight': '9.84',\n",
       " 'airtime': '11.78',\n",
       " 'airwave': '11.35',\n",
       " 'airway': '11.16',\n",
       " 'airworthy': '13.31',\n",
       " 'airy': '10.28',\n",
       " 'aisle': '5.95',\n",
       " 'ajar': '9.67',\n",
       " 'aka': '10.94',\n",
       " 'akimbo': '14.09',\n",
       " 'akin': '12.67',\n",
       " 'alabaster': '13.67',\n",
       " 'alacrity': '15.25',\n",
       " 'alarm': '6.39',\n",
       " 'alarmclock': '5.80',\n",
       " 'alarming': '9.78',\n",
       " 'alarmist': '13.12',\n",
       " 'alarum': '12.38',\n",
       " 'alas': '9.78',\n",
       " 'albacore': '14.47',\n",
       " 'albatross': '11.64',\n",
       " 'albinism': '14.17',\n",
       " 'albino': '9.79',\n",
       " 'album': '6.72',\n",
       " 'albumen': '15.45',\n",
       " 'albumin': '14.20',\n",
       " 'alcalde': '11.00',\n",
       " 'alchemical': '15.64',\n",
       " 'alchemist': '12.00',\n",
       " 'alchemy': '13.35',\n",
       " 'alcohol': '9.00',\n",
       " 'alcoholic': '9.95',\n",
       " 'alcoholism': '11.60',\n",
       " 'alcove': '12.11',\n",
       " 'alder': '14.08',\n",
       " 'alderman': '14.19',\n",
       " 'ale': '10.16',\n",
       " 'alehouse': '13.76',\n",
       " 'aleph': '13.70',\n",
       " 'alert': '8.90',\n",
       " 'alertness': '9.44',\n",
       " 'alexandrine': '14.17',\n",
       " 'alfalfa': '8.83',\n",
       " 'alfresco': '15.29',\n",
       " 'alga': '13.33',\n",
       " 'algebra': '11.47',\n",
       " 'algebraic': '11.11',\n",
       " 'alginate': '14.86',\n",
       " 'algorithm': '14.56',\n",
       " 'algorithmic': '15.31',\n",
       " 'alias': '9.47',\n",
       " 'alibi': '12.50',\n",
       " 'alien': '7.50',\n",
       " 'alienate': '13.05',\n",
       " 'alienation': '13.47',\n",
       " 'alienist': '14.07',\n",
       " 'aliens': '8.58',\n",
       " 'alight': '11.41',\n",
       " 'align': '9.85',\n",
       " 'alignment': '11.76',\n",
       " 'alike': '6.12',\n",
       " 'alimentary': '13.21',\n",
       " 'alimony': '14.47',\n",
       " 'alit': '12.69',\n",
       " 'alive': '5.11',\n",
       " 'alkali': '14.35',\n",
       " 'alkaline': '13.68',\n",
       " 'alkaloid': '14.06',\n",
       " 'alky': '14.18',\n",
       " 'alkyd': '16.62',\n",
       " 'all': '4.24',\n",
       " 'allay': '15.77',\n",
       " 'allegation': '11.84',\n",
       " 'allege': '12.81',\n",
       " 'alleged': '12.05',\n",
       " 'allegiance': '8.42',\n",
       " 'allegiant': '15.18',\n",
       " 'allegorical': '12.41',\n",
       " 'allegory': '14.16',\n",
       " 'allegro': '13.07',\n",
       " 'allele': '14.50',\n",
       " 'allergen': '11.58',\n",
       " 'allergic': '7.72',\n",
       " 'allergy': '8.11',\n",
       " 'allergist': '12.21',\n",
       " 'alleviate': '11.39',\n",
       " 'alley': '7.26',\n",
       " 'alleyway': '8.11',\n",
       " 'alliance': '11.28',\n",
       " 'allied': '10.85',\n",
       " 'ally': '9.61',\n",
       " 'alligator': '4.78',\n",
       " 'alliteration': '13.86',\n",
       " 'alliterative': '15.25',\n",
       " 'allocate': '13.78',\n",
       " 'allocation': '12.42',\n",
       " 'allocution': '15.23',\n",
       " 'allograft': '15.20',\n",
       " 'allot': '13.67',\n",
       " 'allotment': '13.28',\n",
       " 'allow': '5.32',\n",
       " 'allowable': '9.56',\n",
       " 'allowance': '6.95',\n",
       " 'alloy': '14.53',\n",
       " 'allude': '13.11',\n",
       " 'allure': '11.83',\n",
       " 'alluring': '12.89',\n",
       " 'allusion': '13.05',\n",
       " 'alluvial': '16.78',\n",
       " 'almanac': '10.58',\n",
       " 'almandine': '17.22',\n",
       " 'almightiness': '8.89',\n",
       " 'almighty': '8.47',\n",
       " 'almond': '7.67',\n",
       " 'almoner': '13.00',\n",
       " 'almost': '5.47',\n",
       " 'alms': '9.91',\n",
       " 'almshouse': '17.80',\n",
       " 'aloe': '8.95',\n",
       " 'aloft': '9.82',\n",
       " 'aloha': '10.15',\n",
       " 'alone': '4.94',\n",
       " 'aloneness': '9.06',\n",
       " 'along': '5.41',\n",
       " 'alongshore': '11.60',\n",
       " 'aloof': '13.41',\n",
       " 'alopecia': '17.92',\n",
       " 'aloud': '6.22',\n",
       " 'alp': '11.77',\n",
       " 'alpaca': '10.81',\n",
       " 'alpha': '12.22',\n",
       " 'alphabet': '4.22',\n",
       " 'alphabetical': '6.26',\n",
       " 'alphabetize': '7.11',\n",
       " 'alphanumeric': '13.00',\n",
       " 'alphorn': '15.00',\n",
       " 'alpine': '11.28',\n",
       " 'alright': '6.28',\n",
       " 'also': '6.64',\n",
       " 'altar': '7.16',\n",
       " 'altarpiece': '11.21',\n",
       " 'alter': '9.47',\n",
       " 'alteration': '10.17',\n",
       " 'altercation': '11.67',\n",
       " 'alternate': '8.83',\n",
       " 'alternating': '12.00',\n",
       " 'alternation': '12.32',\n",
       " 'alternative': '10.11',\n",
       " 'alternator': '11.32',\n",
       " 'although': '8.08',\n",
       " 'altimeter': '14.13',\n",
       " 'altitude': '9.58',\n",
       " 'alto': '10.19',\n",
       " 'altogether': '8.35',\n",
       " 'altruism': '15.79',\n",
       " 'altruist': '15.50',\n",
       " 'altruistic': '13.80',\n",
       " 'alum': '13.06',\n",
       " 'alumina': '11.78',\n",
       " 'aluminum': '7.26',\n",
       " 'alumna': '14.00',\n",
       " 'alumnus': '14.00',\n",
       " 'alveolar': '15.00',\n",
       " 'alveolus': '14.50',\n",
       " 'always': '6.26',\n",
       " 'be': '5.11',\n",
       " 'amah': '14.00',\n",
       " 'amalgam': '15.25',\n",
       " 'amalgamated': '15.07',\n",
       " 'amalgamation': '16.92',\n",
       " 'amandine': '15.29',\n",
       " 'amanita': '15.50',\n",
       " 'amanuensis': '12.00',\n",
       " 'amaranth': '15.73',\n",
       " 'amaretto': '14.63',\n",
       " 'amaryllis': '15.36',\n",
       " 'amass': '12.00',\n",
       " 'amassment': '14.75',\n",
       " 'amateur': '10.53',\n",
       " 'amateurish': '12.57',\n",
       " 'amateurism': '12.88',\n",
       " 'amaze': '7.50',\n",
       " 'amazed': '7.17',\n",
       " 'amazement': '9.22',\n",
       " 'amazing': '5.22',\n",
       " 'ambassador': '11.00',\n",
       " 'ambassadorial': '12.69',\n",
       " 'ambassadorship': '13.45',\n",
       " 'amber': '9.28',\n",
       " 'ambergris': '17.50',\n",
       " 'ambiance': '14.78',\n",
       " 'ambidextrous': '11.84',\n",
       " 'ambience': '13.28',\n",
       " 'ambient': '12.80',\n",
       " 'ambiguity': '14.06',\n",
       " 'ambiguous': '13.67',\n",
       " 'ambition': '11.29',\n",
       " 'ambitious': '11.05',\n",
       " 'ambivalence': '13.37',\n",
       " 'ambivalent': '12.12',\n",
       " 'amble': '11.87',\n",
       " 'ambrosia': '13.82',\n",
       " 'ambrosial': '17.12',\n",
       " 'ambulance': '6.16',\n",
       " 'ambulation': '15.36',\n",
       " 'ambulatory': '12.59',\n",
       " 'ambuscade': 'NA',\n",
       " 'ambush': '9.30',\n",
       " 'amoeba': '12.65',\n",
       " 'amoebic': '13.68',\n",
       " 'ameliorate': '13.67',\n",
       " 'amelioration': '16.25',\n",
       " 'amen': '5.80',\n",
       " 'amenable': '14.14',\n",
       " 'amend': '11.39',\n",
       " 'amendment': '10.06',\n",
       " 'amends': '10.16',\n",
       " 'americanize': '12.44',\n",
       " 'amethyst': '12.22',\n",
       " 'amiability': '13.25',\n",
       " 'amiable': '13.11',\n",
       " 'amicable': '12.95',\n",
       " 'amice': '12.14',\n",
       " 'amicus': '15.40',\n",
       " 'amid': '11.45',\n",
       " 'amidst': '10.44',\n",
       " 'amino': '13.78',\n",
       " 'amiss': '11.34',\n",
       " 'amity': '13.14',\n",
       " 'ammeter': '16.83',\n",
       " 'ammo': '8.11',\n",
       " 'ammonia': '11.00',\n",
       " 'ammonium': '12.35',\n",
       " 'ammunition': '9.67',\n",
       " 'amnesia': '10.61',\n",
       " 'amnesiac': '12.64',\n",
       " 'amnesic': '13.67',\n",
       " 'amnesty': '12.95',\n",
       " 'amniocentesis': '14.38',\n",
       " 'amniotic': '13.86',\n",
       " 'among': '7.75',\n",
       " 'amongst': '9.01',\n",
       " 'amontillado': '14.43',\n",
       " 'amoral': '11.70',\n",
       " 'amorality': '15.00',\n",
       " 'amorous': '13.71',\n",
       " 'amorphous': '14.75',\n",
       " 'amortization': '17.33',\n",
       " 'amortize': '15.88',\n",
       " 'amount': '6.63',\n",
       " 'amour': '12.67',\n",
       " 'amp': '12.11',\n",
       " 'amperage': '12.07',\n",
       " 'ampersand': '13.20',\n",
       " 'amphetamine': '15.53',\n",
       " 'amphibian': '8.33',\n",
       " 'amphibious': '10.88',\n",
       " 'amphitheater': '11.84',\n",
       " 'amphora': '15.44',\n",
       " 'ample': '11.63',\n",
       " 'amplification': '11.40',\n",
       " 'amplify': '12.06',\n",
       " 'amplifier': '11.35',\n",
       " 'amplitude': '12.44',\n",
       " 'amputate': '9.60',\n",
       " 'amputation': '11.89',\n",
       " 'amputee': '11.00',\n",
       " 'amuck': '11.18',\n",
       " 'amulet': '11.53',\n",
       " 'amuse': '9.00',\n",
       " 'amused': '7.83',\n",
       " 'amusement': '8.79',\n",
       " 'amusing': '8.44',\n",
       " 'amyl': '15.67',\n",
       " 'amylase': '18.20',\n",
       " 'anabolic': '15.53',\n",
       " 'anachronism': '14.30',\n",
       " 'anachronistic': '15.79',\n",
       " 'anaconda': '10.06',\n",
       " 'anaerobic': '13.12',\n",
       " 'anaesthetize': '14.18',\n",
       " 'anagogic': '15.33',\n",
       " 'anagram': '11.38',\n",
       " 'anal': '13.17',\n",
       " 'analeptic': '15.60',\n",
       " 'analgesia': '15.38',\n",
       " 'analgesic': '14.87',\n",
       " 'analog': '13.62',\n",
       " 'analogy': '11.33',\n",
       " 'analogous': '14.40',\n",
       " 'analogue': '12.16',\n",
       " 'analysis': '11.32',\n",
       " 'analyst': '11.75',\n",
       " 'analytic': '12.61',\n",
       " 'analytical': '12.44',\n",
       " 'analyzable': '11.27',\n",
       " 'analyze': '11.39',\n",
       " 'analyzer': '11.37',\n",
       " 'anapestic': '15.67',\n",
       " 'anaphylactic': '16.27',\n",
       " 'anaphylaxis': '17.00',\n",
       " 'anarchic': '14.15',\n",
       " 'anarchism': '12.46',\n",
       " 'anarchist': '12.63',\n",
       " 'anarchistic': '15.95',\n",
       " 'anarchy': '13.94',\n",
       " 'anathema': '13.00',\n",
       " 'anatomic': '12.53',\n",
       " 'anatomical': '12.05',\n",
       " 'anatomy': '11.94',\n",
       " 'anatomist': '14.22',\n",
       " 'anatomize': '14.19',\n",
       " 'ancestor': '8.61',\n",
       " 'ancestors': '9.01',\n",
       " 'ancestral': '11.21',\n",
       " 'ancestry': '10.11',\n",
       " 'anchor': '5.72',\n",
       " 'anchorage': '12.00',\n",
       " 'anchorite': '15.00',\n",
       " 'anchorman': '8.06',\n",
       " 'anchorwoman': '11.16',\n",
       " 'anchovy': '9.32',\n",
       " 'ancient': '8.26',\n",
       " 'ancillary': '13.78',\n",
       " 'and': '4.57',\n",
       " 'andiron': '12.50',\n",
       " 'androgynous': '15.80',\n",
       " 'androgyny': '16.50',\n",
       " 'android': '13.89',\n",
       " 'anecdotal': '12.47',\n",
       " 'anecdote': '14.33',\n",
       " 'anemia': '12.00',\n",
       " 'anemic': '13.42',\n",
       " 'anemone': '13.07',\n",
       " 'anent': '17.00',\n",
       " 'aneroid': '11.80',\n",
       " 'anesthesia': '12.53',\n",
       " 'anesthesiologist': '13.00',\n",
       " 'anesthetic': '13.05',\n",
       " 'anesthetist': '13.28',\n",
       " 'anesthetize': '12.78',\n",
       " 'aneurism': '15.22',\n",
       " 'aneurysm': '15.05',\n",
       " 'anew': '10.58',\n",
       " 'angel': '4.00',\n",
       " 'angelfish': '7.42',\n",
       " 'angelic': '8.33',\n",
       " 'angelical': '11.37',\n",
       " 'anger': '6.00',\n",
       " 'angina': '15.86',\n",
       " 'angiogram': '15.00',\n",
       " 'angiography': '15.67',\n",
       " 'angioplasty': '17.27',\n",
       " 'angle': '8.05',\n",
       " 'angler': '11.53',\n",
       " 'angleworm': '10.23',\n",
       " 'anglophile': '17.11',\n",
       " 'angora': '15.12',\n",
       " ...}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AoAVConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 7)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sven Features\n",
    "\n",
    "In this block of code we load the list of commonly searched for terms used by children as established in the Is Sven Seven data set, and then determines the ratio of those terms occuring in each query. #--- from thesis: children tend to use different vocabulary than adults when searching therefore we count the occurrence of words per  query found in the most common words found in children's websites from the Sven Children dictionary dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>family</th>\n",
       "      <th>71289</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>story</td>\n",
       "      <td>43562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>life</td>\n",
       "      <td>29678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>�</td>\n",
       "      <td>26454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>trtd</td>\n",
       "      <td>20449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>love</td>\n",
       "      <td>19698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1 family  71289\n",
       "0  2  story  43562\n",
       "1  3   life  29678\n",
       "2  4      �  26454\n",
       "3  5   trtd  20449\n",
       "4  6   love  19698"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking into the data that we are loading \n",
    "svendict = pd.read_csv('DataSets/Sven/ChildrenDict.tsv', sep = '\\t')\n",
    "svendict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:04<00:00, 1042.33it/s]\n"
     ]
    }
   ],
   "source": [
    "SVENwords = []\n",
    "st = WordNetLemmatizer()\n",
    "with open('DataSets/Sven/ChildrenDict.tsv') as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter = '\\t')\n",
    "    lineCount = 0\n",
    "    for row in csvReader:\n",
    "#         print(row)\n",
    "        if lineCount == 0:\n",
    "            lineCount +=1\n",
    "        else:\n",
    "            SVENwords.append((row[1]))\n",
    "            \n",
    "SVENcount = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "#             print(word)\n",
    "            wordCount +=1\n",
    "            if word in SVENwords:\n",
    "                countWord +=1\n",
    "\n",
    "        SVENcount.append(countWord/wordCount)\n",
    "        pbar.update()\n",
    "        \n",
    "Vocab['SVEN'] = SVENcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 8)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4746"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SVENcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete/Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_concreteness = pickle.load( open( \"DataSets/word_concreteness.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/assoumerredempta/Documents/aSpring_2023/RYSe_Final/FeatureExtraction'"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_concreteness['word']=word_concreteness['word'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstractWords = word_concreteness.loc[word_concreteness['label']=='abstract', 'word']\n",
    "# concreteWords = word_concreteness.loc[word_concreteness['label']=='concrete', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:04<00:00, 952.74it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "aw = word_concreteness[word_concreteness['label']=='abstract']\n",
    "cw = word_concreteness[word_concreteness['label']=='concrete']\n",
    "\n",
    "abW = []\n",
    "coW = []\n",
    "for w_a in aw['word']:\n",
    "    abW.append(w_a)\n",
    "for w_c in cw['word']:\n",
    "    coW.append(w_c)\n",
    "\n",
    "absrtCount = []\n",
    "concCount = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "#         vocab = []\n",
    "        a_countWord = 0\n",
    "        c_countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "            word.lower()\n",
    "#             print(word)\n",
    "            wordCount +=1\n",
    "            if word in abW:\n",
    "                a_countWord +=1\n",
    "            if word in coW:\n",
    "                c_countWord +=1\n",
    "\n",
    "        absrtCount.append(a_countWord/wordCount)\n",
    "        concCount.append(c_countWord/wordCount)\n",
    "        pbar.update()\n",
    "        \n",
    "Vocab['ratioAbs'] = absrtCount\n",
    "Vocab['ratioConc'] = concCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save abs and con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "aC =  absrtCount.copy()\n",
    "cC = concCount.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {'ratioAbs': aC,\n",
    "      'ratioConc': cC}\n",
    "abs_conc = pd.DataFrame(dt)\n",
    "# abs_conc.to_csv('abs_conc.csv')\n",
    "\n",
    "pickle.dump(abs_conc, open( \"Pickles/4746Abs_concFeat.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/assoumerredempta/Documents/aSpring_2023/RYSe_Final/FeatureExtraction'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>...</th>\n",
       "      <th>com</th>\n",
       "      <th>net</th>\n",
       "      <th>org</th>\n",
       "      <th>edu</th>\n",
       "      <th>gov</th>\n",
       "      <th>http</th>\n",
       "      <th>AND</th>\n",
       "      <th>OR</th>\n",
       "      <th>quotes</th>\n",
       "      <th>inter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.906667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.895000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>1.00</td>\n",
       "      <td>House of dreams</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.196667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>0.50</td>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.095000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>0.00</td>\n",
       "      <td>H</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>0.75</td>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.80</td>\n",
       "      <td>6.47</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.030000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>1.00</td>\n",
       "      <td>kid</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      coreVocab                              query  nonCoreVocab  minAoA  \\\n",
       "0          0.50                US civil war causes          0.50    0.00   \n",
       "1          0.00                     scooter brands          1.00    6.68   \n",
       "2          0.00            scooter brands reliable          1.00    6.68   \n",
       "3          0.00                            scooter          1.00    6.68   \n",
       "4          0.00                      scooter cheap          1.00    6.68   \n",
       "...         ...                                ...           ...     ...   \n",
       "4741       1.00                    House of dreams          0.00    3.16   \n",
       "4742       0.50  When did Desmond doss get married          0.50    0.00   \n",
       "4743       0.00                                  H          1.00    0.00   \n",
       "4744       0.75                find fact about dog          0.25    2.80   \n",
       "4745       1.00                                kid          0.00    4.28   \n",
       "\n",
       "      maxAoA  ratioAoA  queryComplexity      SVEN  ratioAbs  ratioConc  ...  \\\n",
       "0      10.89      0.75         6.100000  0.500000  0.250000   0.250000  ...   \n",
       "1       7.72      1.00         7.200000  0.500000  0.000000   0.500000  ...   \n",
       "2       9.32      1.00         7.906667  0.666667  0.333333   0.333333  ...   \n",
       "3       6.68      1.00         6.680000  1.000000  0.000000   1.000000  ...   \n",
       "4       7.11      1.00         6.895000  1.000000  0.500000   0.500000  ...   \n",
       "...      ...       ...              ...       ...       ...        ...  ...   \n",
       "4741    4.88      1.00         4.196667  0.333333  0.333333   0.000000  ...   \n",
       "4742    5.16      0.50         2.095000  0.333333  0.500000   0.000000  ...   \n",
       "4743    0.00      0.00         0.000000  0.000000  0.000000   0.000000  ...   \n",
       "4744    6.47      1.00         5.030000  0.500000  0.750000   0.250000  ...   \n",
       "4745    4.28      1.00         4.280000  1.000000  0.000000   1.000000  ...   \n",
       "\n",
       "      com  net  org  edu  gov  http  AND  OR  quotes  inter  \n",
       "0       0    0    0    0    0     0    0   0       0      0  \n",
       "1       0    0    0    0    0     0    0   0       0      0  \n",
       "2       0    0    0    0    0     0    0   0       0      0  \n",
       "3       0    0    0    0    0     0    0   0       0      0  \n",
       "4       0    0    0    0    0     0    0   0       0      0  \n",
       "...   ...  ...  ...  ...  ...   ...  ...  ..     ...    ...  \n",
       "4741    0    0    0    0    0     0    0   0       0      0  \n",
       "4742    0    0    0    0    0     0    0   0       0      1  \n",
       "4743    0    0    0    0    0     0    0   0       0      0  \n",
       "4744    0    0    0    0    0     0    0   0       0      0  \n",
       "4745    0    0    0    0    0     0    0   0       0      0  \n",
       "\n",
       "[4746 rows x 37 columns]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Stereotype Uni-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who belong to our stereotype, extract the 250 most common word uni-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 250.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 88243.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "# sID = allSessionsQ['sID'].unique()\n",
    "# corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "# queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "allSessions = allSessions[allSessions['class'] == 1]\n",
    "sID = allSessions['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessions = allSessions[allSessions['sID'].isin(corpus)]\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "text = ''\n",
    "\n",
    "for query in queries:\n",
    "    text += query.lower() + \" \"\n",
    "    \n",
    "queryWords = text.split()\n",
    "\n",
    "resultWords  = [word for word in queryWords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultWords)\n",
    "text = text.split(' ')\n",
    "fdist1 = nltk.FreqDist(text)\n",
    "top250 = []\n",
    "\n",
    "for x in fdist1.most_common(250):\n",
    "    top250.append(x[0])\n",
    "    \n",
    "top250count = []\n",
    "top250avg = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "            wordCount +=1\n",
    "            if word in top250:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top250count.append(countWord)\n",
    "        top250avg.append(countWord/wordCount)\n",
    "        pbar.update()\n",
    "\n",
    "\n",
    "Vocab['top250SterCount'] = top250count\n",
    "Vocab['top250SterRatAnt'] = top250avg\n",
    "Vocab['top250SterRatCon'] = 1-Vocab['top250SterRatAnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>class</th>\n",
       "      <th>sID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>why is idaho called the gem state</td>\n",
       "      <td>1</td>\n",
       "      <td>5179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>a famos mathution</td>\n",
       "      <td>1</td>\n",
       "      <td>8600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>Victorea stillwells Big moments</td>\n",
       "      <td>1</td>\n",
       "      <td>6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>Information about tigers</td>\n",
       "      <td>1</td>\n",
       "      <td>9041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803</th>\n",
       "      <td>science</td>\n",
       "      <td>1</td>\n",
       "      <td>8574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>Kids facts about Norway</td>\n",
       "      <td>1</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>House of dreams</td>\n",
       "      <td>1</td>\n",
       "      <td>5975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>1</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>7864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>1</td>\n",
       "      <td>5316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>759 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query  class   sID\n",
       "3797  why is idaho called the gem state      1  5179\n",
       "3799                  a famos mathution      1  8600\n",
       "3801    Victorea stillwells Big moments      1  6411\n",
       "3802           Information about tigers      1  9041\n",
       "3803                            science      1  8574\n",
       "...                                 ...    ...   ...\n",
       "4740            Kids facts about Norway      1  7234\n",
       "4741                    House of dreams      1  5975\n",
       "4742  When did Desmond doss get married      1  5233\n",
       "4743                                  H      1  7864\n",
       "4744                find fact about dog      1  5316\n",
       "\n",
       "[759 rows x 3 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 13)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Non-Stereotype Uni-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who do not belong to our stereotype, extract the 250 most common word uni-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams: once of ways helping machines understand the a word and it's contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 105130.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "# sID = allSessionsQ['sID'].unique()\n",
    "# corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "# queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "allSessions = allSessions[allSessions['class'] == 0]\n",
    "sID = allSessions['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessions = allSessions[allSessions['sID'].isin(corpus)]\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "text = ''\n",
    "\n",
    "for query in queries:\n",
    "    text += query.lower() + \" \"\n",
    "    \n",
    "queryWords = text.split()\n",
    "\n",
    "resultWords  = [word for word in queryWords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultWords)\n",
    "text = text.split(' ')\n",
    "fdist1 = nltk.FreqDist(text)\n",
    "top250 = []\n",
    "\n",
    "for x in fdist1.most_common(250):\n",
    "    top250.append(x[0])\n",
    "    \n",
    "top250Count = []\n",
    "top250Avg = []\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        for word in query.split(' '):\n",
    "            wordCount +=1\n",
    "            if word in top250:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top250Count.append(countWord)\n",
    "        top250Avg.append(countWord/wordCount)\n",
    "        pbar.update()\n",
    "    \n",
    "Vocab['top250NonSterCount'] = top250Count\n",
    "Vocab['top250NonSterRatAnt'] = top250Avg\n",
    "Vocab['top250NonSterRatCon'] = 1-Vocab['top250NonSterRatAnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 16)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon', 'top250NonSterCount',\n",
       "       'top250NonSterRatAnt', 'top250NonSterRatCon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>top250SterCount</th>\n",
       "      <th>top250SterRatAnt</th>\n",
       "      <th>top250SterRatCon</th>\n",
       "      <th>top250NonSterCount</th>\n",
       "      <th>top250NonSterRatAnt</th>\n",
       "      <th>top250NonSterRatCon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coreVocab                query  nonCoreVocab  minAoA  maxAoA  ratioAoA  \\\n",
       "0        0.5  US civil war causes           0.5    0.00   10.89      0.75   \n",
       "1        0.0       scooter brands           1.0    6.68    7.72      1.00   \n",
       "\n",
       "   queryComplexity  SVEN  ratioAbs  ratioConc  top250SterCount  \\\n",
       "0              6.1   0.5      0.25       0.25                0   \n",
       "1              7.2   0.5      0.00       0.50                0   \n",
       "\n",
       "   top250SterRatAnt  top250SterRatCon  top250NonSterCount  \\\n",
       "0               0.0               1.0                   1   \n",
       "1               0.0               1.0                   1   \n",
       "\n",
       "   top250NonSterRatAnt  top250NonSterRatCon  \n",
       "0                 0.25                 0.75  \n",
       "1                 0.50                 0.50  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Stereotype Bi-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who belong to our stereotype, extract the 50 most common word bi-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 210297.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "# sID = allSessionsQ['sID'].unique()\n",
    "# corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "# queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "allSessions = allSessions[allSessions['class'] == 1]\n",
    "sID = allSessions['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessions = allSessions[allSessions['sID'].isin(corpus)]\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "\n",
    "queries2 = []\n",
    "for query in queries:\n",
    "    queries2.append(query.lower())\n",
    "queries = queries2\n",
    "\n",
    "bigrams = [b for l in queries for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "\n",
    "fdist1 = nltk.FreqDist(bigrams)\n",
    "\n",
    "top50 = []\n",
    "\n",
    "for x in fdist1.most_common(50):\n",
    "     top50.append(x[0])\n",
    "        \n",
    "top50count = []\n",
    "top50avg = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        query = query.lower()\n",
    "        query = query.split(\" \")\n",
    "        split = nltk.bigrams(query)\n",
    "        for word in split:\n",
    "            wordCount +=1\n",
    "            if word in top50:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top50count.append(countWord)\n",
    "        if wordCount > 0:\n",
    "            top50avg.append(countWord/wordCount)\n",
    "        else:\n",
    "            top50avg.append(0) \n",
    "        pbar.update()\n",
    "        \n",
    "Vocab['top50SterCount'] = top50count\n",
    "Vocab['top50SterRatAnt'] = top50avg\n",
    "Vocab['top50SterAntCon'] = 1-Vocab['top50SterRatAnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>class</th>\n",
       "      <th>sID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>why is idaho called the gem state</td>\n",
       "      <td>1</td>\n",
       "      <td>5179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>alcoholic</td>\n",
       "      <td>1</td>\n",
       "      <td>9005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>a famos mathution</td>\n",
       "      <td>1</td>\n",
       "      <td>8600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>Victorea stillwells Big moments</td>\n",
       "      <td>1</td>\n",
       "      <td>6411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>Information about tigers</td>\n",
       "      <td>1</td>\n",
       "      <td>9041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>How many Star Wars movies will be made?</td>\n",
       "      <td>1</td>\n",
       "      <td>9488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>Mount St. Helens</td>\n",
       "      <td>1</td>\n",
       "      <td>9233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>Kids facts about Norway</td>\n",
       "      <td>1</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>1</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>1</td>\n",
       "      <td>5316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>759 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        query  class   sID\n",
       "3797        why is idaho called the gem state      1  5179\n",
       "3798                                alcoholic      1  9005\n",
       "3799                        a famos mathution      1  8600\n",
       "3801          Victorea stillwells Big moments      1  6411\n",
       "3802                 Information about tigers      1  9041\n",
       "...                                       ...    ...   ...\n",
       "4738  How many Star Wars movies will be made?      1  9488\n",
       "4739                         Mount St. Helens      1  9233\n",
       "4740                  Kids facts about Norway      1  7234\n",
       "4742        When did Desmond doss get married      1  5233\n",
       "4744                      find fact about dog      1  5316\n",
       "\n",
       "[759 rows x 3 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 19)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon', 'top250NonSterCount',\n",
       "       'top250NonSterRatAnt', 'top250NonSterRatCon', 'top50SterCount',\n",
       "       'top50SterRatAnt', 'top50SterAntCon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Non-Stereotype Bi-Grams\n",
    "\n",
    "In this block of code we take 80% of the sessions generated by users who do not belong to our stereotype, extract the 50 most common word bi-grams found in that sample, and then calculate the number of those words found in each query as well as the antecedent and the consequent ratio of words per query that are found in the top 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 195476.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "# sID = allSessionsQ['sID'].unique()\n",
    "# corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "# queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "allSessions = allSessions[allSessions['class'] == 0]\n",
    "sID = allSessions['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessions = allSessions[allSessions['sID'].isin(corpus)]\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "\n",
    "queries2 = []\n",
    "for query in queries:\n",
    "    queries2.append(query.lower())\n",
    "queries = queries2\n",
    "\n",
    "bigrams = [b for l in queries for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "\n",
    "fdist1 = nltk.FreqDist(bigrams)\n",
    "\n",
    "top50 = []\n",
    "\n",
    "for x in fdist1.most_common(50):\n",
    "     top50.append(x[0])\n",
    "        \n",
    "top50count = []\n",
    "top50avg = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        vocab = []\n",
    "        countWord = 0\n",
    "        wordCount = 0\n",
    "        query = query.lower()\n",
    "        query = query.split(\" \")\n",
    "        split = nltk.bigrams(query)\n",
    "        for word in split:\n",
    "            wordCount +=1\n",
    "            if word in top50:\n",
    "                countWord +=1\n",
    "            else:\n",
    "                pass\n",
    "        top50count.append(countWord)\n",
    "        if wordCount > 0:\n",
    "            top50avg.append(countWord/wordCount)\n",
    "        else:\n",
    "            top50avg.append(0)\n",
    "        pbar.update()\n",
    "        \n",
    "Vocab['top50NonSterCount'] = top50count\n",
    "Vocab['top50NonSterRatAnt'] = top50avg\n",
    "Vocab['top50NonSterAntCon'] = 1-Vocab['top50NonSterRatAnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 22)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF All\n",
    "\n",
    "In the following block of code we take an 80% sample of all sessions found in SWC, and then calculate the TF-IDF values for each all queries with each query in every session seen as an individual document. \n",
    "\n",
    "----------**TF-IDF** stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc) in a document amongst a collection of documents-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 12157.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "# sID = allSessionsQ['sID'].unique()\n",
    "# corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "# queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "sID = allSessions['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessions = allSessions[allSessions['sID'].isin(corpus)]\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "\n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for m in vectors:\n",
    "        if(m.sum() != 0):\n",
    "            listTFIDF.append(m.sum() / m.count_nonzero())\n",
    "        else:\n",
    "            listTFIDF.append(-1)\n",
    "        pbar.update()\n",
    "        \n",
    "VocabTFIDFAll = pd.DataFrame(data=listTFIDF, columns = ['tfidfAll']).fillna(-1)\n",
    "# VocabTFIDFAll['query'] = allQueries\n",
    "VocabTFIDFAll['qID'] = qID\n",
    "Vocab['qID'] = qID #-----------------------------------------added\n",
    "Vocab = pd.merge(Vocab, VocabTFIDFAll, on='qID')\n",
    "# Vocab = Vocab.merge(VocabTFIDFAll, on = 'query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidfAll</th>\n",
       "      <th>qID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.491051</td>\n",
       "      <td>6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704467</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.574193</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.706889</td>\n",
       "      <td>6221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>0.552780</td>\n",
       "      <td>5975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>0.405230</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>0.498821</td>\n",
       "      <td>5316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidfAll   qID\n",
       "0     0.491051  6352\n",
       "1     0.704467  8305\n",
       "2     0.574193  6814\n",
       "3     1.000000  7688\n",
       "4     0.706889  6221\n",
       "...        ...   ...\n",
       "4741  0.552780  5975\n",
       "4742  0.405230  5233\n",
       "4743 -1.000000  7864\n",
       "4744  0.498821  5316\n",
       "4745  1.000000  8397\n",
       "\n",
       "[4746 rows x 2 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabTFIDFAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>...</th>\n",
       "      <th>top250NonSterRatAnt</th>\n",
       "      <th>top250NonSterRatCon</th>\n",
       "      <th>top50SterCount</th>\n",
       "      <th>top50SterRatAnt</th>\n",
       "      <th>top50SterAntCon</th>\n",
       "      <th>top50NonSterCount</th>\n",
       "      <th>top50NonSterRatAnt</th>\n",
       "      <th>top50NonSterAntCon</th>\n",
       "      <th>qID</th>\n",
       "      <th>tfidfAll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6352</td>\n",
       "      <td>0.491051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8305</td>\n",
       "      <td>0.704467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.906667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6814</td>\n",
       "      <td>0.574193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7688</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.895000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6221</td>\n",
       "      <td>0.706889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>1.00</td>\n",
       "      <td>House of dreams</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.196667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5975</td>\n",
       "      <td>0.552780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>0.50</td>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.095000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5233</td>\n",
       "      <td>0.405230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>0.00</td>\n",
       "      <td>H</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7864</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>0.75</td>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.80</td>\n",
       "      <td>6.47</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.030000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5316</td>\n",
       "      <td>0.498821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>1.00</td>\n",
       "      <td>kid</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8397</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      coreVocab                              query  nonCoreVocab  minAoA  \\\n",
       "0          0.50                US civil war causes          0.50    0.00   \n",
       "1          0.00                     scooter brands          1.00    6.68   \n",
       "2          0.00            scooter brands reliable          1.00    6.68   \n",
       "3          0.00                            scooter          1.00    6.68   \n",
       "4          0.00                      scooter cheap          1.00    6.68   \n",
       "...         ...                                ...           ...     ...   \n",
       "4741       1.00                    House of dreams          0.00    3.16   \n",
       "4742       0.50  When did Desmond doss get married          0.50    0.00   \n",
       "4743       0.00                                  H          1.00    0.00   \n",
       "4744       0.75                find fact about dog          0.25    2.80   \n",
       "4745       1.00                                kid          0.00    4.28   \n",
       "\n",
       "      maxAoA  ratioAoA  queryComplexity      SVEN  ratioAbs  ratioConc  ...  \\\n",
       "0      10.89      0.75         6.100000  0.500000  0.250000   0.250000  ...   \n",
       "1       7.72      1.00         7.200000  0.500000  0.000000   0.500000  ...   \n",
       "2       9.32      1.00         7.906667  0.666667  0.333333   0.333333  ...   \n",
       "3       6.68      1.00         6.680000  1.000000  0.000000   1.000000  ...   \n",
       "4       7.11      1.00         6.895000  1.000000  0.500000   0.500000  ...   \n",
       "...      ...       ...              ...       ...       ...        ...  ...   \n",
       "4741    4.88      1.00         4.196667  0.333333  0.333333   0.000000  ...   \n",
       "4742    5.16      0.50         2.095000  0.333333  0.500000   0.000000  ...   \n",
       "4743    0.00      0.00         0.000000  0.000000  0.000000   0.000000  ...   \n",
       "4744    6.47      1.00         5.030000  0.500000  0.750000   0.250000  ...   \n",
       "4745    4.28      1.00         4.280000  1.000000  0.000000   1.000000  ...   \n",
       "\n",
       "      top250NonSterRatAnt  top250NonSterRatCon  top50SterCount  \\\n",
       "0                0.250000             0.750000               0   \n",
       "1                0.500000             0.500000               0   \n",
       "2                0.333333             0.666667               0   \n",
       "3                1.000000             0.000000               0   \n",
       "4                1.000000             0.000000               0   \n",
       "...                   ...                  ...             ...   \n",
       "4741             0.000000             1.000000               0   \n",
       "4742             0.166667             0.833333               1   \n",
       "4743             0.000000             1.000000               0   \n",
       "4744             0.250000             0.750000               1   \n",
       "4745             0.000000             1.000000               0   \n",
       "\n",
       "      top50SterRatAnt  top50SterAntCon  top50NonSterCount  top50NonSterRatAnt  \\\n",
       "0            0.000000         1.000000                  0                 0.0   \n",
       "1            0.000000         1.000000                  0                 0.0   \n",
       "2            0.000000         1.000000                  0                 0.0   \n",
       "3            0.000000         1.000000                  0                 0.0   \n",
       "4            0.000000         1.000000                  0                 0.0   \n",
       "...               ...              ...                ...                 ...   \n",
       "4741         0.000000         1.000000                  0                 0.0   \n",
       "4742         0.200000         0.800000                  0                 0.0   \n",
       "4743         0.000000         1.000000                  0                 0.0   \n",
       "4744         0.333333         0.666667                  0                 0.0   \n",
       "4745         0.000000         1.000000                  0                 0.0   \n",
       "\n",
       "      top50NonSterAntCon   qID  tfidfAll  \n",
       "0                    1.0  6352  0.491051  \n",
       "1                    1.0  8305  0.704467  \n",
       "2                    1.0  6814  0.574193  \n",
       "3                    1.0  7688  1.000000  \n",
       "4                    1.0  6221  0.706889  \n",
       "...                  ...   ...       ...  \n",
       "4741                 1.0  5975  0.552780  \n",
       "4742                 1.0  5233  0.405230  \n",
       "4743                 1.0  7864 -1.000000  \n",
       "4744                 1.0  5316  0.498821  \n",
       "4745                 1.0  8397  1.000000  \n",
       "\n",
       "[4746 rows x 24 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon', 'top250NonSterCount',\n",
       "       'top250NonSterRatAnt', 'top250NonSterRatCon', 'top50SterCount',\n",
       "       'top50SterRatAnt', 'top50SterAntCon', 'top50NonSterCount',\n",
       "       'top50NonSterRatAnt', 'top50NonSterAntCon', 'qID', 'tfidfAll'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 4746)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VocabTFIDFAll), len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabTFIDFAll['qID'].equals(Vocab['qID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(Vocab, VocabTFIDFAll, on='qID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=Vocab.copy()\n",
    "# b=VocabTFIDFAll.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.rename(columns={'query': 'key'}, inplace=True)\n",
    "# b.rename(columns={'query': 'key'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[a.key == 'youtube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b[b.key == 'youtube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.merge(VocabTFIDFAll)['query'][Vocab.merge(VocabTFIDFAll)['query'] == 'youtube'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.join(b, lsuffix='_Vocab', rsuffix= '_VocabTFIDFAll', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=Vocab['query'].map(lambda x: len(x)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b=VocabTFIDFAll['query'].map(lambda x: len(x)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.equals(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VocabTFIDFAll.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.merge(VocabTFIDFAll)['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VocabTFIDFAll.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[a != b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VocabTFIDFAll.merge(Vocab, on='query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>points</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team  points  ID\n",
       "0    A       1   1\n",
       "1    B       2   2\n",
       "2    A       1   3"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'team' : ['A', 'B', 'A'], \n",
    "                    'points' : [1, 2, 1],\n",
    "                    'ID' : [1, 2, 3]}) \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>age</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team  age  ID\n",
       "0    A    3   1\n",
       "1    D    4   2\n",
       "2    C    5   3"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'team' : ['A', 'D', 'C'],\n",
    "                    'age' : [3,4,5],\n",
    "                    'ID' : [1, 2, 3]})\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team_x</th>\n",
       "      <th>points</th>\n",
       "      <th>ID</th>\n",
       "      <th>team_y</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team_x  points  ID team_y  age\n",
       "0      A       1   1      A    3\n",
       "1      B       2   2      D    4\n",
       "2      A       1   3      C    5"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.merge(df2, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VocabTFIDFAll.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Stereotype\n",
    "\n",
    "In the following block of code we take an 80% sample of all stereotype sessions found in SWC, and then calculate the TF-IDF values for each all queries with each query in every session seen as an individual document. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 14837.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "# allQueries = list(set(allQueries))\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['class'] == 1]\n",
    "# sID = allSessionsQ['sID'].unique()\n",
    "# corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "# queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "# allQueries = list(set(allQueries))\n",
    "allSessions = allSessions[allSessions['class'] == 1]\n",
    "sID = allSessions['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessions = allSessions[allSessions['sID'].isin(corpus)]\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "    \n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(querywords)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for m in vectors:\n",
    "        if(m.sum() != 0):\n",
    "            listTFIDF.append(m.sum() / m.count_nonzero())\n",
    "        else:\n",
    "            listTFIDF.append(-1)\n",
    "        pbar.update()\n",
    "\n",
    "        \n",
    "VocabTFIDF = pd.DataFrame(data=listTFIDF, columns = ['tfidfS']).fillna(-1)\n",
    "# VocabTFIDF['query'] = allQueries\n",
    "# Vocab = pd.merge(Vocab, VocabTFIDF, on='query')\n",
    "# Vocab = Vocab.merge(VocabTFIDF, on = 'query')\n",
    "VocabTFIDF['qID'] = qID\n",
    "Vocab = pd.merge(Vocab, VocabTFIDF, on='qID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why is idaho called the gem state',\n",
       " 'a famos mathution',\n",
       " 'motion waves',\n",
       " 'Victorea stillwells Big moments',\n",
       " 'Information about tigers',\n",
       " 'science',\n",
       " 'fact on tigers',\n",
       " 'meow meow meow meow meow meow meow meow meow meow meow',\n",
       " 'google docs',\n",
       " 'Videos of Victoria stillwell',\n",
       " 'Deathly Hallows part 2',\n",
       " 'test search!',\n",
       " 'South Africa facts kids',\n",
       " 'What is the weeknds most famous song',\n",
       " 'cast',\n",
       " \"when's the last time you let your heart decide?\",\n",
       " 'The leopard project done by Andrew kitties',\n",
       " 'Guatemala fun facts for kids',\n",
       " 'facrs',\n",
       " 'difference between dogs and wolves',\n",
       " 'What is the new son by son and son?',\n",
       " 'When is the Fast and ferious 9 coming in theaters',\n",
       " 'Spirited away',\n",
       " 'interesting fact about space',\n",
       " 'Caculator',\n",
       " 'Ada Lovelace biography for kids',\n",
       " 'Where was mother Teresa born',\n",
       " 'the weeney',\n",
       " 'how do humans use water',\n",
       " 'equal setam',\n",
       " 'what is the state bird of idaho is',\n",
       " 'Glg',\n",
       " 'math',\n",
       " 'Bethany Hamilton 2nd kid name',\n",
       " 'facts about birds',\n",
       " 'Bethany Hamilton Family',\n",
       " 'Glo',\n",
       " 'what is a cheeta',\n",
       " 'bal  done',\n",
       " 'Ways humans use water',\n",
       " 'X facts',\n",
       " 'JK Roling video life',\n",
       " 'pop corn',\n",
       " 'JK Roling family',\n",
       " 'fun facts about bender',\n",
       " 'whats a study of robots',\n",
       " 'Funcki',\n",
       " 'maps',\n",
       " 'fox animal',\n",
       " 'Sylvia earle s family',\n",
       " 'The Fast and ferious 7 movie',\n",
       " 'story design for video games',\n",
       " 'Helen Keller kids biography',\n",
       " 'animal life cycles',\n",
       " 'world war II pictures',\n",
       " 'Slvyia earl',\n",
       " 'Billy',\n",
       " 'When will the movie Zootopia come out',\n",
       " 'Who plays the rabbit from the movie H.O.P.?',\n",
       " 'How long is a lions Lifespan?',\n",
       " 'oranizations that save animals in south america',\n",
       " 'Kr',\n",
       " 'who made the NAO robot',\n",
       " 'Desmond doss quote',\n",
       " 'Dinosur',\n",
       " 'What is the ame of the song with the lyrics ? Now watch me whip watch me nay nay!',\n",
       " 'short circuit cool fact',\n",
       " 'coke',\n",
       " 'What does it mean to be a colony',\n",
       " 'dog wailk asdf asdf asdf asdf asdf asdf asdf asdf asdf asdf asdf asdf asdfsdf dsfddddddddddddddddddddddddddddddddddddddddddddddd',\n",
       " 'panda',\n",
       " 'height of a t-rex',\n",
       " 'How to contact NASA',\n",
       " 'the tittle of the movie',\n",
       " 'amazon prime',\n",
       " 'Pointless website',\n",
       " 'Wen did Victoria Stilwell make the show called its me or the dog .',\n",
       " 'Where did Laura Ingles wilder died',\n",
       " 'dino facts',\n",
       " 'About Helen Keller s family',\n",
       " 'Larry poppins',\n",
       " 'Sumdog Math games',\n",
       " 'When was Albert Einstein born',\n",
       " 'minecraft',\n",
       " 'Billie Ellish',\n",
       " \"Assassin's Creed\",\n",
       " 'Okay',\n",
       " 'how old can dogs grow to be',\n",
       " 'Peru for kids',\n",
       " 'robot girls',\n",
       " 'find the famous mathematicians',\n",
       " 'how tall was tyrannosaurus rex',\n",
       " \"why don't little night mares coupons exist\",\n",
       " 'when is Selena gomez new songs are coming out',\n",
       " 'When does the new m donlads goin to be buiolt',\n",
       " 'how long have lions been extirced',\n",
       " 'Star Wars the Force Awakens',\n",
       " 'state bird of idaho',\n",
       " 'Draft Day movie',\n",
       " 'find ME A INTERESTING FACT ABOUT DOGS',\n",
       " 'What are the most famous robots in the world',\n",
       " 'movies for to day at the edwads movie theater',\n",
       " 'Cast',\n",
       " 'ryan toys',\n",
       " 'soccer.messi',\n",
       " 'When is Fuller House coming out?',\n",
       " 'who invented minecraft',\n",
       " 'red',\n",
       " \"what is idaho's nickname\",\n",
       " 'car',\n",
       " 'Quotes that AmElia',\n",
       " 'test search 8',\n",
       " 'Tesla model S',\n",
       " 'Who was the first computer programmer?',\n",
       " 'why do we need water',\n",
       " 'how far is the sun from earth',\n",
       " 'Global warming organizations',\n",
       " 'Malala  family',\n",
       " 'computer programmer',\n",
       " 'damnit',\n",
       " 'Chicken cow',\n",
       " 'robop',\n",
       " 'memes',\n",
       " 'search test',\n",
       " 'new maclelamore song',\n",
       " 'NASA',\n",
       " 'Helen Keller biographe',\n",
       " 'who needs water?',\n",
       " 'boise state',\n",
       " 'Fiesh',\n",
       " 'why am i so specific',\n",
       " 'albert einstein',\n",
       " 'Bethany Hamilton Quotes 99  ',\n",
       " 'september 19 1978',\n",
       " 'Facts about geckos',\n",
       " 'water',\n",
       " 'when is the next season of stanly supperre humans',\n",
       " 'Dinoser',\n",
       " 'Wangari Maathai birth',\n",
       " 'largest country',\n",
       " 'Dinoser facts',\n",
       " 'What was the first colonies',\n",
       " 'find me a difference between earth and mars',\n",
       " 'bradon',\n",
       " 'Where do vicu as live',\n",
       " 'What songs has a name with the lyrics of \"You use to call my on my self phone\"',\n",
       " 'about Basenji dong time',\n",
       " 'who was the actor of',\n",
       " 'youtube',\n",
       " 'how long do horses live',\n",
       " 'White House',\n",
       " 'three.js how to make shapes clickable',\n",
       " 'Peru',\n",
       " 'belittle',\n",
       " 'abcya',\n",
       " 'When is the next StarWars movie coming out',\n",
       " 'froze',\n",
       " 'what Year was Mauchu poaching',\n",
       " 'When is Batman v. Superman coming out?',\n",
       " 'what are the first ten digits of pi',\n",
       " 'Large es state',\n",
       " 'cat dog',\n",
       " 'fun facts about robots',\n",
       " 'what is the study of robots called',\n",
       " 'who was the first president',\n",
       " 'Who plays Nemo in finding Nemo?',\n",
       " 'hola',\n",
       " 'verchrl robot',\n",
       " 'Dogs 1',\n",
       " 'fishing in boise',\n",
       " 'Companies   That explore  Maasai Mara',\n",
       " 'Victoria stillwells family',\n",
       " 'facts about albert einstien',\n",
       " 'crocodiles',\n",
       " 'animal videos',\n",
       " 'Famous quotes by Susan B Anthony',\n",
       " 'cuke',\n",
       " 'dog parks',\n",
       " 'Laura Ingles wilder brave quotes',\n",
       " 'Parts of speech',\n",
       " 'state bird boise ID',\n",
       " 'dawg',\n",
       " 'Billbow - Bagan in the Hobbit',\n",
       " 'Marcus Gilmore Edson parents',\n",
       " 'how toll are x-rex',\n",
       " \"dogs that don't shed that are smalll\",\n",
       " 'Peru For  KIDS',\n",
       " 'colony',\n",
       " 'WHAT IS THE STATE BIRD OF IDAHO',\n",
       " 'the differences between earth and mars',\n",
       " 'North mythology',\n",
       " 'ise beer',\n",
       " 'Interesting fact about albert einstein',\n",
       " 'dg',\n",
       " 'Consitution',\n",
       " 'Idaho fish and game',\n",
       " 'Water',\n",
       " 'How did Anabell flote',\n",
       " 'oranizations that save animals',\n",
       " 'Dinosaur',\n",
       " 'insects',\n",
       " 'quizlet',\n",
       " 'Where is Switzerland on the map',\n",
       " 'who made the first video game',\n",
       " 'What are the first ten digits in pi',\n",
       " 'living vs nonliving',\n",
       " 'the first musician',\n",
       " 'linux test thanks',\n",
       " 'Laura Ingles wilder mom',\n",
       " 'sports and news',\n",
       " 'How cold is it usually in Boise Idaho in winter',\n",
       " 'information of words',\n",
       " 'star wars ep. 5',\n",
       " 'How old is Johny enlgish?',\n",
       " 'Planet earth   ',\n",
       " 'Star Wars',\n",
       " 'who is the first computer programmer?',\n",
       " 'a fact about albert einstein',\n",
       " 'Watdr',\n",
       " 'how th',\n",
       " 'Ada Lovelace family',\n",
       " 'South Africa',\n",
       " 'cammels',\n",
       " 'ghandi',\n",
       " 'what is the suited of robots',\n",
       " 'solar sistem',\n",
       " 'spelling lity',\n",
       " 'what are the first ten digits in pi ?',\n",
       " 'what do banks do?',\n",
       " 'what is the height of a tyrannosaurus rex',\n",
       " 'Tacare video Jane Goodall',\n",
       " 'Wangari Maathai',\n",
       " 'organizations that protect endangered animals in south america',\n",
       " 'roblox',\n",
       " 'Character from monster high',\n",
       " 'Ada Lovelace quotes',\n",
       " 'diameter of the earth',\n",
       " 'bugs',\n",
       " 'interesting fact about einstein',\n",
       " 'J. K. Rowling',\n",
       " 'Vere si du bist  ein  frau',\n",
       " 'manacle hand',\n",
       " 'rock cycle',\n",
       " 'facts about tigers',\n",
       " 'hkhlkhj',\n",
       " 'How do they make the chewey voice starwars',\n",
       " 'Quotes from victor w',\n",
       " 'Explorers of the Maasai Mara',\n",
       " 'Largest country In the World',\n",
       " 'uiuiuiuiuiiiuiiuiuiuiuiiuiuiuiuiuiiuiuiuiui uoi oi o ioiooooiooiooioioioioiooioioioioioiuiuiuiuiuiuiiuiuiuu',\n",
       " 'games',\n",
       " 'Who played sander in The Walking Dead',\n",
       " 'Wangari Maathai birth date',\n",
       " 'Billie Eillish',\n",
       " 'animal',\n",
       " 'xpmath',\n",
       " 'name of a famous mathematician',\n",
       " 'what was the name if the first computer programmer',\n",
       " 'crechur',\n",
       " 'the Incredibles',\n",
       " 'mincraft songs',\n",
       " 'cool fact about space',\n",
       " 'google stadia',\n",
       " 'calculator',\n",
       " 'armer games',\n",
       " 'Poppys',\n",
       " 'frozen sven',\n",
       " 'beavers',\n",
       " 'test search 123 test',\n",
       " 'Victoria stillwells Hard times',\n",
       " 'Rood',\n",
       " 'Fvffffffffffffccccccc,cast',\n",
       " 'bangladesh',\n",
       " 'soccer.neatness',\n",
       " 'Meow',\n",
       " 'who is the first scientist that invented a robot',\n",
       " 'Shark facts',\n",
       " 'who was the faist computer programmer',\n",
       " 'ocean clean up organizations',\n",
       " 'raild',\n",
       " 'wit is the stud robs',\n",
       " 'Save the forests from pulushin',\n",
       " 'Iran facts for kids',\n",
       " 'child labor',\n",
       " 'alphabet',\n",
       " 'Danica Patrick',\n",
       " 'Desmond Doss the hero',\n",
       " 'When is star wars going to come out',\n",
       " 'Songs buy Charlie Puth',\n",
       " 'what was the first dinosaur discovered',\n",
       " 'How do humans use water',\n",
       " 'Victoria Stilwel',\n",
       " 'start wars',\n",
       " 'What is ecosystem',\n",
       " 'difference between mars and earth',\n",
       " 'What is the biggest rock found on Mars?',\n",
       " 'taste search incoming 2',\n",
       " 'about Basenji',\n",
       " 'Hedgesffvvgvbybyby tv tbtv gtvvtbtbtbtbtbtbtggtbttbthbyngmunhnh hnunumumjmimimimimimimuummununu tv',\n",
       " 'Cohbuj',\n",
       " 'wat is a robot made out of',\n",
       " 'lighting rods',\n",
       " 'how tall is donald trump',\n",
       " 'beaver diet',\n",
       " 'Ca tv',\n",
       " 'hlhlh',\n",
       " 'How many tries did it take for Albert Einstein to figure out the theory of relativity',\n",
       " 'how tall was a tyrannosaurus',\n",
       " 'find me a fact about albert einstein',\n",
       " 'geyser in yellowstone',\n",
       " 'how many adolescents play minecraft',\n",
       " 'Treater',\n",
       " 'dinoser',\n",
       " 'fish',\n",
       " 'Helo',\n",
       " 'Why do dogs wag their tail',\n",
       " 'scientist who invented the robot',\n",
       " 'snake',\n",
       " 'Did Bethany Hamilton say any Quotes',\n",
       " 'watch',\n",
       " 'test search misspelling',\n",
       " 'name of the first computer programmer',\n",
       " 'game',\n",
       " \"mento's foam\",\n",
       " 'idaho hhistory',\n",
       " 'doog',\n",
       " 'what planet is closest to the sun',\n",
       " 'wolf and dog difference',\n",
       " 'Malala mom',\n",
       " 'p',\n",
       " 'jo bidden',\n",
       " 'about poodles dog time',\n",
       " 'Large es stat',\n",
       " 'Ameraca',\n",
       " 'Yemen fun facts for kids',\n",
       " \"i haven't used an apostrophe but now i did thanks\",\n",
       " 'soe',\n",
       " 'How many langenges was Jurassic world made in',\n",
       " 'test search 6',\n",
       " 'mary had a little lamb',\n",
       " 'pupee',\n",
       " 'cake',\n",
       " 'intresting',\n",
       " 'Nikola Tesla berth',\n",
       " 'Bethany Hamilton birth place',\n",
       " 'Ti',\n",
       " 'kids',\n",
       " 'Where do monkeys live',\n",
       " 'what es idaho stat bird',\n",
       " 'The new starwars',\n",
       " 'whale',\n",
       " 'Info on foxes',\n",
       " 'interesting fact about dogs',\n",
       " 'Volcanos',\n",
       " 'MIG games',\n",
       " 'did dinosaurs have feathers',\n",
       " 'hoe',\n",
       " 'state bird of Idaho',\n",
       " 'Calvin and hobbis',\n",
       " 'how was the first person to make the computer program',\n",
       " 'What war did Desmond Doss fight in',\n",
       " 'dinoser test',\n",
       " 'element robots',\n",
       " 'example of a leamer',\n",
       " 'raindeer',\n",
       " 'Sylvia earl',\n",
       " 'Largest country',\n",
       " 'What is the largest country in the world?',\n",
       " 'my favourite movie',\n",
       " 'Rabit',\n",
       " 'What is NASA s address',\n",
       " 'How to make mint chocolate chip ice cream',\n",
       " 'facebook',\n",
       " 'who was the first computer programmer',\n",
       " 'Norway for kid',\n",
       " 'Helen Keller When was Helen Keller born',\n",
       " 'Dogs',\n",
       " 'What are saturns rings made of',\n",
       " 'kidney',\n",
       " 'When is the new ghost busters coming out',\n",
       " 'The fights of a Tyrannosaurus Rex',\n",
       " 'new finding nemo movie',\n",
       " 'row bots',\n",
       " 'Bethany Hamilton Quotes',\n",
       " 'cricket',\n",
       " 'youtube videos',\n",
       " 'South Africa facts for kids   FACTS',\n",
       " 'Planet Mars',\n",
       " 'how computers and robots work',\n",
       " 'coollonys',\n",
       " 'Balll pythons banana',\n",
       " 'Search',\n",
       " 'The leopard project done by Andrew kittle',\n",
       " 'Norway for kids',\n",
       " 'Test search',\n",
       " 'albert instin',\n",
       " 'norse mytology',\n",
       " 'Boise fun',\n",
       " 'Why was Dian fossey  a hero ',\n",
       " 'who was the first  computer programmer',\n",
       " 'why are there falling trees in beavers eco-sysitem',\n",
       " 'What is the capital of Spain?',\n",
       " 'Bog',\n",
       " 'Cats',\n",
       " 'cool',\n",
       " 'Dian fossy',\n",
       " 'how to manufatcure  airplane',\n",
       " 'JK Roling video',\n",
       " 'what was the first computer program',\n",
       " 'Pizza',\n",
       " 'equal seatam',\n",
       " 'obscene',\n",
       " 'dinoser fakts are intresting',\n",
       " 'Mother Teresa biography',\n",
       " 'What do Vicu as eat',\n",
       " 'alligator',\n",
       " 'boise stae university dagree',\n",
       " 'games to play when you board',\n",
       " 'Computer games',\n",
       " 'seasons',\n",
       " 'Wen was Marcus Gilmore Edson born',\n",
       " 'What was Lucy (from chronicels of Narnia) age',\n",
       " 'Facts about Norway',\n",
       " 'How to save whales',\n",
       " 'name of a famous mathaticon',\n",
       " 'weather',\n",
       " 'find a fact about  albert einstein',\n",
       " 'Date of death for Amelia Earhart',\n",
       " 'mamoth facts',\n",
       " 'ecosystem',\n",
       " 'how far is the earthe from the sun',\n",
       " 'What is the capital of Norway',\n",
       " 'Pop',\n",
       " 'the cat and the hat',\n",
       " 'amino acid',\n",
       " 'I m going through my phone to call my phone now just to',\n",
       " 'i skarch of word work',\n",
       " 'Hi',\n",
       " '    ',\n",
       " 'The leopard project',\n",
       " 'What is the movie \"The Secret life of pets\"',\n",
       " 'Who and what needs water.',\n",
       " 'what are the first 10 digits in pie',\n",
       " 'Organizations For VICU A',\n",
       " 'who made the Nail Robot',\n",
       " 'Desmond Doss was the way to go back into my mind',\n",
       " 'What are some organizations that help the hedgehog',\n",
       " 'thanos',\n",
       " 'down under you tube',\n",
       " \"Rod's\",\n",
       " 'hoomadrobots',\n",
       " 'Who is the name of a Macdre song?',\n",
       " 'what is idahos biggest crop',\n",
       " 'hippos',\n",
       " 'hope',\n",
       " 'who created the first computer program',\n",
       " 'studio c',\n",
       " 'technologies used in robots',\n",
       " 'what was the first computer programer',\n",
       " 'Facts about dogs',\n",
       " 'Discombobulated',\n",
       " 'who is the 1st computer programmer',\n",
       " 'staggering bautey (a game)',\n",
       " 'songs',\n",
       " 'paw patrol',\n",
       " 'what are the first ten digits in pi',\n",
       " 'the hight of  a tyrannosaurus rex',\n",
       " 'hight of t rex',\n",
       " 'Diniser facts',\n",
       " 'Famous landmarks in Norwa',\n",
       " 'how tall is a tyrannosaurus rex',\n",
       " 'national vicuna reserve',\n",
       " 'B',\n",
       " 'Friend',\n",
       " 'who was the first computer program',\n",
       " 'frozen full movie online',\n",
       " 'spelling',\n",
       " 'When  Did Nellie bly get Maried',\n",
       " 'methology',\n",
       " 'yellowstone most famous geyser',\n",
       " 'Bop corn',\n",
       " 'famous math matician',\n",
       " 'how tall was a trex',\n",
       " 'good wood',\n",
       " 'How old is Ray from StarWars?',\n",
       " 'Who raise Marcellus Gilmore Edson',\n",
       " ';lkfjdsa\\\\\\\\\\\\\\\\',\n",
       " 'How tall was the tyrannosaurus rex?',\n",
       " 'what are robots',\n",
       " 'WHAT IS THE Difference between earth and mars',\n",
       " 'Chicen  noo',\n",
       " 'America',\n",
       " 'down under',\n",
       " 'aaa',\n",
       " 'Songs by Adele',\n",
       " 'What year did the Russian federation become Russia',\n",
       " 'messi',\n",
       " 'Yellowstone international park',\n",
       " 'jojo bizarre',\n",
       " 'what is an elected official',\n",
       " 'pugs',\n",
       " 'thiewhfw',\n",
       " 'Fast and the ferious',\n",
       " 'rob   ot',\n",
       " 'tyrannosaurus rex',\n",
       " 'facts about robots',\n",
       " 'robs',\n",
       " 'Buddy is gool',\n",
       " 'THe robot that can remember',\n",
       " 'dolphins',\n",
       " 'Mallory holtman  s life as Mallory  holtman s life',\n",
       " 'starwars episode the force returns',\n",
       " 'scientist who evented the robot',\n",
       " 'what is the closest planet to the sun',\n",
       " 'the state  bird of idaho',\n",
       " 'Pop corn',\n",
       " '<MEOWMIX>',\n",
       " 'robots on star wars',\n",
       " 'Walt Disney biographer video',\n",
       " 'Moo meow',\n",
       " 'hedgehog houses',\n",
       " 'Highschool musical',\n",
       " 'Sobelman',\n",
       " 'how cold is it in idaho?',\n",
       " 'math games',\n",
       " 'Norway for kidney',\n",
       " 'jojo',\n",
       " 'how many people are in the world',\n",
       " 'The Alpaca',\n",
       " 'Who plays the good terminater in Terminater two',\n",
       " 'find me a distance between the earth and mars',\n",
       " 'Fuller House Full episodes',\n",
       " 'find me a cool fact about space',\n",
       " 'the first ten digits of pine',\n",
       " 'what is the study of robot',\n",
       " 'about Lhasa Apsodog time',\n",
       " 'Famous quotes by Nellie Bly',\n",
       " 'Water cof',\n",
       " 'How many phones sell in a day',\n",
       " 'favorite',\n",
       " 'do a barrel roll',\n",
       " 'Dog',\n",
       " 'Facts about Norway for kids',\n",
       " 'why are there falling trees in beavers eco-system',\n",
       " 'he',\n",
       " 'Walt Disney biology',\n",
       " 'Jurassic world',\n",
       " 'where does sink water come from',\n",
       " 'funny dog pictures',\n",
       " 'how does flooding caused by beaver dams effect plants',\n",
       " 'How tall was the tyrannosaurus rex',\n",
       " 'what is the tallest dinosers',\n",
       " 'bevers',\n",
       " 'how tall was the tyrannosaurus rex',\n",
       " 'ideas for poems',\n",
       " 'Albert Einstein',\n",
       " 'doig',\n",
       " 'the scientist who invented robots',\n",
       " 'a cool fact about space',\n",
       " 'Constitution',\n",
       " 'Anne Frank quotes',\n",
       " 'Fantastic hair doos',\n",
       " 'friv',\n",
       " 'who voiced Mr.Incredible',\n",
       " 'google',\n",
       " 'Phone search',\n",
       " 'ghandi biography',\n",
       " 'Plastic soap pie bekaese',\n",
       " 'nhight of a t-rex',\n",
       " 'frog',\n",
       " 'Mallory holtman',\n",
       " 'Images of dragons',\n",
       " 'Murry poppins',\n",
       " 'albert enstein',\n",
       " 'idaho hostoruyHistory',\n",
       " 'Ball pythons banana spider',\n",
       " 'Albert Einstein s most difficult part of his life',\n",
       " 'bees',\n",
       " '!#$#%#^^&&*',\n",
       " 'find me an interesting fact about a dinosaur',\n",
       " 'Hfgmhvhjcfhjc facts for kids',\n",
       " 'norse mythology',\n",
       " 'Middle',\n",
       " 'dffdf',\n",
       " 'okay simple test search! lets go',\n",
       " 'Xklapone',\n",
       " 'solar system works',\n",
       " 'sylvia Earie',\n",
       " 'upcoming games for nintendo switch',\n",
       " 'Videos of Victoria stillwell With Dogs',\n",
       " 'find an interesting fact a boat dog',\n",
       " 'What was the largest dinosaur',\n",
       " 'Sylvia Earle quotes',\n",
       " 'the but',\n",
       " 'tin digits of pie',\n",
       " 'What is the top game this week?',\n",
       " 'who is the first computer prorammer',\n",
       " 'The Incredibles',\n",
       " 'first computer scientist',\n",
       " 'Maathai family names',\n",
       " 'ddffffff',\n",
       " 'the first computer programmer',\n",
       " 'Kissing video',\n",
       " 'urban dictionary',\n",
       " 'Hat',\n",
       " 'About idaho',\n",
       " 'Who plays the bad guy in Star Wars the Horde awakends?',\n",
       " 'interesting fact about robots',\n",
       " 'Helen Keller  for kids',\n",
       " 'eco system',\n",
       " 'coding',\n",
       " 'how many four movies are there',\n",
       " 'Refresh!',\n",
       " 'websites for facts',\n",
       " 'Whats the song I cut my hangs with some rusty kitchen sissors',\n",
       " 'movie',\n",
       " 'flying dog',\n",
       " 'famous mathematictions',\n",
       " 'diameter of the sun',\n",
       " 'tortoises',\n",
       " 'mars dose not have air',\n",
       " 'what is the height of a tyrannosaurs rex',\n",
       " 'Vicu a organizations',\n",
       " 'Victoria Stilwell biography',\n",
       " 'Famous quotes of Susan B Anthony',\n",
       " 'brian r clow',\n",
       " 'hello! test here dinosaur yes lets go',\n",
       " 'dinoser fakts',\n",
       " 'Norway',\n",
       " 'how will win a bull shark or a anaconda',\n",
       " 'who made the first robot',\n",
       " 'test again',\n",
       " 'rabeis',\n",
       " 'about idaho',\n",
       " 'A whole with a horn',\n",
       " 'baseballs.neatness',\n",
       " \"Albert Einstein's family\",\n",
       " 'lionfish',\n",
       " 'When is the new finding nemo movie comieng out',\n",
       " 'what causes an earthquake',\n",
       " 'who as the first computer programmer\\\\\\\\',\n",
       " 'Largess state',\n",
       " 'what are robots made of',\n",
       " 'Dinosaur facts interesting okay love',\n",
       " \"minto's foam\",\n",
       " 'ahoy there maytey',\n",
       " 'Ant-man movie',\n",
       " 'games',\n",
       " 'Military Mechanical Dog',\n",
       " 'the name of the first computer programer',\n",
       " 'qwerty',\n",
       " 'nepal',\n",
       " 'firefox test!',\n",
       " 'what es idaho stae bird',\n",
       " 'how many people in Spain speak spanish',\n",
       " 'jingle belles',\n",
       " 'baby yoda',\n",
       " 'hhh',\n",
       " 'Iran',\n",
       " 'how tall was a tyrannosaurus rex',\n",
       " 'Sylvia Earle family member names',\n",
       " 'Dinosdt facts',\n",
       " 'Cats bts',\n",
       " 'dionser',\n",
       " 'What is one of the 8 largest dinosaurs',\n",
       " 'indeginous facts',\n",
       " 'water robots',\n",
       " 'what did albert alberstin sudy',\n",
       " 'Hellen Keller',\n",
       " 'earth day',\n",
       " 'Di easier',\n",
       " 'Military heroes',\n",
       " 'Jurrasic wortl',\n",
       " 'healthy meals',\n",
       " 'science lesson',\n",
       " 'sylvia Eerie',\n",
       " 'Google',\n",
       " 'A COOL FACT ABOUT SPACE',\n",
       " 'How old is TinkleBell',\n",
       " 'what is the Idaho state bird',\n",
       " 'l gdlfk gdlfkg dlkfg dlg sdgdlg dlfg dlfkgj dflg dfg jdfg d',\n",
       " 'Who plays Kylo Ren from StarWars?',\n",
       " 'kllllllllllllllllllllkkkkkkkkkkkkkkkjnnnnnnnn',\n",
       " 'interesting facts about dogs',\n",
       " 'fishing in idaho',\n",
       " 'how tall is a tranosoris',\n",
       " 'I a i',\n",
       " 'Con',\n",
       " 'Famous quotes by Dian Fossey',\n",
       " 'biggest fish',\n",
       " 'hjljkhl',\n",
       " 'how many theeth do the t rex have',\n",
       " 'Child MMR vaccine in boise',\n",
       " 'Bethany Hamilton parents',\n",
       " 'enter sandman',\n",
       " 'What is the',\n",
       " 'how do fallen  tree effect the forest',\n",
       " 'find me 10 cool facts about space',\n",
       " 'the scientist that invented robot',\n",
       " 'dinosar',\n",
       " 'Nikola Tesla',\n",
       " 'When does summer break start?',\n",
       " 'Marcus Gilmore Edson',\n",
       " 'frog kung fu',\n",
       " 'hello',\n",
       " 'Amelia Earhart s young life',\n",
       " 'santu',\n",
       " 'tumblr',\n",
       " 'damnitd',\n",
       " 'sophia carson\\\\\\\\',\n",
       " 'When is the new ghost busters comeing out',\n",
       " 'Star wars',\n",
       " 'test search 10',\n",
       " 'Victoria stillwell video',\n",
       " 'How many people are in Norway?',\n",
       " 'fox',\n",
       " 'jldfdjfgoiuertruoifdjkgf',\n",
       " 'YouTub',\n",
       " 'Yellowstone international school',\n",
       " 'dsfad',\n",
       " 'fact about albert einstein',\n",
       " 'When did Jane Goodalls second husband die',\n",
       " 'test search',\n",
       " 'Spain kids facts',\n",
       " 'Robot facs',\n",
       " 'fact about space',\n",
       " 'wat is the stat bird of idaho',\n",
       " 'camel',\n",
       " 'WHAT ARE THE FIRST 10 Digits OF PI',\n",
       " 'Star wars',\n",
       " 'Marcus gilmore Edson',\n",
       " 'Dantdm',\n",
       " 'linux Firefox test',\n",
       " 'Gogo',\n",
       " 'Harry potter sward in the stone',\n",
       " 'Actor of Hulk in Avengers',\n",
       " 'fortnite',\n",
       " 'Special soldiers',\n",
       " 'how far is the sun',\n",
       " 'how tall is the tranisores rex',\n",
       " 'animal dog',\n",
       " 'facts about dog',\n",
       " 'how many miles is it to the moon?',\n",
       " 'hello! test search here',\n",
       " 'images for project',\n",
       " 'Facts about Spain for kids',\n",
       " 'what is the height of a trex',\n",
       " 'dogs',\n",
       " 'Mother Teresa quotes',\n",
       " 'who was the first cumputer programer',\n",
       " 'loveable',\n",
       " 'When  Did Nellie bly get Marie s',\n",
       " 'the first compt',\n",
       " 'Laura Ingles wilder have kids',\n",
       " 'How many Star Wars movies will be made?',\n",
       " 'House of dreams',\n",
       " 'When did Desmond doss get married',\n",
       " 'H',\n",
       " 'kid']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidfS</th>\n",
       "      <th>qID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.706882</td>\n",
       "      <td>6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>5008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>6503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.667439</td>\n",
       "      <td>6778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.575734</td>\n",
       "      <td>5381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidfS   qID\n",
       "0   0.706882  6352\n",
       "1  -1.000000  8305\n",
       "2  -1.000000  6814\n",
       "3  -1.000000  7688\n",
       "4  -1.000000  6221\n",
       "5  -1.000000  6645\n",
       "6  -1.000000  9056\n",
       "7  -1.000000  7349\n",
       "8   0.707107  5008\n",
       "9   0.707107  6503\n",
       "10  1.000000  9686\n",
       "11  1.000000  9312\n",
       "12  1.000000  8535\n",
       "13  1.000000  8594\n",
       "14  1.000000  8956\n",
       "15  1.000000  9308\n",
       "16  1.000000  7015\n",
       "17  0.667439  6778\n",
       "18  1.000000  7596\n",
       "19  1.000000  5709\n",
       "20  1.000000  9544\n",
       "21  1.000000  7255\n",
       "22  1.000000  9521\n",
       "23  0.575734  5381\n",
       "24 -1.000000  5992\n",
       "25 -1.000000  6014\n",
       "26 -1.000000  7752\n",
       "27  1.000000  5267\n",
       "28  1.000000  6232\n",
       "29  1.000000  5808"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabTFIDF.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>...</th>\n",
       "      <th>top250NonSterRatCon</th>\n",
       "      <th>top50SterCount</th>\n",
       "      <th>top50SterRatAnt</th>\n",
       "      <th>top50SterAntCon</th>\n",
       "      <th>top50NonSterCount</th>\n",
       "      <th>top50NonSterRatAnt</th>\n",
       "      <th>top50NonSterAntCon</th>\n",
       "      <th>qID</th>\n",
       "      <th>tfidfAll</th>\n",
       "      <th>tfidfS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6352</td>\n",
       "      <td>0.491051</td>\n",
       "      <td>0.706882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8305</td>\n",
       "      <td>0.704467</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   coreVocab                query  nonCoreVocab  minAoA  maxAoA  ratioAoA  \\\n",
       "0        0.5  US civil war causes           0.5    0.00   10.89      0.75   \n",
       "1        0.0       scooter brands           1.0    6.68    7.72      1.00   \n",
       "\n",
       "   queryComplexity  SVEN  ratioAbs  ratioConc  ...  top250NonSterRatCon  \\\n",
       "0              6.1   0.5      0.25       0.25  ...                 0.75   \n",
       "1              7.2   0.5      0.00       0.50  ...                 0.50   \n",
       "\n",
       "   top50SterCount  top50SterRatAnt  top50SterAntCon  top50NonSterCount  \\\n",
       "0               0              0.0              1.0                  0   \n",
       "1               0              0.0              1.0                  0   \n",
       "\n",
       "   top50NonSterRatAnt  top50NonSterAntCon   qID  tfidfAll    tfidfS  \n",
       "0                 0.0                 1.0  6352  0.491051  0.706882  \n",
       "1                 0.0                 1.0  8305  0.704467 -1.000000  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidfS</th>\n",
       "      <th>qID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.706882</td>\n",
       "      <td>6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tfidfS   qID\n",
       "0  0.706882  6352\n",
       "1 -1.000000  8305"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabTFIDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 2)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabTFIDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 25)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(Vocab, VocabTFIDFAll, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Vocab.columns).is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 25)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Non-Stereotype\n",
    "\n",
    "In the following block of code we take an 80% sample of all non-stereotype sessions found in SWC, and then calculate the TF-IDF values for each all queries with each query in every session seen as an individual document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 12573.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# allSessions = pickle.load( open( \"../Data/DataSets/SWC/SWC.p\", \"rb\" ) )\n",
    "# allQueries = list(set(allQueries))\n",
    "# allSessionsQ = allSessions.loc[allSessions['type']=='Q']\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['class'] == 0]\n",
    "# sID = allSessionsQ['sID'].unique()\n",
    "# corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "# allSessionsQ = allSessionsQ[allSessionsQ['sID'].isin(corpus)]\n",
    "# queries = allSessionsQ['query'].tolist()\n",
    "\n",
    "allSessions = pickle.load( open( \"../Data/DataSets/SQS/castsventrecSQS.p\", \"rb\" ) )\n",
    "# allQueries = list(set(allQueries))\n",
    "allSessions = allSessions[allSessions['class'] == 0]\n",
    "sID = allSessions['sID'].unique()\n",
    "corpus = np.random.choice(sID,int((len(sID)*.8)), replace=False)\n",
    "allSessions = allSessions[allSessions['sID'].isin(corpus)]\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "\n",
    "text = ''\n",
    "for query in queries:\n",
    "    text += query + \" \"\n",
    "    \n",
    "querywords = text.split()\n",
    "\n",
    "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "text = ' '.join(resultwords)\n",
    "\n",
    "queries = allSessions['query'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit(queries)\n",
    "vectors = vector.transform(allQueries)\n",
    "\n",
    "listTFIDF = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for m in vectors:\n",
    "        if(m.sum() != 0):\n",
    "            listTFIDF.append(m.sum() / m.count_nonzero())\n",
    "        else:\n",
    "            listTFIDF.append(-1)\n",
    "        pbar.update()\n",
    "    \n",
    "VocabTFIDFNA = pd.DataFrame(data=listTFIDF, columns = ['tfidfNS']).fillna(-1)\n",
    "# VocabTFIDFNA['query'] = allQueries\n",
    "# Vocab = pd.merge(Vocab, VocabTFIDFNA, left_index=True, right_index=True)\n",
    "# Vocab = Vocab.merge(VocabTFIDFNA, on = 'query')\n",
    "VocabTFIDFNA['qID'] = qID\n",
    "Vocab = pd.merge(Vocab, VocabTFIDFNA, on='qID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 26)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3037"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon', 'top250NonSterCount',\n",
       "       'top250NonSterRatAnt', 'top250NonSterRatCon', 'top50SterCount',\n",
       "       'top50SterRatAnt', 'top50SterAntCon', 'top50NonSterCount',\n",
       "       'top50NonSterRatAnt', 'top50NonSterAntCon', 'qID', 'tfidfAll', 'tfidfS',\n",
       "       'tfidfNS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "In the following block of code we load up a list of stopwords (not those found in NLTK) and then count the number of stopwords found in each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 26716.82it/s]\n"
     ]
    }
   ],
   "source": [
    "stopWords = []\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "with open('DataSets/stopwords.txt') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        if (row):\n",
    "            stopWords.append(st.lemmatize(row[0]))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "stopCount = []\n",
    "stopAverage= []\n",
    "st = WordNetLemmatizer()\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "        count = 0\n",
    "        for word in query.split(' '):\n",
    "            word = word.lower().strip()\n",
    "            word = re.sub(r'[^\\w\\s]','',word)\n",
    "            word = st.lemmatize(word)\n",
    "            if word in stopWords:\n",
    "                count +=1\n",
    "            else:\n",
    "                pass\n",
    "        stopCount.append(count)\n",
    "        stopAverage.append(count/len(query.split(' ')))\n",
    "        pbar.update()\n",
    "    \n",
    "VocabStop = pd.DataFrame(data=stopCount, columns = ['stopCount'])\n",
    "# VocabStop['query'] = allQueries\n",
    "# Vocab = pd.merge(Vocab, VocabStop, left_index=True, right_index=True)\n",
    "# Vocab = Vocab.merge(VocabStop, on = 'query')\n",
    "VocabStop['qID'] = qID\n",
    "Vocab = pd.merge(Vocab, VocabStop, on='qID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopCount</th>\n",
       "      <th>qID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>1</td>\n",
       "      <td>5975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>3</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>1</td>\n",
       "      <td>7864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>1</td>\n",
       "      <td>5316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>0</td>\n",
       "      <td>8397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stopCount   qID\n",
       "0             2  6352\n",
       "1             0  8305\n",
       "2             0  6814\n",
       "3             0  7688\n",
       "4             0  6221\n",
       "...         ...   ...\n",
       "4741          1  5975\n",
       "4742          3  5233\n",
       "4743          1  7864\n",
       "4744          1  5316\n",
       "4745          0  8397\n",
       "\n",
       "[4746 rows x 2 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 27)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>...</th>\n",
       "      <th>top50SterRatAnt</th>\n",
       "      <th>top50SterAntCon</th>\n",
       "      <th>top50NonSterCount</th>\n",
       "      <th>top50NonSterRatAnt</th>\n",
       "      <th>top50NonSterAntCon</th>\n",
       "      <th>qID</th>\n",
       "      <th>tfidfAll</th>\n",
       "      <th>tfidfS</th>\n",
       "      <th>tfidfNS</th>\n",
       "      <th>stopCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6352</td>\n",
       "      <td>0.491051</td>\n",
       "      <td>0.706882</td>\n",
       "      <td>0.488738</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8305</td>\n",
       "      <td>0.704467</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.704259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.906667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6814</td>\n",
       "      <td>0.574193</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.895000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6221</td>\n",
       "      <td>0.706889</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.706948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   coreVocab                    query  nonCoreVocab  minAoA  maxAoA  ratioAoA  \\\n",
       "0        0.5      US civil war causes           0.5    0.00   10.89      0.75   \n",
       "1        0.0           scooter brands           1.0    6.68    7.72      1.00   \n",
       "2        0.0  scooter brands reliable           1.0    6.68    9.32      1.00   \n",
       "3        0.0                  scooter           1.0    6.68    6.68      1.00   \n",
       "4        0.0            scooter cheap           1.0    6.68    7.11      1.00   \n",
       "\n",
       "   queryComplexity      SVEN  ratioAbs  ratioConc  ...  top50SterRatAnt  \\\n",
       "0         6.100000  0.500000  0.250000   0.250000  ...              0.0   \n",
       "1         7.200000  0.500000  0.000000   0.500000  ...              0.0   \n",
       "2         7.906667  0.666667  0.333333   0.333333  ...              0.0   \n",
       "3         6.680000  1.000000  0.000000   1.000000  ...              0.0   \n",
       "4         6.895000  1.000000  0.500000   0.500000  ...              0.0   \n",
       "\n",
       "   top50SterAntCon  top50NonSterCount  top50NonSterRatAnt  top50NonSterAntCon  \\\n",
       "0              1.0                  0                 0.0                 1.0   \n",
       "1              1.0                  0                 0.0                 1.0   \n",
       "2              1.0                  0                 0.0                 1.0   \n",
       "3              1.0                  0                 0.0                 1.0   \n",
       "4              1.0                  0                 0.0                 1.0   \n",
       "\n",
       "    qID  tfidfAll    tfidfS   tfidfNS  stopCount  \n",
       "0  6352  0.491051  0.706882  0.488738          2  \n",
       "1  8305  0.704467 -1.000000  0.704259          0  \n",
       "2  6814  0.574193 -1.000000  0.574400          0  \n",
       "3  7688  1.000000 -1.000000  1.000000          0  \n",
       "4  6221  0.706889 -1.000000  0.706948          0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- start verify ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "if \"of\" in stopWords:\n",
    "    print('in')\n",
    "else:\n",
    "    print(\"not in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "4741    1\n",
       "4742    3\n",
       "4743    1\n",
       "4744    1\n",
       "4745    0\n",
       "Name: stopCount, Length: 4746, dtype: int64"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabStop['stopCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- end -----\n",
    "\n",
    "## VERIFIED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon', 'top250NonSterCount',\n",
       "       'top250NonSterRatAnt', 'top250NonSterRatCon', 'top50SterCount',\n",
       "       'top50SterRatAnt', 'top50SterAntCon', 'top50NonSterCount',\n",
       "       'top50NonSterRatAnt', 'top50NonSterAntCon', 'qID', 'tfidfAll', 'tfidfS',\n",
       "       'tfidfNS', 'stopCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Vocab\n",
    "\n",
    "In the following block of code we count the occurence of individual net vocabulary found in each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 519363.57it/s]\n"
     ]
    }
   ],
   "source": [
    "www = []\n",
    "com = []\n",
    "net = []\n",
    "org = []\n",
    "gov = []\n",
    "edu = []\n",
    "http = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "\n",
    "        if \"www.\" in query:\n",
    "            www.append(1)\n",
    "        else:\n",
    "            www.append(0)\n",
    "\n",
    "        if \".com\" in query:\n",
    "            com.append(1)\n",
    "        else:\n",
    "            com.append(0)\n",
    "\n",
    "        if \".net\" in query:\n",
    "            net.append(1)\n",
    "        else:\n",
    "            net.append(0)\n",
    "\n",
    "        if \".org\" in query:\n",
    "            org.append(1)\n",
    "        else:\n",
    "            org.append(0)\n",
    "\n",
    "        if \".edu\" in query:\n",
    "            edu.append(1)\n",
    "        else:\n",
    "            edu.append(0)\n",
    "\n",
    "        if \".gov\" in query:\n",
    "            gov.append(1)\n",
    "        else:\n",
    "            gov.append(0)\n",
    "\n",
    "        if \"http\" in query:\n",
    "            http.append(1)\n",
    "        else:\n",
    "            http.append(0)\n",
    "        \n",
    "        pbar.update()\n",
    "        \n",
    "VocabNet = pd.DataFrame(data=com, columns = ['com'])\n",
    "VocabNet['net'] = net\n",
    "VocabNet['org'] = org\n",
    "VocabNet['edu'] = edu\n",
    "VocabNet['gov'] = gov\n",
    "VocabNet['http'] = http\n",
    "# VocabNet['query'] = allQueries\n",
    "# Vocab = Vocab.merge(VocabNet, on = 'query')\n",
    "VocabNet['qID'] = qID\n",
    "Vocab = pd.merge(Vocab, VocabNet, on='qID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>com</th>\n",
       "      <th>net</th>\n",
       "      <th>org</th>\n",
       "      <th>edu</th>\n",
       "      <th>gov</th>\n",
       "      <th>http</th>\n",
       "      <th>qID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   com  net  org  edu  gov  http   qID\n",
       "0    0    0    0    0    0     0  6352\n",
       "1    0    0    0    0    0     0  8305\n",
       "2    0    0    0    0    0     0  6814\n",
       "3    0    0    0    0    0     0  7688\n",
       "4    0    0    0    0    0     0  6221"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VocabNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- start verify --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4746"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VocabNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidfAll</th>\n",
       "      <th>tfidfS</th>\n",
       "      <th>tfidfNS</th>\n",
       "      <th>stopCount</th>\n",
       "      <th>com</th>\n",
       "      <th>net</th>\n",
       "      <th>org</th>\n",
       "      <th>edu</th>\n",
       "      <th>gov</th>\n",
       "      <th>http</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491051</td>\n",
       "      <td>0.706882</td>\n",
       "      <td>0.488738</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704467</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.704259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.906667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574193</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   coreVocab                    query  nonCoreVocab  minAoA  maxAoA  ratioAoA  \\\n",
       "0        0.5      US civil war causes           0.5    0.00   10.89      0.75   \n",
       "1        0.0           scooter brands           1.0    6.68    7.72      1.00   \n",
       "2        0.0  scooter brands reliable           1.0    6.68    9.32      1.00   \n",
       "\n",
       "   queryComplexity      SVEN  ratioAbs  ratioConc  ...  tfidfAll    tfidfS  \\\n",
       "0         6.100000  0.500000  0.250000   0.250000  ...  0.491051  0.706882   \n",
       "1         7.200000  0.500000  0.000000   0.500000  ...  0.704467 -1.000000   \n",
       "2         7.906667  0.666667  0.333333   0.333333  ...  0.574193 -1.000000   \n",
       "\n",
       "    tfidfNS  stopCount  com  net  org  edu  gov  http  \n",
       "0  0.488738          2    0    0    0    0    0     0  \n",
       "1  0.704259          0    0    0    0    0    0     0  \n",
       "2  0.574400          0    0    0    0    0    0     0  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 33)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the \"domain name\" in the qeury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab['com'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab['net'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab['org'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab['gov'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab['http'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kid'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.loc[Vocab['http'] == 1, 'query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.loc[Vocab['org'] == 1, 'query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.loc[Vocab['com'] == 1, 'query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- end verify -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Operators\n",
    "\n",
    "In the following block of code we count the occurence of individual search operators found in each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 521868.89it/s]\n"
     ]
    }
   ],
   "source": [
    "AND = []\n",
    "OR = []\n",
    "quotes = []\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for query in allQueries:\n",
    "\n",
    "        if \"AND\" in query:\n",
    "            AND.append(1)\n",
    "        else:\n",
    "            AND.append(0)\n",
    "\n",
    "        if \"OR\" in query:\n",
    "            OR.append(1)\n",
    "        else:\n",
    "            OR.append(0)\n",
    "\n",
    "        if \"\\\"\" in query:\n",
    "            quotes.append(1) \n",
    "        else:\n",
    "            quotes.append(0)\n",
    "        \n",
    "        pbar.update()\n",
    "            \n",
    "VocabOP = pd.DataFrame(data=AND, columns = ['AND'])\n",
    "VocabOP['OR'] = OR\n",
    "VocabOP['quotes'] = quotes\n",
    "# VocabOP['query'] = allQueries\n",
    "# Vocab = Vocab.merge(VocabOP, on = 'query')\n",
    "VocabOP['qID'] = qID\n",
    "Vocab = pd.merge(Vocab, VocabOP, on='qID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 36)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- begin verify -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.loc[Vocab['OR'] == 1, 'query'] # ---? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab['quotes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab.loc[Vocab['quotes'] == 1, 'query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments:\n",
    "\n",
    "# No queries with 'AND' oparator \n",
    "\n",
    "# ------------ End verify --------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interogatives \n",
    "\n",
    "In the following block of code we determine if a query contains an interogative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4746/4746 [00:00<00:00, 104060.59it/s]\n"
     ]
    }
   ],
   "source": [
    "inter = []\n",
    "VocabInter = pd.DataFrame(data=vocab, columns = ['coreVocab'])\n",
    "\n",
    "x = len(allQueries)\n",
    "\n",
    "with tqdm(total=len(allQueries)) as pbar:\n",
    "    for num in range(x):\n",
    "        query = allQueries[num]\n",
    "\n",
    "        if re.match(r\"who( |'re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"what( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"when( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"where( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"why( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"how( |'re|re|'s|s)\", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"is \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"are \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"can \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"could \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"should \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        elif re.match(r\"would \", query, flags=re.IGNORECASE):\n",
    "            inter.append(1)\n",
    "\n",
    "        else:\n",
    "            inter.append(0)\n",
    "    \n",
    "        pbar.update() \n",
    "        \n",
    "VocabInter = pd.DataFrame(data=inter, columns = ['inter'])\n",
    "# VocabInter['query'] = allQueries\n",
    "# Vocab = Vocab.merge(VocabInter, on = 'query')\n",
    "VocabInter['qID'] = qID\n",
    "Vocab = pd.merge(Vocab, VocabInter, on='qID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>...</th>\n",
       "      <th>com</th>\n",
       "      <th>net</th>\n",
       "      <th>org</th>\n",
       "      <th>edu</th>\n",
       "      <th>gov</th>\n",
       "      <th>http</th>\n",
       "      <th>AND</th>\n",
       "      <th>OR</th>\n",
       "      <th>quotes</th>\n",
       "      <th>inter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.906667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.895000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   coreVocab                    query  nonCoreVocab  minAoA  maxAoA  ratioAoA  \\\n",
       "0        0.5      US civil war causes           0.5    0.00   10.89      0.75   \n",
       "1        0.0           scooter brands           1.0    6.68    7.72      1.00   \n",
       "2        0.0  scooter brands reliable           1.0    6.68    9.32      1.00   \n",
       "3        0.0                  scooter           1.0    6.68    6.68      1.00   \n",
       "4        0.0            scooter cheap           1.0    6.68    7.11      1.00   \n",
       "\n",
       "   queryComplexity      SVEN  ratioAbs  ratioConc  ...  com  net  org  edu  \\\n",
       "0         6.100000  0.500000  0.250000   0.250000  ...    0    0    0    0   \n",
       "1         7.200000  0.500000  0.000000   0.500000  ...    0    0    0    0   \n",
       "2         7.906667  0.666667  0.333333   0.333333  ...    0    0    0    0   \n",
       "3         6.680000  1.000000  0.000000   1.000000  ...    0    0    0    0   \n",
       "4         6.895000  1.000000  0.500000   0.500000  ...    0    0    0    0   \n",
       "\n",
       "   gov  http  AND  OR  quotes  inter  \n",
       "0    0     0    0   0       0      0  \n",
       "1    0     0    0   0       0      0  \n",
       "2    0     0    0   0       0      0  \n",
       "3    0     0    0   0       0      0  \n",
       "4    0     0    0   0       0      0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 37)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon', 'top250NonSterCount',\n",
       "       'top250NonSterRatAnt', 'top250NonSterRatCon', 'top50SterCount',\n",
       "       'top50SterRatAnt', 'top50SterAntCon', 'top50NonSterCount',\n",
       "       'top50NonSterRatAnt', 'top50NonSterAntCon', 'qID', 'tfidfAll', 'tfidfS',\n",
       "       'tfidfNS', 'stopCount', 'com', 'net', 'org', 'edu', 'gov', 'http',\n",
       "       'AND', 'OR', 'quotes', 'inter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- begin veirfy --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4323\n",
       "1     423\n",
       "Name: inter, dtype: int64"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab[\"inter\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31                         how to quit smoking \n",
       "46                  what to take on a road trip\n",
       "100                    where is dulles airport?\n",
       "192                      how to get a pay raise\n",
       "211      are developmental milestones universal\n",
       "                         ...                   \n",
       "4729               what is the height of a trex\n",
       "4733       who was the first cumputer programer\n",
       "4735           When  Did Nellie bly get Marie s\n",
       "4738    How many Star Wars movies will be made?\n",
       "4742          When did Desmond doss get married\n",
       "Name: query, Length: 423, dtype: object"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.loc[Vocab['inter'] == 1, 'query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- end verif ---------\n",
    "\n",
    "\n",
    "# VERIFIED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return Feature Set\n",
    "\n",
    "Due to the length of this notebook, we have been merging data frames as we go. Therefore, at this point we only have to return the overall feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 36)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We drop the pID\n",
    "Vocab__ = Vocab.drop(['qID'], axis=1)\n",
    "Vocab__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Vocab__, open( \"Pickles/4746VocabFeat.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coreVocab</th>\n",
       "      <th>query</th>\n",
       "      <th>nonCoreVocab</th>\n",
       "      <th>minAoA</th>\n",
       "      <th>maxAoA</th>\n",
       "      <th>ratioAoA</th>\n",
       "      <th>queryComplexity</th>\n",
       "      <th>SVEN</th>\n",
       "      <th>ratioAbs</th>\n",
       "      <th>ratioConc</th>\n",
       "      <th>...</th>\n",
       "      <th>com</th>\n",
       "      <th>net</th>\n",
       "      <th>org</th>\n",
       "      <th>edu</th>\n",
       "      <th>gov</th>\n",
       "      <th>http</th>\n",
       "      <th>AND</th>\n",
       "      <th>OR</th>\n",
       "      <th>quotes</th>\n",
       "      <th>inter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>US civil war causes</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter brands reliable</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.906667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>scooter cheap</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.895000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>1.00</td>\n",
       "      <td>House of dreams</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.88</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.196667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>0.50</td>\n",
       "      <td>When did Desmond doss get married</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.095000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>0.00</td>\n",
       "      <td>H</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>0.75</td>\n",
       "      <td>find fact about dog</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.80</td>\n",
       "      <td>6.47</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.030000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>1.00</td>\n",
       "      <td>kid</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4746 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      coreVocab                              query  nonCoreVocab  minAoA  \\\n",
       "0          0.50                US civil war causes          0.50    0.00   \n",
       "1          0.00                     scooter brands          1.00    6.68   \n",
       "2          0.00            scooter brands reliable          1.00    6.68   \n",
       "3          0.00                            scooter          1.00    6.68   \n",
       "4          0.00                      scooter cheap          1.00    6.68   \n",
       "...         ...                                ...           ...     ...   \n",
       "4741       1.00                    House of dreams          0.00    3.16   \n",
       "4742       0.50  When did Desmond doss get married          0.50    0.00   \n",
       "4743       0.00                                  H          1.00    0.00   \n",
       "4744       0.75                find fact about dog          0.25    2.80   \n",
       "4745       1.00                                kid          0.00    4.28   \n",
       "\n",
       "      maxAoA  ratioAoA  queryComplexity      SVEN  ratioAbs  ratioConc  ...  \\\n",
       "0      10.89      0.75         6.100000  0.500000  0.250000   0.250000  ...   \n",
       "1       7.72      1.00         7.200000  0.500000  0.000000   0.500000  ...   \n",
       "2       9.32      1.00         7.906667  0.666667  0.333333   0.333333  ...   \n",
       "3       6.68      1.00         6.680000  1.000000  0.000000   1.000000  ...   \n",
       "4       7.11      1.00         6.895000  1.000000  0.500000   0.500000  ...   \n",
       "...      ...       ...              ...       ...       ...        ...  ...   \n",
       "4741    4.88      1.00         4.196667  0.333333  0.333333   0.000000  ...   \n",
       "4742    5.16      0.50         2.095000  0.333333  0.500000   0.000000  ...   \n",
       "4743    0.00      0.00         0.000000  0.000000  0.000000   0.000000  ...   \n",
       "4744    6.47      1.00         5.030000  0.500000  0.750000   0.250000  ...   \n",
       "4745    4.28      1.00         4.280000  1.000000  0.000000   1.000000  ...   \n",
       "\n",
       "      com  net  org  edu  gov  http  AND  OR  quotes  inter  \n",
       "0       0    0    0    0    0     0    0   0       0      0  \n",
       "1       0    0    0    0    0     0    0   0       0      0  \n",
       "2       0    0    0    0    0     0    0   0       0      0  \n",
       "3       0    0    0    0    0     0    0   0       0      0  \n",
       "4       0    0    0    0    0     0    0   0       0      0  \n",
       "...   ...  ...  ...  ...  ...   ...  ...  ..     ...    ...  \n",
       "4741    0    0    0    0    0     0    0   0       0      0  \n",
       "4742    0    0    0    0    0     0    0   0       0      1  \n",
       "4743    0    0    0    0    0     0    0   0       0      0  \n",
       "4744    0    0    0    0    0     0    0   0       0      0  \n",
       "4745    0    0    0    0    0     0    0   0       0      0  \n",
       "\n",
       "[4746 rows x 37 columns]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['coreVocab', 'query', 'nonCoreVocab', 'minAoA', 'maxAoA', 'ratioAoA',\n",
       "       'queryComplexity', 'SVEN', 'ratioAbs', 'ratioConc', 'top250SterCount',\n",
       "       'top250SterRatAnt', 'top250SterRatCon', 'top250NonSterCount',\n",
       "       'top250NonSterRatAnt', 'top250NonSterRatCon', 'top50SterCount',\n",
       "       'top50SterRatAnt', 'top50SterAntCon', 'top50NonSterCount',\n",
       "       'top50NonSterRatAnt', 'top50NonSterAntCon', 'qID', 'tfidfAll', 'tfidfS',\n",
       "       'tfidfNS', 'stopCount', 'com', 'net', 'org', 'edu', 'gov', 'http',\n",
       "       'AND', 'OR', 'quotes', 'inter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4746, 36)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Vocab dataframe contain a 'qID' column which is the 'sID' from the original dataset. It was added for the sake of merging in order to avoide duplicates. Before saving the 'VocabFeat.p' final dataframe, we drop the 'qID' and save it to save it do 'Vocab__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
