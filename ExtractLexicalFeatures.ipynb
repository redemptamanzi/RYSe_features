{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts the lexical features from each query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "import string\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt\n",
    "from math import log\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "stanza.download('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Lexical Complexity\n",
    "\n",
    "Code taken from:\n",
    "\n",
    "This code is the lexical complexity analyzer described in\n",
    "\n",
    "Lu, Xiaofei (2012). The relationship of lexical richnes to the quality \n",
    "of ESL speakers' oral narratives. The Modern Language Journal, 96(2), 190-208. \n",
    "\n",
    "Version 1.1 Released on February 12, 2013\n",
    "\n",
    "Which can be found at:\n",
    "\n",
    "http://www.personal.psu.edu/xxl13/download.html\n",
    "\n",
    "It has been modified to work with search queries, as it was initially designed for sentences.\n",
    "\n",
    "All but getLex() were imported from that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDW for first z words in a sample\n",
    "def getndwfirstz(z,lemmalist):\n",
    "    ndwfirstztype={}\n",
    "    for lemma in lemmalist[:z]:\n",
    "        ndwfirstztype[lemma]=1\n",
    "    return len(ndwfirstztype.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust minimum sample size here\n",
    "standard=50\n",
    "\n",
    "# Returns the keys of dictionary d sorted by their values\n",
    "def sort_by_value(d): \n",
    "    items=d.items() \n",
    "    backitems=[ [v[1],v[0]] for v in items] \n",
    "    backitems.sort()\n",
    "    return [ backitems[i][1] for i in range(0,len(backitems))]\n",
    " \n",
    "# NDW for first z words in a sample\n",
    "def getndwfirstz(z,lemmalist):\n",
    "    ndwfirstztype={}\n",
    "    for lemma in lemmalist[:z]:\n",
    "        ndwfirstztype[lemma]=1\n",
    "    return len(ndwfirstztype.keys())\n",
    "\n",
    "# NDW expected random z words, 10 trials\n",
    "def getndwerz(z,lemmalist):\n",
    "    ndwerz=0\n",
    "    for i in range(10):\n",
    "        ndwerztype={}\n",
    "        erzlemmalist=random.sample(lemmalist,z) \n",
    "        for lemma in erzlemmalist:\n",
    "            ndwerztype[lemma]=1\n",
    "        ndwerz+=len(ndwerztype.keys())\n",
    "    return ndwerz/10.0\n",
    "\n",
    "# NDW expected random sequences of z words, 10 trials\n",
    "def getndwesz(z,lemmalist):\n",
    "    ndwesz=0\n",
    "    for i in range(10):\n",
    "        ndwesztype={}\n",
    "        startword=random.randint(0,len(lemmalist)-z)\n",
    "        eszlemmalist=lemmalist[startword:startword+z]\n",
    "        for lemma in eszlemmalist:\n",
    "            ndwesztype[lemma]=1\n",
    "        ndwesz+=len(ndwesztype.keys())\n",
    "    return ndwesz/10.0\n",
    "\n",
    "# MSTTR\n",
    "def getmsttr(z,lemmalist):\n",
    "    samples=0\n",
    "    msttr=0.0\n",
    "    while len(lemmalist)>=z:\n",
    "        samples+=1\n",
    "        msttrtype={}\n",
    "        for lemma in lemmalist[:z]:\n",
    "            msttrtype[lemma]=1\n",
    "        msttr+=len(msttrtype.keys())/float(z)\n",
    "        lemmalist=lemmalist[z:]    \n",
    "    return msttr/samples\n",
    "\n",
    "def isLetterNumber(character):\n",
    "    if character in string.printable and not character in string.punctuation:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isSentence(line):\n",
    "    for character in line:\n",
    "        if isLetterNumber(character):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Accepts a list of queries, returns a dataframe of extracted lexical features \n",
    "# that correspond to each query.\n",
    "#\n",
    "# param getLex: list of queries to extract lexical features from\n",
    "# returns lexical: dataframe containing lexical features\n",
    "\n",
    "def getLex(queries):\n",
    "    processor_dict = {\n",
    "    'tokenize': 'gsd', \n",
    "    'pos': 'bnc', \n",
    "    'lemma': 'default'\n",
    "    }\n",
    "\n",
    "    nlp = stanza.Pipeline('en', processors=processor_dict)\n",
    "    # reads information from bnc wordlist\n",
    "    lexFeat = []\n",
    "    adjdict={}\n",
    "    verbdict={}\n",
    "    noundict={}\n",
    "    worddict={}\n",
    "    wordlistfile=open(\"DataSets/BNC/bnc_all_filtered.txt\",\"r\") # list of words with pos\n",
    "    wordlist=wordlistfile.readlines()\n",
    "    wordlistfile.close()\n",
    "    for word in wordlist:\n",
    "        wordinfo=word.strip() \n",
    "        if not wordinfo or \"Total words\" in wordinfo: \n",
    "            continue\n",
    "        infolist=wordinfo.split()\n",
    "        lemma=infolist[0]\n",
    "        pos=infolist[1]\n",
    "        frequency=int(infolist[2])\n",
    "        worddict[lemma]=worddict.get(lemma,0)+frequency\n",
    "        if pos==\"Adj\":\n",
    "            adjdict[lemma]=adjdict.get(lemma,0)+frequency\n",
    "        elif pos==\"Verb\":\n",
    "            verbdict[lemma]=verbdict.get(lemma,0)+frequency\n",
    "        elif pos==\"NoC\" or pos==\"NoP\":\n",
    "            noundict[lemma]=noundict.get(lemma,0)+frequency\n",
    "    wordranks=sort_by_value(worddict)\n",
    "    verbranks=sort_by_value(verbdict)\n",
    "    length = len(queries)\n",
    "    with tqdm(total = length) as pbar:\n",
    "        for query in queries:\n",
    "            filename=query\n",
    "            doc = nlp(query)\n",
    "            for sentence in doc.sentences:\n",
    "                s = ''\n",
    "                for word in sentence.words:\n",
    "                    s+='{}_{}'.format(word.lemma, word.xpos) + ' '\n",
    "            lemlines= s\n",
    "            # print(lemlines)\n",
    "            # process input file\n",
    "            wordtypes={}\n",
    "            wordtokens=0\n",
    "            swordtypes={}\n",
    "            swordtokens=0\n",
    "            lextypes={}\n",
    "            lextokens=0\n",
    "            slextypes={}\n",
    "            slextokens=0\n",
    "            verbtypes={}\n",
    "            verbtokens=0\n",
    "            sverbtypes={}\n",
    "            adjtypes={}\n",
    "            adjtokens=0\n",
    "            advtypes={}\n",
    "            advtokens=0\n",
    "            nountypes={}\n",
    "            nountokens=0\n",
    "            lemmaposlist=[]\n",
    "            lemmalist=[]\n",
    "\n",
    "            for lemline in lemlines.split():\n",
    "                lemline=lemline.strip()\n",
    "                lemline=lemline.lower()\n",
    "                if not isSentence(lemline):\n",
    "                    continue\n",
    "                lemmas=lemline.split()\n",
    "                for lemma in lemmas:\n",
    "                    word=lemma.split(\"_\")[0]\n",
    "                    pos=lemma.split(\"_\")[-1]\n",
    "                    if (not pos in string.punctuation) and pos!=\"sent\" and pos!=\"sym\":\n",
    "                        lemmaposlist.append(lemma)\n",
    "                        lemmalist.append(word)  \n",
    "                        wordtokens+=1\n",
    "                        wordtypes[word]=1 \n",
    "                        try:\n",
    "\n",
    "                            if (not word in wordranks[-2000:]) and pos != \"cd\":\n",
    "                                swordtypes[word]=1\n",
    "                                swordtokens+=1\n",
    "                            if pos[0]==\"n\":\n",
    "                                lextypes[word]=1\n",
    "                                nountypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                nountokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"j\":\n",
    "                                lextypes[word]=1\n",
    "                                adjtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                adjtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"r\" and (adjdict.has_key(word) or (word[-2:]==\"ly\" and adjdict.has_key(word[:-2]))):\n",
    "                                lextypes[word]=1\n",
    "                                advtypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                advtokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                            elif pos[0]==\"v\" and not word in [\"be\",\"have\"]:\n",
    "                                verbtypes[word]=1\n",
    "                                verbtokens+=1\n",
    "                                lextypes[word]=1\n",
    "                                lextokens+=1\n",
    "                                if not word in wordranks[-2000:]:\n",
    "                                    sverbtypes[word]=1\n",
    "                                    slextypes[word]=1\n",
    "                                    slextokens+=1\n",
    "                        except(AttributeError):\n",
    "                            pass\n",
    "\n",
    "            # lexical density\n",
    "            if wordtokens > 0:\n",
    "                ld=float(lextokens)/wordtokens\n",
    "            else:\n",
    "                ld=0\n",
    "            # lexical sophistication\n",
    "            if lextokens != 0:\n",
    "                ls1=slextokens/float(lextokens)\n",
    "            else:\n",
    "                ls1 = 0\n",
    "            if len(wordtypes.keys()) > 0:\n",
    "                ls2=len(swordtypes.keys())/float(len(wordtypes.keys()))\n",
    "            else:\n",
    "                ls2 = 0\n",
    "\n",
    "            # verb sophistication\n",
    "            vs1 = 0\n",
    "            vs2=0\n",
    "            cvs1=0\n",
    "            if verbtokens > 0:\n",
    "                vs1=len(sverbtypes.keys())/float(verbtokens)\n",
    "                vs2=(len(sverbtypes.keys())*len(sverbtypes.keys()))/float(verbtokens)\n",
    "                cvs1=len(sverbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # lexical diversity or variation\n",
    "            # NDW, may adjust the values of \"standard\"\n",
    "            ndw=len(wordtypes.keys())\n",
    "\n",
    "            # TTR\n",
    "            if wordtokens > 0:\n",
    "                ttr=len(wordtypes.keys())/float(wordtokens)\n",
    "                if len(lemmalist)>=standard:\n",
    "                    msttr=getmsttr(standard,lemmalist)\n",
    "                cttr=len(wordtypes.keys())/sqrt(2*wordtokens)\n",
    "                rttr=len(wordtypes.keys())/sqrt(wordtokens)\n",
    "            else:\n",
    "                ttr = 0\n",
    "                cttr = 0\n",
    "                rttr = 0\n",
    "            if wordtokens == 0 or len(wordtypes.keys()) == 0:\n",
    "                logttr = 0\n",
    "#             else:\n",
    "#                 logttr=log(len(wordtypes.keys()))/log(wordtokens) \n",
    "            # 3.3 verb diversity\n",
    "            vv1, svv1, cvv1 = 0, 0, 0\n",
    "            if verbtokens > 0:\n",
    "                vv1=len(verbtypes.keys())/float(verbtokens)\n",
    "                svv1=len(verbtypes.keys())*len(verbtypes.keys())/float(verbtokens)\n",
    "                cvv1=len(verbtypes.keys())/sqrt(2*verbtokens)\n",
    "\n",
    "            # 3.4 lexical diversity\n",
    "            if lextokens != 0:\n",
    "                lv=len(lextypes.keys())/float(lextokens)\n",
    "                vv2=len(verbtypes.keys())/float(lextokens)\n",
    "                adjv=len(adjtypes.keys())/float(lextokens)\n",
    "\n",
    "            else:\n",
    "                lv=0\n",
    "                vv2=0\n",
    "                adjv=0\n",
    "\n",
    "            if nountokens != 0:\n",
    "                nv=len(nountypes.keys())/float(nountokens)\n",
    "            else:\n",
    "                nv=0\n",
    "\n",
    "\n",
    "            lexFeat.append([query, ld, ls1, ls2, vs1, vs2, cvs1, ndw, ttr,\n",
    "                           cttr, rttr,  lv, vv1, svv1, cvv1, vv2, nv, adjv]) \n",
    "            pbar.update()\n",
    "    lexical = pd.DataFrame(data = lexFeat, columns = [\"query\", \"ld\", \"ls1\", \"ls2\", \"vs1\", \"vs2\", \"cvs1\", \"ndw\", \"ttr\",\n",
    "                                                      \"cttr\", \"rttr\", \"lv\", \"vv1\", \"svv1\", \"cvv1\", \"vv2\", \"nv\", \"adjv\"])\n",
    "    return lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Sets\n",
    "\n",
    "This block of code loads the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allSessionsSQS = pickle.load( open( \"../Data/DataSets/SQS/SQS.p\", \"rb\" )) \n",
    "allQueries = allSessionsSQS['query'].tolist()\n",
    "setQueries = allQueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 00:38:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2a3d20cc3849cb9b0b46a86a4e573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 00:38:15 WARNING: Can not find tokenize: gsd from official model list. Ignoring it.\n",
      "2023-03-09 00:38:15 WARNING: Can not find pos: bnc from official model list. Ignoring it.\n",
      "2023-03-09 00:38:16 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-03-09 00:38:16 INFO: Use device: cpu\n",
      "2023-03-09 00:38:16 INFO: Loading: tokenize\n",
      "2023-03-09 00:38:16 INFO: Loading: pos\n",
      "2023-03-09 00:38:17 INFO: Loading: lemma\n",
      "2023-03-09 00:38:17 INFO: Loading: depparse\n",
      "2023-03-09 00:38:17 INFO: Loading: sentiment\n",
      "2023-03-09 00:38:17 INFO: Loading: constituency\n",
      "2023-03-09 00:38:17 INFO: Loading: ner\n",
      "2023-03-09 00:38:18 INFO: Done loading processors!\n",
      "100%|██████████| 1505/1505 [05:27<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "lexical = getLex(queries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ld</th>\n",
       "      <th>ls1</th>\n",
       "      <th>ls2</th>\n",
       "      <th>vs1</th>\n",
       "      <th>vs2</th>\n",
       "      <th>cvs1</th>\n",
       "      <th>ndw</th>\n",
       "      <th>ttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>rttr</th>\n",
       "      <th>lv</th>\n",
       "      <th>vv1</th>\n",
       "      <th>svv1</th>\n",
       "      <th>cvv1</th>\n",
       "      <th>vv2</th>\n",
       "      <th>nv</th>\n",
       "      <th>adjv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>becoming a fireman</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotel in Pocono Mountains</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wedding traditions buddhism</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diversification in hiring</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traiditional swahili recipes</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          query        ld       ls1       ls2  vs1  vs2  cvs1  \\\n",
       "0            becoming a fireman  0.666667  0.500000  0.333333  0.0  0.0   0.0   \n",
       "1     hotel in Pocono Mountains  0.750000  0.666667  0.500000  0.0  0.0   0.0   \n",
       "2   wedding traditions buddhism  1.000000  0.666667  0.666667  0.0  0.0   0.0   \n",
       "3     diversification in hiring  0.666667  1.000000  0.666667  0.0  0.0   0.0   \n",
       "4  traiditional swahili recipes  1.000000  1.000000  1.000000  0.0  0.0   0.0   \n",
       "\n",
       "   ndw  ttr      cttr      rttr   lv  vv1  svv1      cvv1  vv2   nv      adjv  \n",
       "0    3  1.0  1.224745  1.732051  1.0  1.0   1.0  0.707107  0.5  1.0  0.000000  \n",
       "1    4  1.0  1.414214  2.000000  1.0  0.0   0.0  0.000000  0.0  1.0  0.000000  \n",
       "2    3  1.0  1.224745  1.732051  1.0  0.0   0.0  0.000000  0.0  1.0  0.000000  \n",
       "3    3  1.0  1.224745  1.732051  1.0  0.0   0.0  0.000000  0.0  1.0  0.000000  \n",
       "4    3  1.0  1.224745  1.732051  1.0  0.0   0.0  0.000000  0.0  1.0  0.333333  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'ld', 'ls1', 'ls2', 'vs1', 'vs2', 'cvs1', 'ndw', 'ttr', 'cttr',\n",
       "       'rttr', 'lv', 'vv1', 'svv1', 'cvv1', 'vv2', 'nv', 'adjv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Characteristics\n",
    "\n",
    "This block of code extracts lexical characteristics from each query and starts building a dataframe for the values of these features. Filter warnings are set to ignore, as encountering numbers as well as characters with modifiers such as umlauts throw a \n",
    "\n",
    "*UserWarning: Character not defined in sonority_hierarchy*\n",
    "\n",
    "which leads to the character/number being recast as the same symbol, but in a way that is recognized by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1505/1505 [00:00<00:00, 9084.33it/s] \n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "totalSyl = []\n",
    "avgSyl = []\n",
    "simWords = []\n",
    "comWords = []\n",
    "simWordsAvg = []\n",
    "comWordsAvg = []\n",
    "mostSyl = []\n",
    "leastSyl = []\n",
    "SSP = SyllableTokenizer()\n",
    "\n",
    "with tqdm(total = len(setQueries) ) as pbar:\n",
    "    for text in setQueries:\n",
    "        running = 0\n",
    "        count = 0\n",
    "        simpleWords = 0\n",
    "        complexWords = 0\n",
    "        most = 0\n",
    "        least = sys.maxsize\n",
    "        for word in text.split(\" \"):\n",
    "            current = len(SSP.tokenize(word))\n",
    "            running += current\n",
    "            count +=1\n",
    "            if current < 3:\n",
    "                simpleWords += 1 # -- a word is simple when it's syllables are < 3 else it considered as complex \n",
    "            else:\n",
    "                complexWords +=1\n",
    "            if most < current:\n",
    "                most = current\n",
    "            if least > current:\n",
    "                least = current\n",
    "                \n",
    "        totalSyl.append(running)\n",
    "        avgSyl.append(running/count)\n",
    "        simWords.append(simpleWords)\n",
    "        comWords.append(complexWords)\n",
    "        mostSyl.append(most)\n",
    "        leastSyl.append(least)\n",
    "        pbar.update()\n",
    "        \n",
    "lexChar = pd.DataFrame(setQueries)\n",
    "lexChar = lexChar.set_index(0, drop=True)\n",
    "lexChar = lexChar.reset_index().rename(columns={0:'query'})\n",
    "\n",
    "lexChar['totalSyl'] = totalSyl\n",
    "lexChar['avgSyl'] = avgSyl\n",
    "lexChar['simWords'] = simWords\n",
    "lexChar['comWords'] = comWords\n",
    "lexChar['greatestSyl'] = mostSyl\n",
    "lexChar['leastSyl'] = leastSyl\n",
    "lexChar['numChars'] = lexChar['query'].str.len()\n",
    "lexChar['numWords'] = lexChar['query'].str.split().str.len()\n",
    "lexChar['avgLenWord'] = lexChar['numChars']/lexChar['numWords']\n",
    "\n",
    "# lexChar['qID'] = qID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexChar.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Lexical Complexity\n",
    "\n",
    "This block of code below runs the previously defined functions that extract the feature corresponding to lexical complexity. This can be very slow/time consuming.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>class</th>\n",
       "      <th>sID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>becoming a fireman</td>\n",
       "      <td>0</td>\n",
       "      <td>3199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotel in Pocono Mountains</td>\n",
       "      <td>0</td>\n",
       "      <td>2515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wedding traditions buddhism</td>\n",
       "      <td>0</td>\n",
       "      <td>2823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diversification in hiring</td>\n",
       "      <td>0</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traiditional swahili recipes</td>\n",
       "      <td>0</td>\n",
       "      <td>3145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Who plays the bad guy in Star Wars the Horde a...</td>\n",
       "      <td>1</td>\n",
       "      <td>3256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>What is a fox's favorite kind of food?</td>\n",
       "      <td>1</td>\n",
       "      <td>2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>Show me the movie called \"The Martian\"</td>\n",
       "      <td>1</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>What is the biggest rock found on Mars?</td>\n",
       "      <td>1</td>\n",
       "      <td>2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>What is the top game this week?</td>\n",
       "      <td>1</td>\n",
       "      <td>2703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1505 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query  class   sID\n",
       "0                                    becoming a fireman      0  3199\n",
       "1                             hotel in Pocono Mountains      0  2515\n",
       "2                           wedding traditions buddhism      0  2823\n",
       "3                             diversification in hiring      0  3033\n",
       "4                          traiditional swahili recipes      0  3145\n",
       "...                                                 ...    ...   ...\n",
       "1500  Who plays the bad guy in Star Wars the Horde a...      1  3256\n",
       "1501             What is a fox's favorite kind of food?      1  2859\n",
       "1502             Show me the movie called \"The Martian\"      1  3208\n",
       "1503            What is the biggest rock found on Mars?      1  2676\n",
       "1504                    What is the top game this week?      1  2703\n",
       "\n",
       "[1505 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSessionsSQS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 00:43:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecd07895c4741e5bfa6859ec139c96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 00:43:46 WARNING: Can not find tokenize: gsd from official model list. Ignoring it.\n",
      "2023-03-09 00:43:46 WARNING: Can not find pos: bnc from official model list. Ignoring it.\n",
      "2023-03-09 00:43:47 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-03-09 00:43:47 INFO: Use device: cpu\n",
      "2023-03-09 00:43:47 INFO: Loading: tokenize\n",
      "2023-03-09 00:43:47 INFO: Loading: pos\n",
      "2023-03-09 00:43:47 INFO: Loading: lemma\n",
      "2023-03-09 00:43:47 INFO: Loading: depparse\n",
      "2023-03-09 00:43:47 INFO: Loading: sentiment\n",
      "2023-03-09 00:43:48 INFO: Loading: constituency\n",
      "2023-03-09 00:43:48 INFO: Loading: ner\n",
      "2023-03-09 00:43:48 INFO: Done loading processors!\n",
      "100%|██████████| 1505/1505 [05:43<00:00,  4.38it/s]\n"
     ]
    }
   ],
   "source": [
    "lexComplex = getLex(setQueries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'ld', 'ls1', 'ls2', 'vs1', 'vs2', 'cvs1', 'ndw', 'ttr', 'cttr',\n",
       "       'rttr', 'lv', 'vv1', 'svv1', 'cvv1', 'vv2', 'nv', 'adjv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexComplex.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 18)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexComplex.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ld</th>\n",
       "      <th>ls1</th>\n",
       "      <th>ls2</th>\n",
       "      <th>vs1</th>\n",
       "      <th>vs2</th>\n",
       "      <th>cvs1</th>\n",
       "      <th>ndw</th>\n",
       "      <th>ttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>rttr</th>\n",
       "      <th>lv</th>\n",
       "      <th>vv1</th>\n",
       "      <th>svv1</th>\n",
       "      <th>cvv1</th>\n",
       "      <th>vv2</th>\n",
       "      <th>nv</th>\n",
       "      <th>adjv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>becoming a fireman</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotel in Pocono Mountains</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       query        ld       ls1       ls2  vs1  vs2  cvs1  \\\n",
       "0         becoming a fireman  0.666667  0.500000  0.333333  0.0  0.0   0.0   \n",
       "1  hotel in Pocono Mountains  0.750000  0.666667  0.500000  0.0  0.0   0.0   \n",
       "\n",
       "   ndw  ttr      cttr      rttr   lv  vv1  svv1      cvv1  vv2   nv  adjv  \n",
       "0    3  1.0  1.224745  1.732051  1.0  1.0   1.0  0.707107  0.5  1.0   0.0  \n",
       "1    4  1.0  1.414214  2.000000  1.0  0.0   0.0  0.000000  0.0  1.0   0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexComplex.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexChar.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'ld', 'ls1', 'ls2', 'vs1', 'vs2', 'cvs1', 'ndw', 'ttr', 'cttr',\n",
       "       'rttr', 'lv', 'vv1', 'svv1', 'cvv1', 'vv2', 'nv', 'adjv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexComplex.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>totalSyl</th>\n",
       "      <th>avgSyl</th>\n",
       "      <th>simWords</th>\n",
       "      <th>comWords</th>\n",
       "      <th>greatestSyl</th>\n",
       "      <th>leastSyl</th>\n",
       "      <th>numChars</th>\n",
       "      <th>numWords</th>\n",
       "      <th>avgLenWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>becoming a fireman</td>\n",
       "      <td>7</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotel in Pocono Mountains</td>\n",
       "      <td>8</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>6.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       query  totalSyl    avgSyl  simWords  comWords  \\\n",
       "0         becoming a fireman         7  2.333333         1         2   \n",
       "1  hotel in Pocono Mountains         8  2.000000         3         1   \n",
       "\n",
       "   greatestSyl  leastSyl  numChars  numWords  avgLenWord  \n",
       "0            3         1        18         3        6.00  \n",
       "1            3         1        25         4        6.25  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexChar.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'totalSyl', 'avgSyl', 'simWords', 'comWords', 'greatestSyl',\n",
       "       'leastSyl', 'numChars', 'numWords', 'avgLenWord'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexChar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd = pd.merge(lexComplex, lexChar, left_index=True, right_index=True)\n",
    "ddd.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527, 27)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aabbcc = pd.merge(lexComplex, lexChar, on='query')\n",
    "aabbcc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'ld', 'ls1', 'ls2', 'vs1', 'vs2', 'cvs1', 'ndw', 'ttr', 'cttr',\n",
       "       'rttr', 'lv', 'vv1', 'svv1', 'cvv1', 'vv2', 'nv', 'adjv', 'totalSyl',\n",
       "       'avgSyl', 'simWords', 'comWords', 'greatestSyl', 'leastSyl', 'numChars',\n",
       "       'numWords', 'avgLenWord'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aabbcc.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexChar.to_csv('lexCharTest.csv', index = False)\n",
    "# lexComplex.to_csv('lexComplexTest.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/assoumerredempta/Documents/aSpring_2023/RYSe_Final/FeatureExtraction'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine And Return Feature Set\n",
    "\n",
    "This block of code combines all features into one dataframe and outputs that combination as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicalFeatures = pd.merge(lexComplex, lexChar, on='query')\n",
    "# pickle.dump(lexicalFeatures, open( \"Pickles/LexFeat.p\", \"wb\" ) )\n",
    "\n",
    "lexicalFeatures = pd.merge(lexComplex, lexChar, left_index=True, right_index=True)\n",
    "lexicalFeatures.drop(columns = ['query_y'], inplace = True)\n",
    "lexicalFeatures.rename(columns = {'query_x':'query'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ld</th>\n",
       "      <th>ls1</th>\n",
       "      <th>ls2</th>\n",
       "      <th>vs1</th>\n",
       "      <th>vs2</th>\n",
       "      <th>cvs1</th>\n",
       "      <th>ndw</th>\n",
       "      <th>ttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>...</th>\n",
       "      <th>adjv</th>\n",
       "      <th>totalSyl</th>\n",
       "      <th>avgSyl</th>\n",
       "      <th>simWords</th>\n",
       "      <th>comWords</th>\n",
       "      <th>greatestSyl</th>\n",
       "      <th>leastSyl</th>\n",
       "      <th>numChars</th>\n",
       "      <th>numWords</th>\n",
       "      <th>avgLenWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>becoming a fireman</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hotel in Pocono Mountains</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>6.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wedding traditions buddhism</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query        ld       ls1       ls2  vs1  vs2  cvs1  \\\n",
       "0           becoming a fireman  0.666667  0.500000  0.333333  0.0  0.0   0.0   \n",
       "1    hotel in Pocono Mountains  0.750000  0.666667  0.500000  0.0  0.0   0.0   \n",
       "2  wedding traditions buddhism  1.000000  0.666667  0.666667  0.0  0.0   0.0   \n",
       "\n",
       "   ndw  ttr      cttr  ...  adjv  totalSyl    avgSyl  simWords  comWords  \\\n",
       "0    3  1.0  1.224745  ...   0.0         7  2.333333         1         2   \n",
       "1    4  1.0  1.414214  ...   0.0         8  2.000000         3         1   \n",
       "2    3  1.0  1.224745  ...   0.0         7  2.333333         2         1   \n",
       "\n",
       "   greatestSyl  leastSyl  numChars  numWords  avgLenWord  \n",
       "0            3         1        18         3        6.00  \n",
       "1            3         1        25         4        6.25  \n",
       "2            3         2        27         3        9.00  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicalFeatures.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'ld', 'ls1', 'ls2', 'vs1', 'vs2', 'cvs1', 'ndw', 'ttr', 'cttr',\n",
       "       'rttr', 'lv', 'vv1', 'svv1', 'cvv1', 'vv2', 'nv', 'adjv', 'totalSyl',\n",
       "       'avgSyl', 'simWords', 'comWords', 'greatestSyl', 'leastSyl', 'numChars',\n",
       "       'numWords', 'avgLenWord'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicalFeatures.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 27)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicalFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lexicalFeatures, open( \"Pickles/LexFeat.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected shape: 1505, 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "27 + 5 + 41 + 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = SyllableTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'ne']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss.tokenize('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at', 'hle', 'te']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss.tokenize('athlete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
